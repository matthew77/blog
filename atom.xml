<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Liang的风险平价观</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-07-18T14:41:43.366Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Liang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Bottom-up Beta方法 -- 试算腾讯的beta</title>
    <link href="http://yoursite.com/2020/07/18/bottom-up-beta/"/>
    <id>http://yoursite.com/2020/07/18/bottom-up-beta/</id>
    <published>2020-07-18T13:03:53.000Z</published>
    <updated>2020-07-18T14:41:43.366Z</updated>
    
    <content type="html"><![CDATA[<p>Beta是一个非常流行的个股风险度量尺度，通过大名鼎鼎的CAPM广为传播。当需要确定某个股的discount rate的时候最常用的方式就是通过CAPM：$ E(R)=R_f + \beta * (R_m-R_f) $ 。其中$R_m$ 是 expected market return。 </p><a id="more"></a><p>那么该如何获取beta呢？一种方式就是通过linear regression。将independent variables:$ R_m, R_f $ 以及dependent variable E(R) 通过linear regression便可获取参数beta了。但是，很多终端包括在网上找到的所有例子，在计算beta时不考虑$R_f$，直接通过$ E(R) = \beta*(R_m) $ 来确定beta；这种做法可能有一定的道理，由于$R_f$本身很小，比如国内10年期国债的yield在3%左右，如果以weekly return计算的话，对应的return才0.0577%，而国外的$R_f$就更不用说了，因此可以近似忽略。另外，如何选择risk free rate本身又是一件不太简单的事情。例如，对于成熟市场来说如美国来说，就可以有多种选择，除了国债外（10年期或其他？）还有Overnight Index Swap (OIS)等。</p><p>接下来通过linear regression方法来计算腾讯的beta。因为腾讯在香港上市，因此，market return使用恒生指数。$ R_f $使用香港政府10年期国债。仿照Bloomberg计算beta的方法，使用最近2年的weekly return数据。我自己计算获得的结果是：</p><ul><li>不包含$ R_f $，使用regression 计算得出的beta = 1.219</li><li>包含$ R_f $，使用regression 计算得出的beta = 1.22</li><li>直接使用公式：$Covariance(market_{rt}, stock_{rt}) / Var(market_{rt})$，计算得出的beta = 1.231</li></ul><p>可见，risk free rate对于beta的影响确实很小。另外，我在Choice终端上，使用同样的周期得到的结果是1.2123。而yahoo finance上，5 年，月return显示的beta是1.02，我用yahoo的数据自己算了一下，发现它的结果是不对的！应该是1.2471，接近我上面的计算结果。看来大网站的数据也不能相信。</p><p>当然，本文不是讨论regression beta如何计算，从上面可以看出来，单个股票的beta其实是一个含有大量噪音的数据，如果直接就这么用，那心里肯定没底，十有八九这个beta的值是有问题的。因此，有一种“bottom-up Betas”的方法可以比较好的消除单只股票的beta的噪音。</p><p>什么是“bottom-up beta”？就是根据目标公司所在的行业，在这些行业里找到同目标公司相似的一堆公司，用这些公司的beta计算平均值获得一个比较准确的beta。bottom-up beta的一大好处就是，当你找到足够多的样本公司，你估算出来的目标公司beta的standaerd error会大大降低。另一个好处就是即使当目标公司没有很多历史数据的情况下，你一样可以通过bottom-up beta计算得到目标公司的beta。</p><p>以下是bottom-up beta的具体计算方法：</p><ol><li>确定目标公司所处的行业，目标公司有可能是单一行业，也有可能是跨多个行业。</li><li>在每个行业里找到相似的公司，当然多多益善。如果是跨行业的公司，则需要分别在每个行业里面找到相似的公司，并获得这些公司的beta。然后分别按行业计算这些公司beta的平均值。</li><li>计算unlevered beta。$ \beta_u = \beta_r / (1 + (1-tax) D/E) $ 。其中D/E是debt to equity ratio，这里指相似公司的平均D/E。</li><li>确定目标公司在各业务上的比重，通常可以按照销售收入或opterating income来确定比重。当然最理想的方式是估算各业务的价值，以价值来确定比重。</li><li>计算目标公司unlevered beta的加权平均值。个业务unlevered beta来自第3步，权重来自第4步。</li><li>最后计算levered beta，$ \beta_{bottomup} = \beta_u (1+ (1-tax) (D/E))$，这里的D/E是目标公司的D/E。</li></ol><p>方法讲完了，接下来就以腾讯控股为例来确定她的bottom-up beta。按照以上第1步，先确定腾讯所在行业即她涉及哪些业务。腾讯的收入包括如下几块：</p><ul><li>网络游戏</li><li>数字内容，包括：视频（传统+短视频），音乐，阅读</li><li>网络广告</li><li>金融科技</li><li>云及企业服务</li></ul><p>而从GICS分类来看腾讯属于：信息技术-软件与服务-信息技术服务-互联网服务与基础设施。显然这样的分类不能体现腾讯的全部业务。一种比较仔细的做法是根据腾讯的业务，分别找到对应的公司。这里，我就比较粗粒度的找到一些我觉得相关的在香港上市的公司，见下表：</p><table><thead><tr><th>证券代码</th><th>证券名称</th><th>所属GICS行业</th><th>Beta</th><th>总市值（亿元）</th><th>D/E</th></tr></thead><tbody><tr><td>09988.HK</td><td>阿里巴巴-SW</td><td>非日常生活消费品-零售业-互联网与直销零售-互联网与直销零售</td><td>0.9181</td><td>51,250.7415</td><td>0.493</td></tr><tr><td>03690.HK</td><td>美团点评-W</td><td>信息技术-软件与服务-软件-应用软件</td><td>0.8307</td><td>11,231.8189</td><td>0.339</td></tr><tr><td>01810.HK</td><td>小米集团-W</td><td>信息技术-软件与服务-信息技术服务-互联网服务与基础设施</td><td>1.1003</td><td>3,733.3914</td><td>1.070</td></tr><tr><td>00268.HK</td><td>金蝶国际</td><td>信息技术-软件与服务-软件-应用软件</td><td>1.5451</td><td>598.9131</td><td>0.370</td></tr><tr><td>08083.HK</td><td>中国有赞</td><td>非日常生活消费品-零售业-互联网与直销零售-互联网与直销零售</td><td>1.1385</td><td>282.5032</td><td>1.693</td></tr><tr><td>02013.HK</td><td>微盟集团</td><td>信息技术-软件与服务-信息技术服务-互联网服务与基础设施</td><td>1.9174</td><td>252.0400</td><td>0.660</td></tr><tr><td>00136.HK</td><td>恒腾网络</td><td>信息技术-软件与服务-信息技术服务-互联网服务与基础设施</td><td>0.7934</td><td>182.7986</td><td>0.347</td></tr><tr><td>01686.HK</td><td>新意网集团</td><td>信息技术-软件与服务-信息技术服务-互联网服务与基础设施</td><td>1.1249</td><td>136.7236</td><td>2.696</td></tr><tr><td>00777.HK</td><td>网龙</td><td>信息技术-软件与服务-软件-应用软件</td><td>1.4445</td><td>127.7308</td><td>0.447</td></tr><tr><td>00799.HK</td><td>IGG</td><td>信息技术-软件与服务-信息技术服务-互联网服务与基础设施</td><td>1.5530</td><td>93.0871</td><td>0.272</td></tr><tr><td>01089.HK</td><td>乐游科技控股</td><td>信息技术-软件与服务-信息技术服务-互联网服务与基础设施</td><td>0.6488</td><td>92.4880</td><td>0.282</td></tr><tr><td>00302.HK</td><td>中手游</td><td>信息技术-软件与服务-软件-应用软件</td><td>1.0677</td><td>77.5940</td><td>0.403</td></tr><tr><td>01675.HK</td><td>亚信科技</td><td>信息技术-软件与服务-软件-应用软件</td><td>0.4815</td><td>71.0940</td><td>0.244</td></tr><tr><td>01357.HK</td><td>美图公司</td><td>信息技术-软件与服务-信息技术服务-互联网服务与基础设施</td><td>1.3548</td><td>68.5665</td><td>0.236</td></tr><tr><td>01137.HK</td><td>香港电视</td><td>通讯服务-媒体与娱乐-娱乐-电影与娱乐</td><td>1.1290</td><td>56.4704</td><td>0.656</td></tr><tr><td>01806.HK</td><td>汇付天下</td><td>金融-综合金融-综合金融服务-其它综合性金融服务</td><td>0.8373</td><td>39.5487</td><td>3.959</td></tr><tr><td>BIDU.O</td><td>百度</td><td>信息技术-软件与服务-信息技术服务-互联网服务与基础设施</td><td>1.4500</td><td>423.1467</td><td>0.751</td></tr><tr><td>NTES.O</td><td>网易</td><td>信息技术-软件与服务-信息技术服务-互联网服务与基础设施</td><td>0.7400</td><td>631.9686</td><td>0.599</td></tr></tbody></table><p>以上数据来自东方财富Choice数据。</p><p>一共12家上市公司。在查找这些公司的时候首先我考虑市值，毕竟腾讯的体量摆在那里，找对应的公司市值也不能太小，否则就没有参考意义了；另外，还得考虑上市时间，像网易，京东这样的公司虽然比较匹配，但是上市时间实在太短，历史数据太少，因此也没参考意义，我就直接拿美股的数据过来了（虽然跨市场，但只要样本足够多，通过大数定律还是可以得到比较准确的结果）。</p><p>有了参照数据，很容易就得到了beta的平均值：1.115。然后计算unlevered beta，需要先获得相似公司的D/E，我这里将所有相关公司的detb和equity分别加总，再用这两个总和求得D/E = 0.573。然后还需要确定tax rate，根据国家政策，高新技术企业税率是15%，代入这些数据可得：</p><p>$\beta_u$ = 1.115/(1+(1-0.15) * 0.573) = 0.75</p><p>最后，为了获取腾讯的bottom-up beta，我们还需要确定腾讯的D/E，根据腾讯的财报，可得到她的D/E = 0.976。那么，对应的beta应该为：</p><p>$\beta_{bottomup}$ = 0.75*(1+(1-0.15)*0.976) = 1.372</p><p>这就是腾讯的beta。那么，将这个bottom-up beta同腾讯的regressison beta（=1.22）比大了一点（12.5%）。但1.372也是有一定道理的，因为，从D/E比来看，网易，百度，阿里都比腾讯低。因此，我个人觉得1.372这个值还是比较靠谱。确定beta作为股票估值的重要的一步到此完成。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Beta是一个非常流行的个股风险度量尺度，通过大名鼎鼎的CAPM广为传播。当需要确定某个股的discount rate的时候最常用的方式就是通过CAPM：$ E(R)=R_f + \beta * (R_m-R_f) $ 。其中$R_m$ 是 expected market return。 &lt;/p&gt;
    
    </summary>
    
    
      <category term="Valuation" scheme="http://yoursite.com/categories/Valuation/"/>
    
    
  </entry>
  
  <entry>
    <title>中国股票市场的风险溢价</title>
    <link href="http://yoursite.com/2020/07/14/cn-market-rp/"/>
    <id>http://yoursite.com/2020/07/14/cn-market-rp/</id>
    <published>2020-07-14T13:11:41.000Z</published>
    <updated>2020-07-14T14:02:02.274Z</updated>
    
    <content type="html"><![CDATA[<p>所谓的Risk Premium（以下简写成RP）是值市场的预期收益(expected return)减去无风险收益率（risk free rate）$ r_f $。RP是估值的关键，当对某只股票进行估值的时候，你首先需要确定其required return（或称discount rate）r。而$ r = r_f + \beta*RP $。那么中国市场的RP是多少呢？作为emerging market，中国市场的RP是否就比成熟市场高呢？</p><a id="more"></a><p>RP的确定通常有2种方式。Historical Premium 和 Implied Premium。Historical Premium其实就是根据历史数据计算市场的平均收益率（算数或几何平均）。这里以沪深300指数来代表中国股票市场整体。那么，先来看看Historical Premium是多少？参见下表：</p><table><thead><tr><th>时间</th><th>开盘</th><th>收盘</th><th>收益</th></tr></thead><tbody><tr><td>2005/12/30</td><td>994.76</td><td>923.45</td><td>-7.17%</td></tr><tr><td>2006/12/29</td><td>926.56</td><td>2041.05</td><td>121.02%</td></tr><tr><td>2007/12/28</td><td>2073.25</td><td>5338.27</td><td>161.55%</td></tr><tr><td>2008/12/31</td><td>5349.76</td><td>1817.72</td><td>-65.95%</td></tr><tr><td>2009/12/31</td><td>1848.33</td><td>3575.68</td><td>96.71%</td></tr><tr><td>2010/12/31</td><td>3592.47</td><td>3128.26</td><td>-12.51%</td></tr><tr><td>2011/12/30</td><td>3155.56</td><td>2345.74</td><td>-25.01%</td></tr><tr><td>2012/12/31</td><td>2361.5</td><td>2522.95</td><td>7.55%</td></tr><tr><td>2013/12/31</td><td>2551.81</td><td>2330.03</td><td>-7.65%</td></tr><tr><td>2014/12/31</td><td>2323.43</td><td>3533.71</td><td>51.66%</td></tr><tr><td>2015/12/31</td><td>3566.09</td><td>3731</td><td>5.58%</td></tr><tr><td>2016/12/30</td><td>3725.86</td><td>3310.08</td><td>-11.28%</td></tr><tr><td>2017/12/29</td><td>3313.95</td><td>4030.86</td><td>21.78%</td></tr><tr><td>2018/12/28</td><td>4045.21</td><td>3010.65</td><td>-25.31%</td></tr><tr><td>2019/12/31</td><td>3017.07</td><td>4096.58</td><td>36.07%</td></tr><tr><td>2020/07/14</td><td>4121.35</td><td>4806.69</td><td>17.33%</td></tr></tbody></table><p>一共才16条数据，数据样本很小，未必能揭示真实的RP。如果以算数平均计算，可得沪深300的预期收益是：22.77%。而以几何平均计算，预期收益是：10.35%，几何平均更合理，通常来说几何平均是会小于算数平均。对于无风险收益率来说，选取10年期国债的收益率：3.06%，由此可得RP=10.35%-3.06% = 7.29%。看上去好像还蛮不错的哦！</p><p>当然，使用historical premium的问题也很明显，你需要有大量的历史数据，对于成熟市场，如果美国股市，可以得到相对可靠的数据（其实方差也很大），而对于中国这样的emerging market，historical premium的准确性更是值得怀疑。另外一个问题，历史数据只能反应历史，而很多新的变化就无法体现在结果中，就好比看着后视镜开车。</p><p>Implied Premium，在计算过程中使用了“未来数据”，将最新的预期加入计算，因此更加能反应未来的变化。那么，如何计算implied premium呢？简单来说就是根据dividend discount model (DDM) 来推算出r–required return, 我们知道DDM的简单形式可以写成：<br>$$<br>P = D_1/(r-g)<br>$$<br>其中，P对应现在的指数，D1=下一年的分红，g=收入增长率。那么，如果知道P，D1，g就可以推算出r来了。而D1，g都是预测值，是对未来的预期。通过这种方式计算出来的r对估值具有更好的效果。</p><p>接下来还是以沪深300为例来计算Implied Premium。在本例中使用两段式的DDM， 即一开始的5年为高速增长期，5年以后为稳定增长期。输入参数如下：</p><table><thead><tr><th>参数</th><th>值</th><th>说明</th></tr></thead><tbody><tr><td>当前指数</td><td>4806.69</td><td>这个很容获取，直接填上最新的指数值，我这里填的式2020/7/14日沪深300的收盘价</td></tr><tr><td>分红和回购率</td><td>2.07%</td><td>这个要花一点功夫了，不得不说中证指数有限公司工作很粗糙，照理这些数据应该提供的，我在他们网站上找了好久也没找到，而S&amp;P以及恒生指数都提供这些数据的。最后只能自己计算了，将300只成分股一个一个数据汇集起来。最终的结果也符合直觉，国内红率不是很高，尤其对应这几天的大涨，分红率就更低了。</td></tr><tr><td>今后5年收入增加率</td><td>14.364%</td><td>这个数据也挺难弄。我从Choice终端上查找一致性预期获得的。原始数据是今后2年复合增长率21.9%，但是感觉这数据太高了。有可能是疫情的原因。我自己作了调整。</td></tr><tr><td>五年以后增长率</td><td>3.06%</td><td>这个值代表了永续增长率，我取和无风险收益保持一致</td></tr><tr><td>10年期国债收益率</td><td>3.06%</td><td>无风险收益率</td></tr></tbody></table><p>然后，利用MS Excel的规划求解就能得到implied risk premium，参见下图：</p><img src="/2020/07/14/cn-market-rp/irp.PNG"><p>expected dividend是根据今后5年收入增长率推算得到。terminal value根据DDM公式计算得到。应用规划求解，可变单元格的值设置为Implied Risk Premium，最后的结果为3.49%，同historical premium的7.29%比低了好多啊！！！至于说哪个值更加符合市场实际情况呢？我个人觉得3.49%似乎比较符合直观感受。我大哀股如同绞肉机，说直白一点，就是收益和风险不对称，股民承担了很高的风险却为了不太理想的收益。</p><p>为何implied premium会那么低？首先，分红+回购比例很低！这也导致了terminal value很低，而另一方面，指数本身又很高。这就进一步解释了高风险低收益的市场状况。假设市场跌去50%，而分红绝对值保持不变，那么implied premium就变成了6.81%，相对来说就改善了很多。</p><p>所以，implied premium能从风险收益比较的角度来提供我们观察市场的一种方法。那么，究竟该使用historical premium还是implied premium呢？</p><p>下表是Damodaran教授提供的美国市场的数据：</p><img src="/2020/07/14/cn-market-rp/which_to_use.PNG"><p>显然implied premium效果好很多了。不过需要指出的一点，implied premium需要高质量的预期数据，例如今后几年的盈利增长率，分红率。获得高质量的一致性预期数据就显得尤为重要。当然还需要指出的是implied premium并没有一个客观正确的值，每个人可以有自己预期，并得到对应的值，就好像每个人对市场都可以有自己的看法一样。RP是一项很关键的数据，后面的股票估值直接需要RP的参与。我打算每周都会更新implied risk premium。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;所谓的Risk Premium（以下简写成RP）是值市场的预期收益(expected return)减去无风险收益率（risk free rate）$ r_f $。RP是估值的关键，当对某只股票进行估值的时候，你首先需要确定其required return（或称discount rate）r。而$ r = r_f + \beta*RP $。那么中国市场的RP是多少呢？作为emerging market，中国市场的RP是否就比成熟市场高呢？&lt;/p&gt;
    
    </summary>
    
    
      <category term="Valuation" scheme="http://yoursite.com/categories/Valuation/"/>
    
    
  </entry>
  
  <entry>
    <title>vanishing gradients</title>
    <link href="http://yoursite.com/2020/06/02/vanishing-gradients/"/>
    <id>http://yoursite.com/2020/06/02/vanishing-gradients/</id>
    <published>2020-06-02T09:13:32.000Z</published>
    <updated>2020-06-02T09:37:34.701Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Vanishing-Gradients"><a href="#Vanishing-Gradients" class="headerlink" title="Vanishing Gradients"></a>Vanishing Gradients</h1><p>这里只使用numpy来实现一个neural network，而非借助pytorch这样的框架，如此，可以更好的帮助我理解neural network以及vanishing gradients.</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">10.0</span>, <span class="number">8.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># generate random data -- not linearly separable</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">N = <span class="number">100</span> <span class="comment"># number of points per class</span></span><br><span class="line">D = <span class="number">2</span> <span class="comment"># dimensionality (向量维度)</span></span><br><span class="line">K = <span class="number">3</span> <span class="comment"># number of classes</span></span><br><span class="line">X = np.zeros((N*K, D))</span><br><span class="line">num_train_examples = X.shape[<span class="number">0</span>]</span><br><span class="line">y = np.zeros(N*K, dtype=<span class="string">'uint8'</span>)  <span class="comment"># 无符号整数</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(K):</span><br><span class="line">    ix = range(N*j, N*(j+<span class="number">1</span>))</span><br><span class="line">    r = np.linspace(<span class="number">0.0</span>, <span class="number">1</span>, N) <span class="comment"># radius, evenly spaced numbers</span></span><br><span class="line">    t = np.linspace(j*<span class="number">4</span>, (j+<span class="number">1</span>)*<span class="number">4</span>, N) + np.random.randn(N)*<span class="number">0.2</span> <span class="comment"># theta</span></span><br><span class="line">    X[ix] = np.c_[r*np.sin(t), r*np.cos(t)] <span class="comment"># 变成2列的matrix</span></span><br><span class="line">    y[ix] = j</span><br><span class="line">fig = plt.figure()</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.Spectral)</span><br><span class="line">plt.xlim([<span class="number">-1</span>,<span class="number">1</span>])</span><br><span class="line">plt.ylim([<span class="number">-1</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p><img src="1.svg" alt="svg"></p><p>sigmoid 函数的值在0,1之间，尤其是在输入值的绝对值很大的情况下，两端会无限接近0或1，因此变得很扁平，由此，对应的梯度（Gradient）就会无限逼近0。这就导致了所谓的梯度消失（vanishing gradients）的现象。因为梯度消失使得神经网络无法进行有效的学习（因为每次迭代对参数W的修正几乎都是0）。</p><p>而另一方面，relu函数不会出现因为输入参数变大而输出变得不敏感。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    x = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_grad</span><span class="params">(x)</span>:</span> <span class="comment"># sigmoid函数的导数</span></span><br><span class="line">    <span class="keyword">return</span> (x)*(<span class="number">1</span>-x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>,x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot the 2 function</span></span><br><span class="line">x = np.linspace(<span class="number">-3</span>,<span class="number">3</span>,<span class="number">100</span>)</span><br><span class="line">y_sigmoid = sigmoid(x)</span><br><span class="line">y_relu = relu(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y_sigmoid, <span class="string">'b-'</span>, x, y_relu, <span class="string">'r-'</span>)</span><br></pre></td></tr></table></figure><p><img src="2.svg" alt="svg"></p><p>接下来让我们看一下2种不同的非线性函数（sigmoid和relu）对神经网络在训练时的影响。以下，我们会创建一个简单的3层神经网路（2 hidden layers）。 通过使用sigmoid和relu我们可以比较在训练过程中的区别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># function to train a 3 layer neural net with either relu or sigmoid nonlinearity via vanilla grad decent.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">three_layer_net</span><span class="params">(NONLINEARITY, X, y, model, step_size, reg)</span>:</span></span><br><span class="line">    <span class="comment"># param init</span></span><br><span class="line">    h = model[<span class="string">'h'</span>]</span><br><span class="line">    h2 = model[<span class="string">'h2'</span>]</span><br><span class="line">    W1 = model[<span class="string">'W1'</span>]</span><br><span class="line">    W2 = model[<span class="string">'W2'</span>]</span><br><span class="line">    W3 = model[<span class="string">'W3'</span>]</span><br><span class="line">    b1 = model[<span class="string">'b1'</span>]</span><br><span class="line">    b2 = model[<span class="string">'b2'</span>]</span><br><span class="line">    b3 = model[<span class="string">'b3'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># some hyperparameters</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># gradient descent loop</span></span><br><span class="line">    num_examples = X.shape[<span class="number">0</span>]</span><br><span class="line">    plot_array_1 = []</span><br><span class="line">    plot_array_2 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">50000</span>):</span><br><span class="line">        <span class="comment"># forward prop</span></span><br><span class="line">        <span class="comment"># 假设有X的维度[N, M], 即有N条training 数据，每条数据有M个feature</span></span><br><span class="line">        <span class="keyword">if</span> NONLINEARITY == <span class="string">'RELU'</span>:</span><br><span class="line">            <span class="comment"># 从X到hidden layer 1, X=[N*2], W1=[2*50] --&gt; hidden layer=[N*50]</span></span><br><span class="line">            hidden_layer = relu(np.dot(X,W1) + b1)  </span><br><span class="line">            <span class="comment"># 从 hidden layer 1 到 layer2. hidden 1 = [N*50], W2=[50*50]--&gt;hidden2 = [N*50]</span></span><br><span class="line">            hidden_layer2 = relu(np.dot(hidden_layer, W2) + b2)  </span><br><span class="line">            <span class="comment"># 从 hidden 2 到 最总output layer， hidden 2 = [1*50], W3=[50*3] --&gt; output = [3] </span></span><br><span class="line">            scores = np.dot(hidden_layer2, W3) + b3  <span class="comment"># scores的维度是[N,3]</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">elif</span> NONLINEARITY == <span class="string">'SIGM'</span>:</span><br><span class="line">            hidden_layer = sigmoid(np.dot(X,W1) + b1)  <span class="comment"># 从X到hidden layer 1</span></span><br><span class="line">            hidden_layer2 = sigmoid(np.dot(hidden_layer, W2) + b2)  <span class="comment"># 从 hidden layer 1 到 layer2</span></span><br><span class="line">            scores = np.dot(hidden_layer2, W3) + b3</span><br><span class="line"></span><br><span class="line">        exp_scores = np.exp(scores) <span class="comment"># [N*K], K=3</span></span><br><span class="line">        <span class="comment"># keepdims=True 意思是np.sum被sum的维度变为1，另一个维度保持不变。</span></span><br><span class="line">        probs = exp_scores / np.sum(exp_scores, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>) <span class="comment"># normalize，行方向sum</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute the loss: average cross-entropy loss and regularization</span></span><br><span class="line">        corect_logprobs = -np.log(probs[range(num_examples), y]) <span class="comment"># y的取值范围0，1，2。结果[N*1]</span></span><br><span class="line">        data_loss = np.sum(corect_logprobs)/num_examples</span><br><span class="line">        <span class="comment"># regularization项</span></span><br><span class="line">        reg_loss = <span class="number">0.5</span>*reg*np.sum(W1*W1) + <span class="number">0.5</span>*reg*np.sum(W2*W2)+<span class="number">0.5</span>*reg*np.sum(W3*W3)</span><br><span class="line">        loss = data_loss + reg_loss</span><br><span class="line">        <span class="comment"># if i % 1000 == 0:</span></span><br><span class="line">        <span class="comment">#     print("iteration &#123;0&#125;: loss &#123;1&#125;".format(i, loss))</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># comput the gradient on scores</span></span><br><span class="line">        dscores = probs  <span class="comment">#  -- [N*K]</span></span><br><span class="line">        dscores[range(num_examples), y] -= <span class="number">1</span>  <span class="comment"># error value -- [N*K]</span></span><br><span class="line">        dscores /= num_examples</span><br><span class="line"></span><br><span class="line">        <span class="comment"># backprop here</span></span><br><span class="line">        <span class="comment"># 从output layer back prop 到 hidden layer2, 由于该层没有nonlinearity</span></span><br><span class="line">        <span class="comment"># 即 hidden2*W3 = s, ds/dW3 = hidden2 (local gradient), 因此back prop error</span></span><br><span class="line">        <span class="comment"># 只需要将error * local gradient，即hidden2.</span></span><br><span class="line">        dW3 = (hidden_layer2.T).dot(dscores)  <span class="comment"># [50*N] * [N*3] = [50*3]</span></span><br><span class="line">        db3 = np.sum(dscores, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>) <span class="comment"># [3]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> NONLINEARITY == <span class="string">'RELU'</span>:</span><br><span class="line">            <span class="comment"># backprop Relu nonlinearity here</span></span><br><span class="line">            dhidden2 = np.dot(dscores, W3.T)</span><br><span class="line">            dhidden2[hidden_layer2 &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">            dW2 = np.dot(hidden_layer.T, dhidden2)</span><br><span class="line">            plot_array_2.append(np.sum(np.abs(dW2))/np.sum(np.abs(dW2.shape)))</span><br><span class="line">            db2 = np.sum(dhidden2, axis=<span class="number">0</span>)</span><br><span class="line">            dhidden = np.dot(dhidden2, W2.T)</span><br><span class="line">            dhidden[hidden_layer &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">elif</span> NONLINEARITY == <span class="string">'SIGM'</span>:</span><br><span class="line">            <span class="comment"># backprop sigmoid nonlinearity there</span></span><br><span class="line">            <span class="comment"># hidden layer 2 是 sigmoid的输出，因此需要乘以一个local gradiant</span></span><br><span class="line">            dhidden2 = dscores.dot(W3.T)*sigmoid_grad(hidden_layer2) </span><br><span class="line">            dW2 = (hidden_layer.T).dot(dhidden2)</span><br><span class="line">            plot_array_2.append(np.sum(np.abs(dW2))/np.sum(np.abs(dW2.shape)))</span><br><span class="line">            db2 = np.sum(dhidden2, axis=<span class="number">0</span>)</span><br><span class="line">            dhidden = dhidden2.dot(W2.T)*sigmoid_grad(hidden_layer)</span><br><span class="line"></span><br><span class="line">        dW1 = np.dot(X.T, dhidden)</span><br><span class="line">        plot_array_1.append(np.sum(np.abs(dW1))/np.sum(np.abs(dW1.shape)))</span><br><span class="line">        db1 = np.sum(dhidden, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># add regularization</span></span><br><span class="line">        dW3 += reg * W3</span><br><span class="line">        dW2 += reg * W2</span><br><span class="line">        dW1 += reg * W1</span><br><span class="line"></span><br><span class="line">        <span class="comment"># option to return loss, grads, </span></span><br><span class="line">        <span class="comment"># grads = &#123;&#125;</span></span><br><span class="line">        <span class="comment"># grads['W1'] = dW1</span></span><br><span class="line">        <span class="comment"># grads['W2'] = dW2</span></span><br><span class="line">        <span class="comment"># grads['W3'] = dW3</span></span><br><span class="line">        <span class="comment"># grads['b1'] = db1</span></span><br><span class="line">        <span class="comment"># grads['b2'] = db2</span></span><br><span class="line">        <span class="comment"># grads['b3'] = db3</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># update grads</span></span><br><span class="line">        W1 += -step_size * dW1</span><br><span class="line">        b1 += -step_size * db1</span><br><span class="line">        W2 += -step_size * dW2</span><br><span class="line">        b2 += -step_size * db2        </span><br><span class="line">        W3 += -step_size * dW3</span><br><span class="line">        b3 += -step_size * db3</span><br><span class="line">    <span class="comment"># evaluate training set accuracy， 使用training出来的W1，W2,W3,b1,b2,b3</span></span><br><span class="line">    <span class="comment"># 来计算结果。</span></span><br><span class="line">    <span class="keyword">if</span> NONLINEARITY == <span class="string">'RELU'</span>:</span><br><span class="line">        hidden_layer = relu(np.dot(X, W1) + b1)</span><br><span class="line">        hidden_layer2 = relu(np.dot(hidden_layer, W2) + b2)</span><br><span class="line">    <span class="keyword">elif</span> NONLINEARITY == <span class="string">'SIGM'</span>:</span><br><span class="line">        hidden_layer = sigmoid(np.dot(X, W1) + b1)</span><br><span class="line">        hidden_layer2 = sigmoid(np.dot(hidden_layer, W2) + b2)</span><br><span class="line"></span><br><span class="line">    scores = np.dot(hidden_layer2, W3) + b3</span><br><span class="line">    predicted_class = np.argmax(scores, axis=<span class="number">1</span>)  <span class="comment"># 找出每一行的最大值的index</span></span><br><span class="line">    print(<span class="string">'training accuracy: &#123;0&#125;'</span>.format(np.mean(predicted_class==y)))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> plot_array_1, plot_array_2, W1, W2, W3, b1, b2, b3</span><br></pre></td></tr></table></figure><p>关于back propagation 的计算参见下图。 其实就是对chain rule的应用。</p><img src="/2020/06/02/vanishing-gradients/8.jpg"><h3 id="Train-net-with-sigmoid-nonlinearity-first"><a href="#Train-net-with-sigmoid-nonlinearity-first" class="headerlink" title="Train net with sigmoid nonlinearity first"></a>Train net with sigmoid nonlinearity first</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize toy model, train sigmoid net </span></span><br><span class="line"></span><br><span class="line">N = <span class="number">100</span>  <span class="comment"># number of points per class</span></span><br><span class="line">D = <span class="number">2</span>  <span class="comment"># dimensionality</span></span><br><span class="line">K = <span class="number">3</span>  <span class="comment"># number of classes</span></span><br><span class="line">h = <span class="number">50</span>  <span class="comment"># hidden layer 1 size</span></span><br><span class="line">h2 = <span class="number">50</span>  <span class="comment"># hidden layer 2 size</span></span><br><span class="line">num_train_examples = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">model = &#123;&#125;</span><br><span class="line">model[<span class="string">'h'</span>] = h</span><br><span class="line">model[<span class="string">'h2'</span>] = h2</span><br><span class="line">model[<span class="string">'W1'</span>] = <span class="number">0.1</span> * np.random.randn(D, h)</span><br><span class="line">model[<span class="string">'b1'</span>] = np.zeros((<span class="number">1</span>,h))</span><br><span class="line">model[<span class="string">'W2'</span>] = <span class="number">0.1</span> * np.random.randn(h, h2)</span><br><span class="line">model[<span class="string">'b2'</span>] = np.zeros((<span class="number">1</span>,h2))</span><br><span class="line">model[<span class="string">'W3'</span>] = <span class="number">0.1</span> * np.random.randn(h2, K)</span><br><span class="line">model[<span class="string">'b3'</span>] = np.zeros((<span class="number">1</span>,K))</span><br><span class="line"></span><br><span class="line">(sigm_array_1, sigm_array_2, s_W1, s_W2, s_W3, s_b1, s_b2, s_b3) = three_layer_net(<span class="string">'SIGM'</span>, X, y, model, step_size=<span class="number">1e-1</span>, reg=<span class="number">1e-3</span>)</span><br></pre></td></tr></table></figure><p>training accuracy: 0.97</p><h3 id="Now-train-net-with-ReLU-nonlinearity"><a href="#Now-train-net-with-ReLU-nonlinearity" class="headerlink" title="Now train net with ReLU nonlinearity"></a>Now train net with ReLU nonlinearity</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Re-initialize model, train relu net</span></span><br><span class="line">model = &#123;&#125;</span><br><span class="line">model[<span class="string">'h'</span>] = h</span><br><span class="line">model[<span class="string">'h2'</span>] = h2</span><br><span class="line">model[<span class="string">'W1'</span>] = <span class="number">0.1</span> * np.random.randn(D, h)</span><br><span class="line">model[<span class="string">'b1'</span>] = np.zeros((<span class="number">1</span>,h))</span><br><span class="line">model[<span class="string">'W2'</span>] = <span class="number">0.1</span> * np.random.randn(h, h2)</span><br><span class="line">model[<span class="string">'b2'</span>] = np.zeros((<span class="number">1</span>,h2))</span><br><span class="line">model[<span class="string">'W3'</span>] = <span class="number">0.1</span> * np.random.randn(h2, K)</span><br><span class="line">model[<span class="string">'b3'</span>] = np.zeros((<span class="number">1</span>,K))</span><br><span class="line"></span><br><span class="line">(relu_array_1, relu_array_2, r_W1, r_W2, r_W3, r_b1, r_b2, r_b3) = three_layer_net(<span class="string">'RELU'</span>, X, y, model, step_size=<span class="number">1e-1</span>, reg=<span class="number">1e-3</span>)</span><br></pre></td></tr></table></figure><p>training accuracy: 0.9933333333333333</p><h2 id="The-Vanishing-Gradient-Issue-–-梯度消失的问题"><a href="#The-Vanishing-Gradient-Issue-–-梯度消失的问题" class="headerlink" title="The Vanishing Gradient Issue – 梯度消失的问题"></a>The Vanishing Gradient Issue – 梯度消失的问题</h2><p>我们可以对某一hidden层W的梯度（dW）进行加总，用这个简单的指标来衡量学习的速度，显然，sum(dW)越大，说明神经网络的学习速度越快。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(np.array(sigm_array_1))</span><br><span class="line">plt.plot(np.array(sigm_array_2))</span><br><span class="line">plt.title(<span class="string">'Sum of magnitudes of gradients -- SIGM weights'</span>)</span><br><span class="line">plt.legend((<span class="string">"sigm first layer"</span>, <span class="string">"sigm second layer"</span>))</span><br></pre></td></tr></table></figure><p><img src="3.svg" alt="svg"></p><p>由上图可见，第二层的梯度显著大于第一层。说明在进行back prop的时候，hidden层越多，那么排在最前的hidden层对应的梯度（dW）就会变得越来越小。直观的讲，因为chain rule的原因，hidden层越多，则chain rule里乘的local gradient越多，而另一方面，由于nonlinearity使用的是sigmod，sigmoid grad = δ*(1-δ)，输出必在[0,1]之间。所以local gradient的值都落在[0，1]，那么随着hidden层的增加，排在最前的hidden层对应的梯度（dW）就会变得越来越小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(np.array(relu_array_1))</span><br><span class="line">plt.plot(np.array(relu_array_2))</span><br><span class="line">plt.title(<span class="string">'Sum of magnitudes of gradients -- ReLU weights'</span>)</span><br><span class="line">plt.legend((<span class="string">"relu first layer"</span>, <span class="string">"relu second layer"</span>))</span><br></pre></td></tr></table></figure><p><img src="4.svg" alt="svg"></p><p>由上图可见，ReLU收敛的速度比sigmoid快很多，而且收敛后就很稳定。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># overlaying the 2 plots to compare</span></span><br><span class="line">plt.plot(np.array(sigm_array_1))</span><br><span class="line">plt.plot(np.array(sigm_array_2))</span><br><span class="line">plt.plot(np.array(relu_array_1))</span><br><span class="line">plt.plot(np.array(relu_array_2))</span><br><span class="line">plt.title(<span class="string">'Sum of magnitudes of gradients -- hidden layer neurons'</span>)</span><br><span class="line">plt.legend((<span class="string">"sigm first layer"</span>, <span class="string">"sigm second layer"</span>, <span class="string">"relu first layer"</span>, <span class="string">"relu second layer"</span>))</span><br></pre></td></tr></table></figure><p><img src="5.svg" alt="svg"></p><p>上图可以更明显的看到，ReLU的收敛速度快，一开始的gradient更高。</p><p>最后，看看2种分类器的表现，由于ReLU训练速度更快，因此用同样的epochs，ReLU表现的更好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot the classifiers -- SIGMOID</span></span><br><span class="line">h = <span class="number">0.02</span></span><br><span class="line">x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">1</span>, X[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line">y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">1</span>, X[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                     np.arange(y_min, y_max, h))</span><br><span class="line">Z = np.dot(sigmoid(np.dot(sigmoid(np.dot(np.c_[xx.ravel(), yy.ravel()], s_W1) + s_b1), s_W2) + s_b2), s_W3) + s_b3</span><br><span class="line">Z = np.argmax(Z, axis=<span class="number">1</span>)</span><br><span class="line">Z = Z.reshape(xx.shape)</span><br><span class="line">fig = plt.figure()</span><br><span class="line">plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=<span class="number">0.8</span>)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.Spectral)</span><br><span class="line">plt.xlim(xx.min(), xx.max())</span><br><span class="line">plt.ylim(yy.min(), yy.max())</span><br></pre></td></tr></table></figure><p><img src="6.svg" alt="svg"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot the classifiers-- RELU</span></span><br><span class="line">h = <span class="number">0.02</span></span><br><span class="line">x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">1</span>, X[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line">y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">1</span>, X[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                     np.arange(y_min, y_max, h))</span><br><span class="line">Z = np.dot(relu(np.dot(relu(np.dot(np.c_[xx.ravel(), yy.ravel()], r_W1) + r_b1), r_W2) + r_b2), r_W3) + r_b3</span><br><span class="line">Z = np.argmax(Z, axis=<span class="number">1</span>)</span><br><span class="line">Z = Z.reshape(xx.shape)</span><br><span class="line">fig = plt.figure()</span><br><span class="line">plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=<span class="number">0.8</span>)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.Spectral)</span><br><span class="line">plt.xlim(xx.min(), xx.max())</span><br><span class="line">plt.ylim(yy.min(), yy.max())</span><br></pre></td></tr></table></figure><p><img src="7.svg" alt="svg"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Vanishing-Gradients&quot;&gt;&lt;a href=&quot;#Vanishing-Gradients&quot; class=&quot;headerlink&quot; title=&quot;Vanishing Gradients&quot;&gt;&lt;/a&gt;Vanishing Gradients&lt;/h1&gt;&lt;p&gt;这里只使用numpy来实现一个neural network，而非借助pytorch这样的框架，如此，可以更好的帮助我理解neural network以及vanishing gradients.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Financial Machine Learning" scheme="http://yoursite.com/categories/Financial-Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Bars part 1</title>
    <link href="http://yoursite.com/2020/03/28/bars-part-1/"/>
    <id>http://yoursite.com/2020/03/28/bars-part-1/</id>
    <published>2020-03-28T09:51:32.000Z</published>
    <updated>2020-03-28T12:26:36.820Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Financial-Machine-Learning-Bars"><a href="#Financial-Machine-Learning-Bars" class="headerlink" title="Financial Machine Learning - Bars"></a>Financial Machine Learning - Bars</h1><p>最近在啃Marcos Lopez de Prado的 Advances in Financial Machine Learning，感觉很有难度，我想通过笔记的形式将自己的理解慢慢记录下来。总体讲这本书给我很多的启发。个人感觉，网上很多所谓machine learning在finance和investment上的应用都是谬误的，其实这本书并不会讨论具体的算法，而更多的是提供一种machine learning在finance 应用上的标准流程或者说是方法论，懂得了这些流程/方法论并不能保证你就能写出赚钱的模型来，但是，可以帮助你避免很多错误和陷阱，从而节约了你很多的时间。本书的观点认为，ML在投资领域的应用犹如一种工业化的生产过程，在当今市场的有效性条件下，企图靠某个个人所谓的深厚功力写出一个盈利可观的模型变得越来越不切实际了，只有依靠大量的人力，标准化的流程才可能在高度有效的市场环境下生存。</p><a id="more"></a><p>该书首先从数据准备讲起。他的观点是，如果某项数据所有人都在用，那这个数据的价值就不大了。比如财报数据，当然，传统bar（K线）数据的价值也不大，一方面人人都在用，另外一方k线数据的采样是有缺陷的。举例来说，假如我们使用5分钟线，那么通常在每天开盘和收盘的时间段成交比较密集，也就意味着这段时间包含的信息量比较多。但是，传统的k线只做定期采样，造成的结果就是，各个5分钟k线包含的信息量是不等的，导致很多的信息量无法被体现出来。为了避免这样的缺陷，该书提出了以下几种bar：</p><ul><li><p>Tick Bars</p></li><li><p>Volume Bars</p></li><li><p>Dollar Bars</p></li><li><p>Imbalance Bars</p></li></ul><p>接下来我以沪深300指数的1分钟k线为基础，分别来看这些Bar有什么区别。</p><h2 id="Time-Bars"><a href="#Time-Bars" class="headerlink" title="Time Bars"></a>Time Bars</h2><p>Time bars就是指最常见的k线了。原始数据是1分钟k线，我可以将其合成为15分钟k线</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load data</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">"hs300.csv"</span>) <span class="comment">#沪深300 1分钟k线</span></span><br><span class="line"><span class="comment"># change column name</span></span><br><span class="line">data.rename(columns=&#123;data.columns[<span class="number">0</span>]:<span class="string">"timestamp"</span>&#125;, inplace=<span class="keyword">True</span>)</span><br><span class="line">data[<span class="string">'timestamp'</span>] = pd.to_datetime(data[<span class="string">'timestamp'</span>])</span><br><span class="line"><span class="comment"># data['timestamp'] = data.timestamp.map(lambda t: datetime.strptime(t, "%Y-%m-%d %H:%M:%S"))</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_vwap</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="comment"># 因为groupby有upsampling的问题，所以，必须剔出这些额外增加出来的非交易时段的ts</span></span><br><span class="line">    <span class="keyword">if</span> df.isna().to_numpy().any():</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    q = df[<span class="string">'volume'</span>]</span><br><span class="line">    p = df[<span class="string">'close'</span>] <span class="comment"># use close price</span></span><br><span class="line">    vwap = np.sum(p * q) / np.sum(q)</span><br><span class="line">    df[<span class="string">'vwap'</span>] = vwap</span><br><span class="line">    <span class="comment"># print(df.head())</span></span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line">data_timeidx = data.set_index(<span class="string">'timestamp'</span>)</span><br><span class="line"></span><br><span class="line">sub_data = data_timeidx[<span class="string">'2015-05'</span>] <span class="comment">#选取2015年5月的数据</span></span><br><span class="line">data_time_grp = sub_data.groupby(pd.Grouper(freq=<span class="string">'15Min'</span>, closed=<span class="string">'right'</span>))</span><br><span class="line">num_time_bars = <span class="number">1</span></span><br><span class="line"><span class="comment"># 计算groupby以后一共有多少跟k线</span></span><br><span class="line"><span class="keyword">for</span> name, group <span class="keyword">in</span> data_time_grp:</span><br><span class="line">    <span class="keyword">if</span> group.empty == <span class="keyword">False</span>:</span><br><span class="line">        num_time_bars += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">data_time_vwap = data_time_grp.apply(compute_vwap)    </span><br><span class="line">sub_df = data_time_vwap.copy()</span><br><span class="line">sub_df.index = sub_df.index.map(str)</span><br><span class="line">sub_df[<span class="string">'vwap'</span>].plot()</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x180d0b81f88&gt;</code></pre><p><img src="bars_4_1.svg" alt="svg"></p><h2 id="Tick-Bars"><a href="#Tick-Bars" class="headerlink" title="Tick Bars"></a>Tick Bars</h2><p>为了避免上述Time bar的问题，一种方法是构建tick bar。 所谓tick bar是以某个固定的tick数量来切分并生成对应的bar，例如：每个tick bar包含50个tick。这样做的好显而易见，就是当市场的某些时段交易很活跃的时候，就有更多的tick bar生成出来即更多的采样，反之，生成的tick bar的数量减少。因此，相较于传统的time bar，每个tick bar包含的信息更加均衡。</p><p>但是tick bar也有一个缺点，例如：一个tick对应10手买单，和10个tick，每个tick分别对应1手买单。但是，他们包含的信息量显然不一样，简单来说，前者可能是掌握了某些信息的市场参与者的行为，而后者可能更多的是一种随机行为。按照tick bar的生成方式，前者就无法被及时采样，并容易被其他低信息量的tick淹没。</p><p>由于我的原始数据是1分钟k线而非tick数据，因此无法合成tick bar。</p><h2 id="Volume-Bars"><a href="#Volume-Bars" class="headerlink" title="Volume Bars"></a>Volume Bars</h2><p>所谓的volume bar就是以某个固定的成交量来切分并生成对应的bar。例如：每个bar包含10000股的成交量。这样就改进了tick bar存在的问题，使得每个volume bar所含的信息量更为平均</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">data_cm_vol = sub_data.assign(cmVol=sub_data[<span class="string">'volume'</span>].cumsum())  <span class="comment">#计算总成交量</span></span><br><span class="line">total_vol = data_cm_vol.cmVol.values[<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># 令volume bar的数量和time bar的数量相同，便于显示的时候做比较</span></span><br><span class="line">vol_per_bar = total_vol / num_time_bars</span><br><span class="line">vol_per_bar = round(vol_per_bar, <span class="number">0</span>) </span><br><span class="line">data_vol_grp = data_cm_vol.assign(grpId=<span class="keyword">lambda</span> row: row.cmVol // vol_per_bar)</span><br><span class="line">data_vol_vwap =  data_vol_grp.groupby(<span class="string">'grpId'</span>).apply(compute_vwap)</span><br><span class="line"><span class="comment"># plot</span></span><br><span class="line">sub_df1 = data_vol_vwap.copy()</span><br><span class="line">sub_df1.index = sub_df1.index.map(str)</span><br><span class="line">plot_data = &#123;<span class="string">'time'</span>: sub_df[<span class="string">'vwap'</span>], <span class="string">'volume'</span>:sub_df1[<span class="string">'vwap'</span>]&#125;</span><br><span class="line">plot_df = pd.DataFrame(plot_data)</span><br><span class="line">plot_df.plot(alpha=<span class="number">0.6</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x180ce3cf788&gt;</code></pre><p><img src="bars_7_1.svg" alt="svg"></p><p>由上图可见volume bar在一些波峰和波谷的位置更加的显著，说明在偏极端的行情的环境下，volume bar保存的信息更多。如下所示的放大图。</p><img src="/2020/03/28/bars-part-1/enlarge1.PNG"><h2 id="Dollar-Bars"><a href="#Dollar-Bars" class="headerlink" title="Dollar Bars"></a>Dollar Bars</h2><p>同Volume bar类似，所谓的Dollar bar就是以某个固定的成交金额来切分并生成对应的bar。例如：每个bar包含100000元。一个比较直观想法是：当上证综合指数在6000点买入100手股票同在1000点买入100手股票是完全不同的概念，由于在资金量上的巨大的差异，虽然手数相同，但是这两个决定所包含的信息应该是差异很大。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">data_cm_dollar = sub_data.assign(cmDollar=sub_data[<span class="string">'total_turnover'</span>].cumsum()) </span><br><span class="line">total_dollar = data_cm_dollar.cmDollar.values[<span class="number">-1</span>]</span><br><span class="line">dollar_per_bar = total_dollar / num_time_bars</span><br><span class="line">dollar_per_bar = round(dollar_per_bar, <span class="number">0</span>) </span><br><span class="line">data_dollar_grp = data_cm_dollar.assign(grpId=<span class="keyword">lambda</span> row: row.cmDollar // dollar_per_bar)</span><br><span class="line">data_dollar_vwap =  data_dollar_grp.groupby(<span class="string">'grpId'</span>).apply(compute_vwap)</span><br><span class="line"><span class="comment"># plot</span></span><br><span class="line">sub_df2 = data_dollar_vwap.copy()</span><br><span class="line">sub_df2.index = sub_df2.index.map(str)</span><br><span class="line"></span><br><span class="line">plot_data = &#123;<span class="string">'dollar'</span>: sub_df2[<span class="string">'vwap'</span>], <span class="string">'volume'</span>:sub_df1[<span class="string">'vwap'</span>]&#125;</span><br><span class="line">plot_df = pd.DataFrame(plot_data)</span><br><span class="line">plot_df.plot(alpha=<span class="number">0.6</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x180d1e57988&gt;</code></pre><p><img src="bars_10_1.svg" alt="svg"></p><p>上图是dollar bar 和 volume bar的比较。区别不是很大。可以看出，在一些比较极端的走势图形上，dollar bar和volume bar的采样还是有一定区别。如下图所示：</p><img src="/2020/03/28/bars-part-1/enlarge2.PNG"><p>除了以上这些bar外，还有一种bar叫做Imbalance bar，由于相对比较复杂，所以会单独开一篇来介绍。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Financial-Machine-Learning-Bars&quot;&gt;&lt;a href=&quot;#Financial-Machine-Learning-Bars&quot; class=&quot;headerlink&quot; title=&quot;Financial Machine Learning - Bars&quot;&gt;&lt;/a&gt;Financial Machine Learning - Bars&lt;/h1&gt;&lt;p&gt;最近在啃Marcos Lopez de Prado的 Advances in Financial Machine Learning，感觉很有难度，我想通过笔记的形式将自己的理解慢慢记录下来。总体讲这本书给我很多的启发。个人感觉，网上很多所谓machine learning在finance和investment上的应用都是谬误的，其实这本书并不会讨论具体的算法，而更多的是提供一种machine learning在finance 应用上的标准流程或者说是方法论，懂得了这些流程/方法论并不能保证你就能写出赚钱的模型来，但是，可以帮助你避免很多错误和陷阱，从而节约了你很多的时间。本书的观点认为，ML在投资领域的应用犹如一种工业化的生产过程，在当今市场的有效性条件下，企图靠某个个人所谓的深厚功力写出一个盈利可观的模型变得越来越不切实际了，只有依靠大量的人力，标准化的流程才可能在高度有效的市场环境下生存。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Financial Machine Learning" scheme="http://yoursite.com/categories/Financial-Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>AdaBoost Example</title>
    <link href="http://yoursite.com/2020/02/23/AdaBoost-Example/"/>
    <id>http://yoursite.com/2020/02/23/AdaBoost-Example/</id>
    <published>2020-02-23T14:08:03.000Z</published>
    <updated>2020-02-26T08:50:10.899Z</updated>
    
    <content type="html"><![CDATA[<p>我终于想到自己应该滚过来更新了。实在是因为考完CFA以后，整个人生好像没有了明确的目标，导致自己什么都想做，结果什么都做不好。这段时间自己的状态一直保持低迷，原因肯定是对自己各种的放纵。前阵子，德约科维奇又拿下来澳网的冠军，我就顺便看了一本他的自传，印象最深的是极度的自律。在拿到某次冠军后（好像也是澳网），他说他想吃一块巧克力庆祝一下（因为平时绝对不会碰这种东西，怕影响状态），结果就吃了一小口。只有能做到这样的自律，才能取得不平凡的成绩。</p><p>说了一堆废话，还是言归正传。前段时间正好用到random forrest就想顺便把AdaBoost也看一下，李航《统计学习方法》里有一个例子，但是我怎么都没看懂（智商不够用，顺便吐槽一下这本书，虽然很多人推荐，但我一直没觉得这本书好在哪里），于是就到网上找例子，到油管上看视频，结果发现能把AdaBoost讲明白的例子几乎没有。后来不知道哪天突然开悟了，李航那本书上的例子竟然被我看明白了！</p><a id="more"></a><p>我就直接上例子了，AdaBoost具体的理论还请自行脑补。训练数据如下表：</p><table><thead><tr><th>序号</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th></tr></thead><tbody><tr><td>x</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td></tr><tr><td>y</td><td>1</td><td>1</td><td>1</td><td>-1</td><td>-1</td><td>-1</td><td>1</td><td>1</td><td>1</td><td>-1</td></tr></tbody></table><p>x是特征，y是label。显然根据特征x对y分类，分为2种类型1，-1。我一开始一直没搞明白评价分类结果依据什么？后来才理解了评价分类的依据是各训练数据的权值！我们知道Adaboost会将分类错误的训练数据的权值提高，这样，在下一次弱分类器的训练过程中就可以重点“关照”这些被分错的数据了。那么，也就有可能出现以下这种情况：某个弱分类器可能错误率很高但却被采纳了，因为它可能将权重更高的训练数据进行准确地分类。参见以下训练过程</p><h3 id="m-1-训练第一个弱分类器"><a href="#m-1-训练第一个弱分类器" class="headerlink" title="m=1 (训练第一个弱分类器)"></a>m=1 (训练第一个弱分类器)</h3><p>对于第一个弱分类器而言，每个训练数据的权值w都是一样的，即1/N（初始情况下的默认值），N表示训练数据量。就本例而言，N=10，每个训练数据的权值w为0.1，在此前提下可以训练出一个最优的弱分类器：</p><p>$$<br>    G_1(x) =<br>    \begin{cases}<br>    1,  &amp; \text{x $&lt;$ 2.5} \\<br>    -1, &amp; \text{x $&gt;$ 2.5} \\<br>    \end{cases}<br>$$</p><p>切分点为x=2.5，因为误差率（分错权值加总）最低，注意这里需要强调的是选择弱分类器的依据是误差率最低的那个，而不是分对的数量最多的那个（当然，权值相同的时候分对数量最高则代表误差率最低），我一开始没看明白就是在这里犯糊涂了。那么有3个数据被错误分类了（x=6，7，8），由于w都为0.1，因此误差率$e_1$（分错权值加总）=0.3。这个误差率是当前权值（w）下最小的。</p><p>计算第一个弱分类器的系数（权重），套用公式了，$ \alpha_1 = \frac{1}{2}log\frac{1-e_1}{e_1} $ = 0.5*ln((1-0.3)/0.3) = 0.4236 ； 直观的理解就是如果这个分类器的误差率越低，则在一系列弱分类器的组合里所占的权重越高。</p><p>更新训练数据的权值，也就是将分对的数据的权值调低，而将分错的数据的权值调高，计算权值的公式为：$w_{m+1, i} = w_{m, i}exp(-\alpha_m y_i G_m(x_i))$，计算结果如下：</p><table><thead><tr><th>序号</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th></tr></thead><tbody><tr><td>y</td><td>1</td><td>1</td><td>1</td><td>-1</td><td>-1</td><td>-1</td><td>1</td><td>1</td><td>1</td><td>-1</td></tr><tr><td>$w_1$</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td></tr><tr><td>$G_1(x)$</td><td>1</td><td>1</td><td>1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td></tr><tr><td>$ \alpha_1 $</td><td>0.4236</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>$w_2$ without normalized</td><td>0.06547</td><td>0.06547</td><td>0.06547</td><td>0.06547</td><td>0.06547</td><td>0.06547</td><td>0.15275</td><td>0.15275</td><td>0.15275</td><td>0.06547</td></tr><tr><td>Z</td><td>0.9165</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>$w_2$ normalized</td><td>0.07143</td><td>0.07143</td><td>0.07143</td><td>0.07143</td><td>0.07143</td><td>0.07143</td><td>0.16667</td><td>0.16667</td><td>0.16667</td><td>0.07143</td></tr></tbody></table><p>以计算第一个$w_2$为例，原来的$w_2=0.1, \alpha=0.4236, y=1, G_1(x_1)=1$，也就是分类正确，那么可得: 0.1 * exp(-0.4236 * 1 * 1) = 0.06547<br>，标准化后为0.07143（$w_2$ 需要标准化，也就是$w_2$的sum为1，表中的Z为所有w的加总，那么normalized w = w without normalized / Z），因此权重降低了！。而对$x_7, x_8, x_9$来说，由于分错了，因此权重升高了。至此第一个弱分类器训练完成，可得：<br>$$<br>f_1(x) = 0.4236G_1(x)<br>$$</p><h3 id="m-2-训练第二个弱分类器"><a href="#m-2-训练第二个弱分类器" class="headerlink" title="m=2 (训练第二个弱分类器)"></a>m=2 (训练第二个弱分类器)</h3><p>通过训练可得x=8.5最为最佳切分点，因为误差率最小，即：<br>$$<br>    G_2(x) =<br>    \begin{cases}<br>    1,  &amp; \text{x $&lt;$ 8.5} \\<br>    -1, &amp; \text{x $&gt;$ 8.5}<br>    \end{cases}<br>$$<br>$G_2(x)$的误差率为$e_2 = 0.2143$。因为当以8.5为切分点的时候，4，5，6被错误分类了，而他们对应的权重$w_2$ 分别为：0.07143，0.07143，0.07143，即 0.07143 * 3 = 0.2143。如果选择8.5以外的切分点，误差率都比0.2143高。</p><p>同样的计算$\alpha_2 = 0.5*LN((1-0.2143)/0.2143) = 0.6496$</p><p>其余数据参见下表：</p><table><thead><tr><th>序号</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th></tr></thead><tbody><tr><td>y</td><td>1</td><td>1</td><td>1</td><td>-1</td><td>-1</td><td>-1</td><td>1</td><td>1</td><td>1</td><td>-1</td></tr><tr><td>$w_2$</td><td>0.07143</td><td>0.07143</td><td>0.07143</td><td>0.07143</td><td>0.07143</td><td>0.07143</td><td>0.16667</td><td>0.16667</td><td>0.16667</td><td>0.07143</td></tr><tr><td>$G_2(x)$</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>-1</td><td>-1</td></tr><tr><td>$ \alpha_2 $</td><td>0.6496</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>$w_3$ without normalized</td><td>0.0373</td><td>0.0373</td><td>0.0373</td><td>0.1368</td><td>0.1368</td><td>0.1368</td><td>0.0870</td><td>0.0870</td><td>0.0870</td><td>0.0373</td></tr><tr><td>Z</td><td>0.8207</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>$w_3$ normalized</td><td>0.0455</td><td>0.0455</td><td>0.0455</td><td>0.1667</td><td>0.1667</td><td>0.1667</td><td>0.1061</td><td>0.1061</td><td>0.1061</td><td>0.0455</td></tr></tbody></table><p>至此第二个弱分类器训练完成，可得：<br>$$<br>f_2(x) = 0.4236G_1(x) + 0.6496G_2(x)<br>$$</p><p>如果，此时将数据喂给$f_2(x)$，可得以下结果：</p><table><thead><tr><th>序号</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th></tr></thead><tbody><tr><td>x</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td></tr><tr><td>y</td><td>1</td><td>1</td><td>1</td><td>-1</td><td>-1</td><td>-1</td><td>1</td><td>1</td><td>1</td><td>-1</td></tr><tr><td>$f_2(x)$</td><td>1.0732</td><td>1.0732</td><td>1.0732</td><td>0.226</td><td>0.226</td><td>0.226</td><td>0.226</td><td>0.226</td><td>0.226</td><td>-1.0732</td></tr><tr><td>$sign(f_2(x))$</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>-1</td></tr></tbody></table><p>由y同$sign(f_2(x))$比较可知，一共有3个误分类点：4，5，6。</p><h3 id="m-3-训练第三个弱分类器"><a href="#m-3-训练第三个弱分类器" class="headerlink" title="m=3 (训练第三个弱分类器)"></a>m=3 (训练第三个弱分类器)</h3><p>通过训练可得x=5.5最为最佳切分点，因为误差率最小，即：<br>$$<br>    G_3(x) =<br>    \begin{cases}<br>    1,  &amp; \text{x $&gt;$ 5.5} \\<br>    -1, &amp; \text{x $&lt;$ 5.5}<br>    \end{cases}<br>$$<br>$G_3(x)$的误差率为$e_3 = 0.1818$。因为当以5.5为切分点的时候1, 2, 3, 10被错误分类了，而他们对应的权重$w_3$ 分别为：0.0455, 0.0455, 0.0455, 0.0455，即 0.0455 * 4 = 0.1818。</p><p>同样的计算$\alpha_3 = 0.5*LN((1-0.1818)/0.0.1818) = 0.7520$</p><p>至此第三个弱分类器训练完成，可得：<br>$$<br>f_3(x) = 0.4236G_1(x) + 0.6496G_2(x) + 0.7520G_3(x)<br>$$</p><p>如果，此时将数据喂给$f_3(x)$，可得以下结果：</p><table><thead><tr><th>序号</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th></tr></thead><tbody><tr><td>x</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td></tr><tr><td>y</td><td>1</td><td>1</td><td>1</td><td>-1</td><td>-1</td><td>-1</td><td>1</td><td>1</td><td>1</td><td>-1</td></tr><tr><td>$f_3(x)$</td><td>0.3212</td><td>0.3212</td><td>0.3212</td><td>-0.526</td><td>-0.526</td><td>-0.526</td><td>0.978</td><td>0.978</td><td>0.978</td><td>-0.3212</td></tr><tr><td>$sign(f_3(x))$</td><td>1</td><td>1</td><td>1</td><td>-1</td><td>-1</td><td>-1</td><td>1</td><td>1</td><td>1</td><td>-1</td></tr></tbody></table><p>由y同$sign(f_3(x))$比较可知已经无错误分类点！因此，可以认为达到了精度要求，无需再进一步训练。</p><p>以上就是对李航《统计学习方法》里关于AdaBoost例子的详细解释，作此笔记以备今后复习。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我终于想到自己应该滚过来更新了。实在是因为考完CFA以后，整个人生好像没有了明确的目标，导致自己什么都想做，结果什么都做不好。这段时间自己的状态一直保持低迷，原因肯定是对自己各种的放纵。前阵子，德约科维奇又拿下来澳网的冠军，我就顺便看了一本他的自传，印象最深的是极度的自律。在拿到某次冠军后（好像也是澳网），他说他想吃一块巧克力庆祝一下（因为平时绝对不会碰这种东西，怕影响状态），结果就吃了一小口。只有能做到这样的自律，才能取得不平凡的成绩。&lt;/p&gt;
&lt;p&gt;说了一堆废话，还是言归正传。前段时间正好用到random forrest就想顺便把AdaBoost也看一下，李航《统计学习方法》里有一个例子，但是我怎么都没看懂（智商不够用，顺便吐槽一下这本书，虽然很多人推荐，但我一直没觉得这本书好在哪里），于是就到网上找例子，到油管上看视频，结果发现能把AdaBoost讲明白的例子几乎没有。后来不知道哪天突然开悟了，李航那本书上的例子竟然被我看明白了！&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Lasso Regression Notes</title>
    <link href="http://yoursite.com/2019/09/08/lasso-regression-notes/"/>
    <id>http://yoursite.com/2019/09/08/lasso-regression-notes/</id>
    <published>2019-09-08T09:48:58.000Z</published>
    <updated>2019-09-08T12:16:28.512Z</updated>
    
    <content type="html"><![CDATA[<p>这周读了一篇论文《基于LASSO和神经网络的量化交易智能系统构建》，借机看了一下Lasso的概念。做点笔记以备复习。</p><a id="more"></a><p>Lasso 全称Least absolute shrinkage and selection operator。主要的作用是regularization，也就是防止overfitting。通常用在linear regression和logistic regression里。说到regularization，另一种方式是ridge regression。两种表现形式如下：</p><ul><li>Ridge:<img src="/2019/09/08/lasso-regression-notes/ridge.PNG"></li></ul><ul><li>Lasso:<img src="/2019/09/08/lasso-regression-notes/lasso.PNG"></li></ul><p>Ridge和Lasso最重要的区别是，Ridge regression只能将不相关variables的beta减小至接近0；而Lasso regression则可以将beta减小至0。另外，需要注意的是Ridge Regression是无偏的，而Lasso则是有偏的（biased）。</p><p>那为何要用lasso呢？由于Lasso的特性–能够将无关variables的beta减少至0，因此它具有feature selection的能力。它能自动识别出哪些variables对模型的预测起到作用。</p><p>下面这副图解释了为何Lasso能将无关variables的beta减少至0。</p><img src="/2019/09/08/lasso-regression-notes/lasso_ridge.png"><p>右边是Ridge regression，左边是Lasso regression。等高线的中点β hat代表了最优拟合点，但是，这个点通常是会造成overfitting的。为了避免overfitting，必须对beta的取值做出限制。也就是蓝色的区域。由图可见，Lasso区别Ridge的地方在于Lasso的beta取值范围是一个正方形，而Ridge则是圆形。在加入了regularization后的最优点一定落在红色等高线和蓝色区域相切的位置。而Lasso的切点一般会是正方形的顶点。也就是说，某些variables的beta取值为0。因此，Lasso具有feature selection的能力。</p><p>在看下图：</p><img src="/2019/09/08/lasso-regression-notes/lasso-reduce.png"><p>横轴代表了λ（hyper parameter）的取值范围，纵轴代表了variables的数量。从图上可以看到，在λ取值较小的时候，所有的variables都包括在了模型内，而随着λ的增加，模型里的variables不断减少。因此，当遇到variables非常多的情况（模型复杂），Lasso是可以帮助识别出哪些variables对模型影响最大。</p><p>如何确定λ的值？通常使用cross validation的方式。根据不同的λ在cross validation set上所得到的误差来确定合理的值。</p><p>最后还是要吐槽一下国内博士的水平。个人觉得这篇论文质量不怎么样，也许是期刊《投资研究》本身水平不行？该博士第一步用linear regression + lasso 来选择feature（各种股票技术指标），第二步，这些通过lasso筛选出来的featrue被用于神经网络。最后得出的结论是lasso + 神经网络在预测股市涨跌方面的效果最好，sharpe ratio最高。我能看到的一个明显的逻辑上的问题是lasso是用在linear regression上的，也就是说这一步feature selection体现的线性相关性。而神经网络可用于非线性关系的发掘。那么在第一步中筛选出来的是线性显著的技术指标，这一步很可能把非线性的关系给过滤掉了。然后在第二步中，把这些线性显著的技术指标在输入给神经网络，那是希望从已知的线性关系中进一步发掘非线性关系？！觉得逻辑上挺混乱的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这周读了一篇论文《基于LASSO和神经网络的量化交易智能系统构建》，借机看了一下Lasso的概念。做点笔记以备复习。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>CFA Level III 准备过程 【2019年6月】</title>
    <link href="http://yoursite.com/2019/08/25/How-did-I-prepare-CFA-level-III/"/>
    <id>http://yoursite.com/2019/08/25/How-did-I-prepare-CFA-level-III/</id>
    <published>2019-08-25T09:48:33.000Z</published>
    <updated>2019-08-25T12:14:00.304Z</updated>
    
    <content type="html"><![CDATA[<p>很幸运一次性过了CFA level III, 一二三级连续一次过关，真的是上天保佑了。</p><a id="more"></a><p>先说说考试本身。上午是essay，下午同1，2级一样是选择题。相对一二级来说，我三级的准备不是太充分。有一部分事先准备的复习资料最后也是没有时间去看了。因此上考场的时候心里就有点虚的。上午的essay可谓是悲惨啊，一上来就卡住了，结果花了1个小时前2题都没做完，当时心里就毛了，然后开始赶时间，后面也是有几个地方做不出，最后一大题彻底没时间了，只完成了第一个小问题。考完当时的第一反应就是：我也算是坚持到最后一秒了，没有放弃！但是，心里觉得这次考试很玄了。整个上午可以用梦游或魂不守舍来形容。中午休息的时候，依旧看到很多同学拿着资料在复习，我对这些人的敬佩之心如滔滔江水。已经考了3个小时了，还不休息一下？！下午还有3个小时啊。。。这时候还在复习，下午还有精力吗？反正我是觉得中午还是不要看什么资料了，好好休息休息，迎接下午的考试。下午进入考场的时候，发现原本坐我右边的同学撤了。。。我猜估计上午也遭到打击了！下午的难度明显低了很多，做题的速度因此变的很快。零星碰到一点有难度的题目，反正不是很多。因此，下午整个人的状态就很放松，不如上午来的那么崩溃了。不过还是受到了打击，还是源自上午的essay！我下午做题的时候，突然发现，计算器上计算PV的模式设置成了annuity due!!!当时如五雷轰顶啊～～～上午有2题是需要用到pv计算的，而且只需要普通的模式就行了，当时两眼一黑，想完了，上午肯定是一塌糊涂了。然后我也只能调整心态，不去想这件事了。所以，各位同学，必须要吸取我的教训，一定注意自己计算器的pv模式是否设置对了。</p><h2 id="关于复习资料"><a href="#关于复习资料" class="headerlink" title="关于复习资料"></a>关于复习资料</h2><p>老规矩，官方教材我还是老老实实看了一遍的。不过ethics我这次直接放弃了。因为，我在level II的时候花了很多的时间去复习这部分，结果成绩还是很差！这次我干脆就放弃了。ethics的内容相当多，handbook就有100多页，我觉得通读对提高我的成绩没有帮助，性价比太低了，因此3级复习ethics直接忽略，只是做书后习题+mock上的题目。事实证明这么做是对的，因为，成绩出来ethics考的不错。官方教材的所有习题我做了2变。</p><p>然后的Notes，依旧从网上下的:(  看了一遍。有些章节看了多变。这里需要指出的是3级的复习其实是有陷阱的。粗看好像3级没啥特别难的地方，看教材，看Notes看完了觉得好像不难，但是，看完了又觉得自己好像也没记住什么东西，再看一遍的时候，觉得好像新的内容一样。我觉得之所以会有这样的情况和三级的侧重点有关系。1，2级的考点其实很具体，尤其涉及计算的，一旦你掌握了，基本不会忘记。但是，3级以portfolio manager的视角，关注的是如何给client定制资产管理的方案，里面的知识点都是围绕这个主题。相对high level，概念性的东西偏多。因此，造成了说看完一遍感觉不难，但是，要记住这些概念，并能理解使用这些概念那就非常难了！一方面是内容多，一方面是要会应用，真的不容易的。我到后期觉得再通看notes并不会给我带来多少提升，所以还是以刷题为主。</p><p>再次是Mock题目了，同2级一个道理，历年Mock题目的重复率太高了（下午考试部分），我从网上找了2010年开始的历年mock，做了几套，觉得重复率太高了，再这么做下去没啥意义。而且难度差异巨大。因此，个人觉得Mock的成绩没太多参考价值。我的建议是，Mock题目如果时间充裕那就做，如果不充裕的，那还是挑近几年的做一下了。2019年的mock题目我觉得超级难。。。</p><p>再后是essay真题了（CFA网站提供近3年的），这块强烈建议大家重点关照！！！我找了从2005年开始的essay真题。由于复习时间不够用了，只做了1遍，再回扫了一部分。个人觉得做一遍肯定不够的。因为，essay是新题型，常考的范围是哪些，如何组织你的答案，这些都需要不断练习的。切记不是说你写的多了就安全了！这种想法是危险的，首先是时间，你会发现essay的时间非常紧张的，历年的真题我也基本来不及做，到了考场，我也依旧来不及，最后一大题没时间做。其次，答案如果回答不到点，不给分的。其实essay重点就是回答要简洁，切题！语法无所谓。我建议历年真题最好能反复做个3遍，很有用。虽然不会考同样的题目，但是，真题的难度毋庸置疑，真题的答案是需要自己好好分析总结的，因此绝对有价值。</p><p>最后，我这次花了血本，用了200元买了Kaplan出的practice exam！一共4套题目。我个人也是推荐的。题目的质量真心不错，难度接近考试。更重要的是，答案里提供了分值的计算方式。比如，essay的分值是如何计算的。这对我考试有很大的帮助。有些题目最终答案我不知，但是，我知道把一些相关的分析和计算写上去肯定也是得分的。复习资料就这些了。</p><h2 id="复习"><a href="#复习" class="headerlink" title="复习"></a>复习</h2><p>我没上任何的复习班，还是坚持自己的观点：没必要（估计那些培训机构要吐血了）。时间一定要留足。尤其尤其是刷题的时间！！！一个月刷题是不够的！！！我只留了1.5个月来刷题，事实证明是不够的。essay历年真题最好能做个3遍，Kaplan practice exam至少2遍，再加历年mock选择题，1个月怎么也不够的。试想，一天3个小时复习，也就够做一份essay或下午的选择题（而且3小时肯定不够）。再加上对答案，分析，总结至少2个小时，也就是说一份考卷需要差不多5个小时。光essay就有10多份！需要自己一开始就能估计好刷题的时间。切记留足刷题的时间。</p><p>通读教材和notes的效果不好，有时间可以多读几遍，没时间的话还通过刷题来找自己的弱项吧！</p><p>和1，2级一样，强调笔记的重要性！我的那本笔记本真的是被我翻烂了，literally！</p><h2 id="考试"><a href="#考试" class="headerlink" title="考试"></a>考试</h2><p>这块大家经历了1，2级都经验很丰富了，也没啥好多写的。重点还是合理安排答题时间。我这次essay来不及就是因为一开始的2道大题卡住了。当时，没有很好的调整自己的心态，事后觉得应该马上跳过，做后面。但是，考场上，头脑一热，觉得自己肯定能做出来啊！就一直卡在那里。也有部分原因是因为新题型，如果是选择题，可能也就跳过了。但是，碰到这种新题型，脑子就有点发晕了。</p><p>带个耳塞，因为二级的时候有惨痛的教训，很奇怪，早上开考以后，外面会有隐隐约约广播的声音，严重影响我集中注意力，可能我对噪音比较敏感。这次带了3M的耳塞，效果还不错。由于早上essay是直接做在考卷上的，周围都是翻页的声音，带了耳塞感觉安静很多。</p><h2 id="心路历程"><a href="#心路历程" class="headerlink" title="心路历程"></a>心路历程</h2><p>还是非常幸运的，自己能一口气连过3级！你要问我难不难，我的回答是难！难在哪里？未必说考试本身有多难。但是，要在近3年的时间里，有工作的情况下，每天还需要留出2，3个小时复习，并且是专注地复习，这真的是太难了！！！2，3级准备期间都发生了很多分心的事情，2级的时候差点崩溃，literally，之前很难理解为啥有些人会因为一件事情变成疯子或是自杀，但是考2级的时候接二连三的打击，真真切切感受到自己不堪再承受的临界点，因此也可以理解那些做出极端行为的人，他们的行为真的已经不受自己控制了。3级的阻碍也不少，工作上的烦心事、家人住院、自己又忙装修，还有自己的健康。各种烦心的事情接二连三，导致了复习断断续续。那时内心很挣扎，到底该怎么办？其他准备考试的人肯定比我复习的好很多。后来我对自己说，即使牛人碰到这些事情肯定也是会受影响的，因此我复习受到影响也是很正常的，不能要求自己在这样的客观条件下还能保持专注高效的复习，因此，我可以允许我自己在种情况下不坚持复习，因为即使坚持也没有啥效率。那就干脆安安心心地处理这些烦心事，等处理完成了再回来继续专注复习。鼓励自己不要放弃。也保持弹性。这种情况下复习到哪里是哪里，但是keep walking不要放弃！这里必须感谢家人尤其是妻子。没有她的默默支持，我根本不可能完成考试。自己在压力之下也会脾气失控，更要感谢妻子和家人的包容。</p><p>越是临近考试，越感到自己濒临极限了。有那么几次周末根本连书都不想翻，一开始复习就想吐了。真是强弩之末了。因此干脆也就只能看看电视休息休息。最后两周请假在家复习，也是碰到意外事件，丈母娘紧急送医院。可能是压力太多或是神经衰弱，睡眠也开始出现了问题，只能靠吃安眠药。我承受压力的能力就是这么差。</p><p>这段时间自己一直在调整恢复，也在思考，考CFA的整个过程带给我什么？我想整个过程让我更清楚地知道了我是谁。近3年的压力，回看自己的表现，更清楚自己几斤几两，例如承受压力的能力，体力，智力乃至情商。从而能让自己更加心平气和，接受自己的平庸。清楚看到自己和牛人之间的差距之大。因此，也就不应该对自己有不切实际的期望和要求。就这点来看整个过程还是很值的。在收到考试成绩之前，我也在思考，如果没过要不要继续？我自己也没想好。觉得如果继续，那是有点浪费时间了，因为，我可以将这些时间做更有意义的事情上去，如管理自己的portfolio，看看我有兴趣的书。考完以后，整个人的状态自由落体。什么都不想做，什么都不想看。幸运的是自己通过了level 3，菩萨保佑！接下来我想多花些时间在家人身上，也要规划一下该如何前行了，keep walking！</p><p>最后祝大家复习顺利，考试成功！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;很幸运一次性过了CFA level III, 一二三级连续一次过关，真的是上天保佑了。&lt;/p&gt;
    
    </summary>
    
    
      <category term="CFA" scheme="http://yoursite.com/categories/CFA/"/>
    
    
  </entry>
  
  <entry>
    <title>CFA Level II 准备过程 【2018年6月】</title>
    <link href="http://yoursite.com/2018/08/16/How-did-I-prepare-CFA-level-II/"/>
    <id>http://yoursite.com/2018/08/16/How-did-I-prepare-CFA-level-II/</id>
    <published>2018-08-15T16:00:00.000Z</published>
    <updated>2018-08-25T10:13:21.403Z</updated>
    
    <content type="html"><![CDATA[<p>很幸运一次过了CFA level II，算下来差不多一年半的时间连续过了levelI和level II, 接下来的目标就是level III了，不过说实话觉得好累，对于level III好像没有那么高的passion了，真想好好休息一年。因为考level II的时候真的感觉压力太大了，压力主要来自要同时处理很多的事情。如今准备考试，不像在学校，可以安心看书。我需要同时面对很多事情，不断切换，而且其他事情会影响心情，每次都需要逼着自己去集中注意力在CFA复习上，那是非常非常疲劳和折磨的，而且还是一个长期的过程。我不像网上的有些人花3，4个月就能搞定，我没那么大能力，必须拉长准备的周期。</p><a id="more"></a><p>我差不多从9月份就开始准备了。先说说CFA 2级的难度。我一开始也看到网上考过的人说过二级比一级难很多很多很多，甚至有说难10倍的。当时我觉得那些人就是夸张，能有多难呢？后来自己开始准备的时候就闷逼了。真的是很难很难很难！而且我觉得即使没有难10倍，但难个5倍也是有的！！！所以这里需要提醒大家，如果你觉得自己基础很一般的话，那么真的要多留一些时间的！</p><p>同level I一样，level II我也没参加学习班，我的观点就是没有意义，你也并不知道这些所谓的学习班水平到底如何。而且说白了教材就在那，考试范围也都明确写出来了，我更相信自己。CFA范围很广，靠猜题我更是觉得不靠谱。教材方面依旧是官方的电子版，我成了IPAD的重度用户，说个励志的事情：被我看坏一本IPAD！！！哈哈，是不是真的被我看坏的我不知道，但是确确实实坏掉了，所以，大部分level II的教材阅读是在新的IPAD上完成，够励志吧！教材之外是notes。我是教材看完一遍，一些对我来说很难的地方，教材是看了多遍的，比如Financial Reporting。Notes更是看了无数遍了。。。如果大家没有时间的话那就看Notes吧，很精简。</p><p>二级从教材的内容角度来看，量化部分难度增加了不少，比如如何判断数据间是否相关，如果判断是否有异方差，如何判断unit root，如何判断serial correlation，如何做F test，autoregressive （AR）model等。总之，这部分我自己笔记记了一大把。economics里面汇率的三角套利。corparate finance里关于项目的评估，这部分level I里很简单，就是比较NPV，但是level II里，计算项目的NPV就复杂的多了。Financial reporting这部分我自己觉得是难度增加的最高的一块了。这些topic对我来说都是很高级的，包括了公司间投资，比如一家公司投资另一家公司，可以有三种类型，每种类型在报表上该如何体现。employee compensation是关于年金如何计算并体现在财报上的。还有一大块是关于跨国公司如何将不同的币种收入计入在报表上的。总之，财务报表分析这部分我一直是觉得很迷茫，看了无数遍的教材和notes，慢慢的消化理解。equity这块是level II的大头，但是其实这块从概念上来说没什么新东西，增加了一些新的价值估算的方法，比如free cash flow valuation，residule income valuation等。剩下的fix income, derivatives, alternative investment和portfolio management都难，真的！反正就是不断重复的看notes。</p><p>这里想插一些学习CFA，尤其是level II以后的感想。网上有说CFA没用的。但是，我自己的感觉就是CFA的内容编排贴近实际，对实践很有帮助。尤其是财务报表，corparate finance和equity，对于理解实际股票估值很有帮助。比如我现在看《巴菲特致股东信》就觉得很有收获。看一些价值投资方面的书籍，比如《巴菲特之道》，《聪明的投资者》等觉得也容易理解。总之，我觉得通过学习CFA，对我在理解投资方面的帮助是很明显的，虽然过程很枯燥。CFA关于量化和Portfolio Management方面的知识更是对我有特别的帮助，因为我自己走的是量化的方向，对资产配置的兴趣大过个股分析，我觉得CFA上的这些知识极大的提高了在阅读一些论文和量化专业书籍上的效率。可以这么说，没学CFA之前，我看《QUANTITATIVE EQUITY PORTFOLIO MANAGEMENT》(作者Chincarini)这本书的时间没啥特别感觉，很多东西无法深入理解，但是学了CFA二级以后，再看这本书，真切感觉到对书里的内容理解更深刻了。而且，自己也有机会接触了更多的portfolio management的内容，比如，我也实践了risk parity。可以说，没有在CFA上所投入的时间和精力，就不可能让我形成自己的投资理念，也不会让我更好地看懂各类投资方面的书籍包括量化和价值投资！</p><p>说一下Ethical部分的复习，handbook我看了大概有2～3遍了，但是这次考二级得了个C！我觉得Ethical部分投入的时间是性价比最低的。如果我知道自己花了那么多的精力和时间还是得了C，那我一定是会将时间用到其他部分的。所以，我觉得，对于时间不够的朋友们来说，Ethical不如放弃了。真的性价比太低了。</p><p>还是强调自己的笔记的重要性，将你不懂的部分，容易做错的题目等一一记录到你的笔记本上，我记了有50多页啊！这些是你后期复习的重点。CFA内容那么多，想要从头到尾多看几遍显然不现实，最后阶段就是挑自己最没有把握的部分重点加强，而你的笔记能让你有的放矢。一定一定要记笔记。</p><p>关于考试时间，我上下午都没来得及做完所有题目，尤其是上午，最后一个case就做了一题，其它就只能涂成b应付了事了。下午有2题来不及，也用了同样的方法。而我在做mock的时候基本上能剩余20分钟样子，所以，我上考场的时候有意放慢了节奏，希望自己不要粗心大意，出现低级失误。但是，实际考试的题目难度和mock比，我自己觉得并不会低的。所以，还是要控制自己的节奏，尽量做的快一点，遇到需要思考的题目，或明显没有思路的题目应该先跳过。我没有这么做，导致最后15分钟的时候很慌乱，有些题目根本就没有精力去应付了，反而会造成失误，把一些应该会做的题目都做错了，让我后悔不已！如果最后能有30分钟的时间来处理一些前面遗留的题目，那么心态上就不一样了，放松会让自己更好的来应对这些题目。我在level I的时候就是这样，效果很好。但是这次真的是做的很不好。不过也要提醒一句，那就是二级的题目确实是有难度的。</p><p>准备CFA二级的时候，我也收集了历年mock题目（从2010年开始），原本想复制一级时候的策略。但是，结果告诉我，这样做的效果不大。首先，很明显level I的时候每一年有240题，从概率角度来说题型，知识点会覆盖的比较全面，因此将此作为复习重点是事半功倍的！但是到了二级，情况不一样了，level II每一年只有120题，而知识点和level I比起来一点也不少，甚至更多。所以从覆盖知识点和题型的角度来说肯定没有level I的mock来的效果好。其次，更要命的是，level II每年的mock题目大部分都是相同的！！！所以，我最后还是去重新做了一遍教材上的习题。</p><p>最后，还是应该感谢家人，尤其是妻子的支持与鼓励。今年18年真的是比较难熬的一年，就在考试前一段时间，甚至是离考试只剩下一周的时间，各种烦心事，打击，分心的事情，让我真的有濒临崩溃的感觉。有了妻子和家人的支持，让我熬过来了，至少我走进了考场，完成了考试！很幸运，最终也是过了。我想说，很多事情是必须要熬的，就是坚持。第一遍看教材还觉得可以学到很多新的东西，但是后面就是不断重复，很枯燥，我时常觉得是浪费时间，有这些时间是不是可以做更有意义的事情呢？比如看更多quant方面的书。但是，我后来觉得，其实就是因为这样的重复看教材，看notes，做题，才让我把很多知识掌握扎实。而光靠看一遍教材，很多地方其实是不能理解的。花掉的这些时间其实是在帮我打基础，是非常值得的。制定目标之前先把意义之类的事情想清楚了，最好能写下来，目标一旦定了，就不要犹豫怀疑，想东想西了。就是埋头干，直到达成目标。中间会有怀疑，犹豫和放弃的念头。我觉得有这些想法是很正常的，反正我是有的。比如，得知炒币的都是什么一两百倍的收益，那我是不是该去炒币啊，或者一边复习一遍炒币呢？哈哈！但是，冷静下来想想，我想还是CFA对于我来说更有意义，因此，也就放弃了“加入”币圈的念头。我不知道这样的决定是否正确，但是，我知道，我的能力只适合一步一步走，一口一口吃饭。既要。。。又要。。。还要。。。，臣妾做不到啊！</p><p>每个人情况都不一样，大家还是要结合自己的实际情况，找到最适合自己的复习CFA level II的方法。最后，祝大家复习顺利，考试成功！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;很幸运一次过了CFA level II，算下来差不多一年半的时间连续过了levelI和level II, 接下来的目标就是level III了，不过说实话觉得好累，对于level III好像没有那么高的passion了，真想好好休息一年。因为考level II的时候真的感觉压力太大了，压力主要来自要同时处理很多的事情。如今准备考试，不像在学校，可以安心看书。我需要同时面对很多事情，不断切换，而且其他事情会影响心情，每次都需要逼着自己去集中注意力在CFA复习上，那是非常非常疲劳和折磨的，而且还是一个长期的过程。我不像网上的有些人花3，4个月就能搞定，我没那么大能力，必须拉长准备的周期。&lt;/p&gt;
    
    </summary>
    
    
      <category term="CFA" scheme="http://yoursite.com/categories/CFA/"/>
    
    
  </entry>
  
  <entry>
    <title>Trump税改的宏观分析</title>
    <link href="http://yoursite.com/2017/12/07/Trump-tax-reform/"/>
    <id>http://yoursite.com/2017/12/07/Trump-tax-reform/</id>
    <published>2017-12-07T13:21:53.000Z</published>
    <updated>2017-12-07T13:25:05.320Z</updated>
    
    <content type="html"><![CDATA[<p>每次市场上出现一些热点事件后，通常都是众说风云，自己看了这些新闻，以及网上大V们的论述之后往往觉得是一头雾水，也没有办法分清楚谁对谁错。最近想想，觉得其实自己有武器却没有利用起来啊，毕竟也是学过宏观经济学的人，教材里面清清楚楚的把分析框架提供给我了，而我却视而不见，反倒是迷失在各种标题党中。想想觉得自己也是蛮愚蠢的。情愿相信不可靠的消息和评论，却不依靠教材上提供的可靠的框架。经常听见说教材上的东西都是过时的，没有实际作用的，我自己也很轻易就相信了。但是，稍微分析一下：首先：教材上的东西都是经典，是经过时间积累下来的精华部分，如果没有用怎么会写进教材呢？其次，这些经典理论的作者都是大家，比我要聪明无数倍了，我没有理由不去使用他们。最后，反省一下我看了那么多所谓大V的文章有收获和进步么？</p><a id="more"></a><p>Trump的税改于2017/12/02日在参议院通过。税改的主要目的就是减税，而减税作为一项财政政策，会对宏观经济，包括：GDP，利率，汇率，通胀，股市造成哪些影响呢？接下来就根据教材上提供的框架来一一分析之。</p><p>宏观分析的时间纬度是一个非常重要的着眼点，很多时候大家为了某些观点争论不休，其原因可能就是考察的时间纬度不同。拿长期的观点和短期的观点进行比较显然会造成各说各的。那么，首先从短期的时间纬度进行分析，从短期来看，需求决定了产出（短期究竟是多久？答案是不一定，有可能几个月，也有可能几年，主要看决定长期的因素–price level有没有开始发生改变，如果price level开始改变，那么短期的作用也就over了）。刚才提到了减税是一项财政政策，能刺激需求端。使用短期的分析框架：IS-LM模型，如下图所示：</p><img src="/2017/12/07/Trump-tax-reform/IS-LM.png"><p>从上图可见，因为短期内由于减税，消费者的可支配收入增加，进而增加消费，通过乘数效应进一步增加产出。也就是说IS curve右移至IS’。那么，在来看LM curve，从美联储的货币政策来看，当前处在加息通道中，那么从保守的角度来说，LM保持不变，而如果美联储在2018年继续加息的话，其实LM是该向上平移的。由此，我们从IS-LM模型可知：产出（GDP）会比当前增加，而利率会上升，而利率上升会导致货币升值，也就是美元汇率也上涨。</p><p>接下来分析一下减税的长期情况。这里有一个概念叫做“natural level of output”。从短期来看output是可以偏离natural level of output的，但是长期来说是要回归的，因为price level是会做出调整的。之所以会调整是因为，劳动力市场本身也存在一个natrual rate of umemployement，由于短期的财政政策的刺激导致失业率低于这个natrual rate, 使得工资上升，从而导致物价上升。从长期角度来说，可以使用AS-AD模型进行分析。影响AS curve主要有以下这些因素：</p><ul><li>工资水平；这块要关注的是工资增长率，如果工资增长速度加快，那么就会影响到price level，使得AS curve向上平移。目前来看美国工资的增长相对平稳，所以多AS curve的影响不大。</li><li>原油价格；这一项其实应该算做是短期因素，原油价格最近上涨较快，就原油价格对AS curve影响主要考虑原油价格的上涨是永久的还是短期波动。我更倾向于是一种短期的波动，那么对AS的影响不会很大。</li><li>汇率；这项是短期因素还是长期因素呢？如果产生一个趋势，那么影响的时间就会比较长了。从短期分析可知，美元汇率是会受到减税的影响而上涨，那么，上涨的美元会对美国的price level产生抑制作用。</li></ul><p>另外，还要提一下财政赤字，此次降税如果没有伴随财政紧缩，所以会造成财政赤字上升。财政赤字的上升会影响消费者的预期从而影响到AD curve。但是，现在似乎大家都不关心这事情，一则是因为相信Trump的供给侧改革会成功，要么就是今朝有酒今朝醉，因此，财政赤字对AD curve的影响不明朗。</p><p>综上，工资水平使得AS curve轻微向上平移，原油价格使得AS curve轻微向上平移，汇率使得AS curve轻微向下平移。总体来说，AS会向上，但是程度很小。如下图所示：</p><img src="/2017/12/07/Trump-tax-reform/AS-AD.png"><p>再从open economy的角度来分析：减税会刺激国内的需求，但是这些新增的需求并非会全部购买国内的商品和服务，有部分会去购买国外的产品和服务，所以进口增加而出口不变，因此贸易赤字增加，另外，对国内的产出（GDP）的刺激效果也相应的减弱了。之前在短期情况的分析可知利率会走高，汇率也会走高而通胀保持稳定，那么美国的金融市场会比较有吸引力。为了保证减税的效果，从美国的角度来说，还是希望美元不要太强势。</p><p>综合以上的分析，可得出美国会在一段时间内维持一个高于natural level of output。也就是相对高的增长，而通胀相对比较温和。所以可以得到以下这些结论：</p><ul><li>利率会稳步抬高，对债市而言是利空；</li><li>股市依旧有机会，崩盘的可能性近期不大，下跌仍旧是买入机会；</li><li>商品：由于经济继续增长对商品是利好，另一方面通胀压力不大，所以预期不会有很大的波动行情；</li><li>美元汇率：温和上涨，因为处于加息通道，并且经济持续增长；</li><li>黄金：震荡向下。作为零息债券，随着实际利率的抬升，不看好它的表现。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;每次市场上出现一些热点事件后，通常都是众说风云，自己看了这些新闻，以及网上大V们的论述之后往往觉得是一头雾水，也没有办法分清楚谁对谁错。最近想想，觉得其实自己有武器却没有利用起来啊，毕竟也是学过宏观经济学的人，教材里面清清楚楚的把分析框架提供给我了，而我却视而不见，反倒是迷失在各种标题党中。想想觉得自己也是蛮愚蠢的。情愿相信不可靠的消息和评论，却不依靠教材上提供的可靠的框架。经常听见说教材上的东西都是过时的，没有实际作用的，我自己也很轻易就相信了。但是，稍微分析一下：首先：教材上的东西都是经典，是经过时间积累下来的精华部分，如果没有用怎么会写进教材呢？其次，这些经典理论的作者都是大家，比我要聪明无数倍了，我没有理由不去使用他们。最后，反省一下我看了那么多所谓大V的文章有收获和进步么？&lt;/p&gt;
    
    </summary>
    
    
      <category term="Macro" scheme="http://yoursite.com/categories/Macro/"/>
    
    
  </entry>
  
  <entry>
    <title>风险评价策略（Risk Parity)基准指数</title>
    <link href="http://yoursite.com/2017/10/16/rp-benchmark-index/"/>
    <id>http://yoursite.com/2017/10/16/rp-benchmark-index/</id>
    <published>2017-10-16T13:09:07.000Z</published>
    <updated>2017-10-16T13:12:09.203Z</updated>
    
    <content type="html"><![CDATA[<p>6月份考完CFA leve I 就打算用R实现一个Risk Parity的策略，原本觉得很快就能搞定，但是没想到越写东西越多，细节的问题不断浮出水面，直到最近才完成了大部分的功能。写了1000多行的R代码，本篇就是梳理自己在编程过程中碰到的一些问题和想法。如果想了解Risk Parity量化的方法可以参考之前的《<a href="http://www.keepswalking.com/2017/07/23/risk-parity-quantitative-fundamental/" target="_blank" rel="noopener">Risk Parity 量化入门</a>》</p><a id="more"></a><h2 id="最根本的想法：为什么要实现一个Risk-Parity，或为什么Risk-Parity能实现收益？"><a href="#最根本的想法：为什么要实现一个Risk-Parity，或为什么Risk-Parity能实现收益？" class="headerlink" title="最根本的想法：为什么要实现一个Risk Parity，或为什么Risk Parity能实现收益？"></a>最根本的想法：为什么要实现一个Risk Parity，或为什么Risk Parity能实现收益？</h2><p>我自己主要是参考了Ray Dalio关于全天候基金的一些介绍，觉得很有道理。主要有以下几点：</p><ul><li>长期来看，beta也就是资产的收益一定是超过现金的。</li><li>理由是因为，其一，这是有资本主义制度本质决定的，资金总是流向能产生更高收益的地方。而那些能产生正收益的人或项目就会借入资金，而投资人则会借出资金获取收益。显然，资本市场就是投资人和项目汇集地。当然，有些制度下未必如此。。。</li><li>其二，风险应该获得对应的补偿。简而言之就是高风险高收益。</li></ul><p>对于我自己而言，我需要一个benchmark，来衡量自己投资的绩效。同时，这个基准指数，也是我用来观察市场的一个很好的工具。因此，我觉得实现这个基准指数是一件很有意义的事情。</p><h2 id="为什么不去追求alpha？"><a href="#为什么不去追求alpha？" class="headerlink" title="为什么不去追求alpha？"></a>为什么不去追求alpha？</h2><p>我觉得是一个因人而异的事情。alpha是零和游戏，你获得的alpha其实别人的loss，从这点来看就知道获取alpha的竞争激烈，也就是说，你必须比别人在某些方面强，你才能从他人身上抢到钱。明白了这点，就需要对自身进行一些必要分析，自己到底在哪些方面比别人强？要长期跑赢市场是一件几乎不可能完成的任务！在这个市场里面，有人靠基本面分析，有人靠技术分析，有的靠行为心理。。。但是，我相信这个市场的效率会越来越高，越来越逼近semi-strong efficient market。秉持这个观点，再加之我自己并没有什么过人之处，那么，我应该安心于获取beta，或者说，要获取alpha我会借助别人的能力，比如选则基金。这里我想多说一句，对于大部分散户而言，不要去走所谓的基本面这条路，因为其实这条路是相当昂贵的，光从数据角度来看，要获取和机构对等的数据就是一件不可能的事情！机构可以获取各类研究报告，机构有专职人员看某个行业，当需要了解行业细节时候，随时有人可以说清楚，他们甚至可以花钱用卫星来数船。</p><p>以上的大原则指导了我努力的方向。那么，要获取beta，搞个股票指数基金定投也ok啊，为啥要用Risk Parity呢？如果你能承受得起指数的波动并能坚持到底，我想也是一条可选则的路。就我自己而言，我相信分散投资，而Risk Parity能做到比指数基金更好的分散投资，因此也就实现了更低的波动。</p><h2 id="Risk-Parity的组合里面该包括什么品种？"><a href="#Risk-Parity的组合里面该包括什么品种？" class="headerlink" title="Risk Parity的组合里面该包括什么品种？"></a>Risk Parity的组合里面该包括什么品种？</h2><p>我主要是根据Edward E. Qian的那本 Risk Parity Fundmentals来选择品种。大类上来讲包括了以下三种：</p><ul><li>interest risk premium，也就是债券了，其实主是指利率债。长期来看，投资者通过承担利率风险（比如买入长期国债）获得利率的风险溢价。</li><li>equity risk premium，也就是股票了，长期来看投资者通过投资股票必然可以获得高于无风险利率的收益。其实从微观的角度来看，企业主通过股票市场融资，通过持续经营，获得高于融资成本的收益。</li><li>inflation risk premium，也就是商品，长期来看商品可以对冲通货膨胀。</li></ul><p>而这三大类资产类别本身的相关性不高，所以是比较理想的构建portfolio的要素。比如，在高通胀时期，股票和债券的表现不好，但是，商品的表现会比较突出；而在经济低迷的时期，债券的表现要好于股票和商品。需要说明的是以上三大类都包含了“长期来看”这个前提，因为从某个时间段来说，会出现三类资产都在下跌的情况。但是从长期来看，这3类资产的期望收益都是正的。</p><h2 id="都有哪些资产入选到Risk-Parity基准指数里？"><a href="#都有哪些资产入选到Risk-Parity基准指数里？" class="headerlink" title="都有哪些资产入选到Risk Parity基准指数里？"></a>都有哪些资产入选到Risk Parity基准指数里？</h2><p>刚一开始我并不想加入太多的资产品种，我想从简单的开始，逐步完善，而基准指数也是我观察市场的一个窗口。我初步的想法是在大类的risk parity下包含小类的risk parity。大类上已经提到了要包括债券，股票和商品。而每个大类下，我想再通过跨国配置做进一步的risk parity，目的是将风险分散的更彻底。那么需要包括哪些国家呢？我想以：</p><ul><li>消费国：美国，欧洲</li><li>生产国：中国</li><li>资源国：澳洲</li></ul><p>这样的分类来选择国家，之所以这样想，是因为，通常这些国家在同一时间会处于不同的经济周期上，因此，也就能更好的分散风险。所以，最后每个大类都会包含这些国家的资产。例如，在股票的大类下会分别包含：美国、欧洲、中国和澳洲的股票，而这些不同国家的股票之间也要实现risk parity。以下就是目前包括的基准指数里包含的资产：</p><ul><li>股票<ul><li>美国：S&amp;P500 - 标普500指数(Total Return)</li><li>欧洲：德国DAX指数，本来想用stoxx 50的，但是不确定stoxx是否是total return（有谁知道麻烦告知一下），而DAX指数本身就是total return的方式。</li><li>中国：沪深300指数，但是找不到沪深300的total return指数，所以就直接用沪深300EFT（510310）并使用后复权获得最终的数据。不过300EFT历史太短了，导致我很难做回测，我就用50ETF接300ETF，这么处理一下后就有比较长的历史数据了！</li><li>澳洲：ASX200 - 澳洲200指数（Total Return）</li></ul></li><li>债券<ul><li>美国：Thomson Reuters US 10 Year Government Benchmark - 美国10年期债券指数 （目测应该是total return）</li><li>欧洲：S&amp;P Eurozone Sovereign Bond 7-10 Years Index （total return）</li><li>中国：Shanghai SE Treasury Bond - 国债指数 （目测应该是total return）</li><li>澳洲：S&amp;P/ASX Government Bond 5-10 Year Index (Total return)</li></ul></li><li>商品，对于商品而言，并没有国别的区分，主要考虑到国内市场和国际市场会有不同步的现象，所以分别使用两个商品指数。同时，将黄金单列出来，我的想法是黄金本身是对冲通胀的一种工具，同时她兼具一种信用保障，比如她一种避险资产，因此将她单列出来，目的是能起到更好的分散风险的作用。<ul><li>黄金 - 国内黄金现货价格 </li><li>国际商品：S&amp;P GSCI 商品指数（total return）</li><li>国内商品：通达信期货通里面有一个大宗商品指数（T001），不清楚是谁编制的，暂时就先用这个指数吧！这个指数应该不是total return。不过影响应该不会太大。</li></ul></li></ul><p>还是要提一句数据收集，没想到花掉那么多精力，之前也没有考虑指数是否是total return。随着测试的开始，渐渐意识到这个问题。而我自己也没有什么好的数据终端（花不起这个钱啊！）好在investing.com上的数据还比较全面。后来又发现了<a href="http://www.spindices.com这个网站，他是标普的一个关于指数汇编的网站，提供了非常全面的指数品种，还能下载数据，不过普通用户只能回溯10年的数据。看来以后花钱买数据的必然的了。回测的过程中发现了很多数据上的问题，需要自己不断地对数据进行修正，数据的准备和清理觉绝对是一件耗时耗精力的事情，远远超出了自己的预期。" target="_blank" rel="noopener">www.spindices.com这个网站，他是标普的一个关于指数汇编的网站，提供了非常全面的指数品种，还能下载数据，不过普通用户只能回溯10年的数据。看来以后花钱买数据的必然的了。回测的过程中发现了很多数据上的问题，需要自己不断地对数据进行修正，数据的准备和清理觉绝对是一件耗时耗精力的事情，远远超出了自己的预期。</a></p><h2 id="资金管理"><a href="#资金管理" class="headerlink" title="资金管理"></a>资金管理</h2><p>本系统使用一个很简单的资金管理模型：1倍恒定杠杆（1份本金+1份融资）。也就是在任何情况下保持1倍左右的杠杆。从资金管理的角度来看，重点关注是否会爆仓。那么1倍杠杆爆仓会发生在所持的portfolio瞬时下跌50%。这种可能性从一个交易日的时间窗口来看几乎是不可能发生的。因为portfolio本身是分散投资的，包括了债券、股票和商品。另外由于是恒定1倍杠杆的算法，当资产价格下跌，导致超过1倍杠杆时就会触发系统卖出所持资产保证杠杆维持在1倍。</p><h2 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h2><p>以下罗列一些实现上的细节：</p><ul><li>Risk Parity everywhere。不光是在资产大类上实现Risk Parity，在子类上也实现Risk Parity，例如在股票大类里面可以分为，中国股票，美国股票，德国股票等，那么这些品种也会是Risk Parity。 再下一层，比如中国股票里面可以分创业板，上证50，那么仍旧是Risk Parity。 而我编写的R程序可以自动完成这些工作，我所需要做的仅仅是写出一个资产配置的树形结构。</li><li>Rebalance 策略。我没有使用定期Rebalance的策略，而是针对每个品种设置一个threadhold。超过了就进行Rebalance。这个threadhold是根据每个品种的return std(standard deviation)计算出来的。</li><li>correlation matrix不是静态的，而是会变动的，目前的策略就是根据最近3~5年的数据进行调整。判断correlation matrix是否需要更新的标准是同上次使用的matrix做比较，如果发生统计意义上的改变，则correlation matrix需要更新，那么对应的，各类资产的比重也都需要调整。R程序里使用biotools包的boxM方法来进行判断。</li><li>计算各类资产子Risk Parity下的比重的时候，需要用到优化包，R程序里使用的是BB包里面的spg方法。她能模拟Excel里面的规划求解（solver）。</li><li>计算指数的时候考虑到了交易费用和借款利息。交易费用目前固定为1/1000，借款利息使用的是14天回购利率（204014）。不清楚这样是否合理。</li><li>将所有国外的资产价格换算成RMB（CNY），因为，在计算correlation的时候，必须用同种货币才有意义。</li></ul><h2 id="下一个阶段需要做哪些方面的改进？"><a href="#下一个阶段需要做哪些方面的改进？" class="headerlink" title="下一个阶段需要做哪些方面的改进？"></a>下一个阶段需要做哪些方面的改进？</h2><p>目前我最希望能加入的是timing model，也就择时调整个资产的比重。一个直观的想法是，如果债券处在高位了，那么我希望在Risk Parity里面减少债券的比重。同样，如果商品处在低位，那么我希望能增加该品种的比重。看了一些资料，目前的方向是引入momentum/trend following的方法，对各个品种的比重进行调整。当然也有很多的细节问题需要考虑，比如：当某个品种加速上冲的时候，也往往是最后一波行情了，比如2015年6月份的股灾。那么，这个时候，momentum/trend following并不能指出这种风险。这就需要设计一个指标，根据历史数据的统计加入判断，比如乖离率的统计。具体我也没有想清楚。有时候觉得，既然是一个benchmark，就不应加入这些目的是为了增加alpha的东西。而这些可以加入到我自己的真实的投资组合当中去。</p><h2 id="回测演示"><a href="#回测演示" class="headerlink" title="回测演示"></a>回测演示</h2><p>最后就显示一下自己做的Risk Parity benchmark index的资金曲线。如下图所示：</p><img src="/2017/10/16/rp-benchmark-index/rp_ts.jpg"><p>红色是1倍杠杆的净值曲线，黑色是S&amp;P500 total return并换算成人民币。下面针对回测结果做一下说明和总结：</p><ul><li>首先，还是有点出乎意外，竟然能和S&amp;P500差不多打平，要知道我才用了1倍的杠杆。</li><li>该净值曲线是在扣除了交易费用之后获得的。不过没有计算融资费用，也就是1倍杠杆的融资费用。这算是小小耍流氓。会在下次回测的时候做一下灵敏度的测试，看看不同的融资费用和交易费用会对净值造成多大的影响</li><li>资产分配的权重演变来看，初始权重（不包括杠杆，也就是总计为1的情况下），最初债券的权重为74%，演变到后来债券的权重为85%以上，几乎就是一只债券基金了。主要的原因就是债券超低的波动率，尤其是上证国债的超低波动率（几乎就是一根直线），再加上全球央行的大放水，造成了债券的超低波动率，这就直接导致了在Risk Parity中，债券占比的大幅提高。一个值得思考的点就是，如果债券波动率突然增加，就会对现有的配置造成很大的冲击。是否应该给每个大类设置设置一个配置的上限呢？</li><li>其实，这个Risk Parity的组合是可以通过增加杠杆来提高收益的。因为，通过数据可以看到，未加杠杆的情况下（总计权重为1）的Risk Parity的standard deviation和S&amp;P500 total return (人民币计价)的standard deviation的比为：1：5.2。也就是说，如果将Risk Parity的波动率调成和S&amp;P500一致的话，可以用4倍杠杆！而目前只用了1倍杠杆。所以，在实际运行Risk Parity的时候，也可以考虑动态杠杆。</li><li>Risk Parity回测结果的Annualized Sharpe Ratio为2.13。很不错的值，不过考虑到其实她几乎就是一只债券基金，这个sharp ratio也算靠谱。</li><li>最后提一下08年金融危机时的表现，从净值曲线的表现来看是远好于S&amp;P500的，这得益于危机期间债券，黄金和其他资产之间的负相关性。</li></ul><p>参考文献：</p><ul><li>Edward E. Qian, Risk Parity Fundmentals</li><li>Bridgewater, Engineering targeted return &amp; risks</li><li>Bridgewater, Our thoughts about Risk Parity and All Weather</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;6月份考完CFA leve I 就打算用R实现一个Risk Parity的策略，原本觉得很快就能搞定，但是没想到越写东西越多，细节的问题不断浮出水面，直到最近才完成了大部分的功能。写了1000多行的R代码，本篇就是梳理自己在编程过程中碰到的一些问题和想法。如果想了解Risk Parity量化的方法可以参考之前的《&lt;a href=&quot;http://www.keepswalking.com/2017/07/23/risk-parity-quantitative-fundamental/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Risk Parity 量化入门&lt;/a&gt;》&lt;/p&gt;
    
    </summary>
    
    
      <category term="Risk Parity" scheme="http://yoursite.com/categories/Risk-Parity/"/>
    
    
  </entry>
  
  <entry>
    <title>随笔如何准备CFA Level I 【2017年6月】</title>
    <link href="http://yoursite.com/2017/07/26/How-did-I-prepare-CFA-level-I/"/>
    <id>http://yoursite.com/2017/07/26/How-did-I-prepare-CFA-level-I/</id>
    <published>2017-07-26T12:54:48.000Z</published>
    <updated>2017-07-26T13:52:06.551Z</updated>
    
    <content type="html"><![CDATA[<p>今天早收到email，一看开头是congratulations我就激动的叫起来了，过了！说一下我的背景供大家参考，我在校期间是非金融专业，工作也和金融没半毛关系。纯粹是对这投资有兴趣。个人觉得CFA level I还是有难度的，并非网上看到的牛人们，花3，4个月轻松就过。 当然，每个人的情况不同，所以，还是要根据自己的客观情况来制定复习策略。</p><a id="more"></a><p>虽然我非金融专业，但是在考CFA之前的近2年时间里面，几乎把CFA所涉及的范围都看到了，说来也很奇怪，我在涉及这些内容的时候根本没想到要去考CFA，只是觉得要学投资的话就需要这些知识，到后来听说CFA，打算去考的时候，才发现原来自己之前看的东西和CFA都非常的切合，这也从另外一个侧面说明了CFA确实是系统的覆盖了投资的方方面面。</p><p>接下来说说我是怎么准备CFA level I的考试的， 关于教材，我就直接用的电子档。CFA报名后就可以下载。我没有买什么纸质的教材，而且觉得也没有这个必要。我也没有上任何的课程或看视频，我也觉得没这个必要。我复习的材料就是官方电子版教材+notes（电子版）。</p><p>我一共复习了6个月，一开始的5个月每天平均2个小时，最后1个月每天至少3，4个小时。</p><p>电子版教材有6本，我是从头到尾看过一遍的。这里的一些建议是：</p><ul><li>顺序就按照官方给的顺序来吧，我觉得这个顺序蛮好的</li><li>每个章节的题目一定要做，而且要把错误的题目记录下来，这些错题代表了你对这些知识点不熟悉，所以要准备一个笔记本，专门将这些薄弱的知识点记录下来，这样在后期复习的时候会非常有帮助！！！</li></ul><p>看完了教材之后，我又看了一遍notes，notes一共5本。看notes的主要目的，一是复习，二是notes会针对考点来编写内容，这样的编排对考试很有帮助。看notes的时候也是要发现自己薄弱环节，将这些点记录下来。</p><p>看完了notes以后，我觉得自己在Financial Reporting &amp; Analysis, Corporate Finance方面还是比较弱，所以又看了一遍这些topic的notes。</p><p>在最后的一个月中就是题海大战了，除了昨天还是做题。个人觉得这样做题的效果是相当显著的！这里需要强调一下，我用的题目都是官方历年的模拟题，不建议大家去找什么非官方的题目。一张试卷就有120道题目，一套模拟题又有2张试卷（上下午），所以一套题目就有240题，我做了2008到2017年的所有的模拟题。我觉得这些题目足以覆盖绝大部分的题型了！从统计角度来看也应该是这样了。事实也证明这样的题海练习还是蛮有用的。错的题目要记录，要反复看。</p><p>Ethical &amp; Professional Standards对于我来说是相当有难度的，每次模拟如果是新的题目，一般都做的不理想。这个没有办法，只能把handbook多看几遍，当然越多越好了。但是，我复习的时候这块没安排好，就看了一遍多一点。不过，最后分数上看还算满意。</p><p>正式考试分上下午，各3个小时，对体能也是相当大的考验，建议带些硬糖，这是被允许的。巧克力是不行的，坐我旁边一个小姑娘带了巧克力，被没收了，结果考试时看我吃糖。。。红牛太大瓶了，喝了会小便，还是用小瓶的功能性饮料吧。我喝力保健。中午最好自己带个三明治。这样方便又节约时间。</p><p>关于考试时间是否来得及的问题，我觉得应该来得及。正式考试不如模拟考那么难，基本上做完应该是没啥问题的。建议一路扫下去，一看不会的或比较耗时间的题目先放掉，回过头来再做。我用的是这样的策略。</p><p>复习的时候，我觉得Financial Reporting &amp; Analysis里面关于tax，deffered tax asset/liability很难，像我这种非专业人士真的很难理解这些点，而官方的教材写的也不是很容易明白，所以，我自己花了不少时间在网上找相关的材料。</p><p>还要提一下阅读速度，我觉得挺关键的，我自己阅读速度很慢，看教材就花了大量的时间。所以，我现在也在努力想办法提高自己的阅读速度。</p><p>准备CFA是一个漫长的过程，一路走来肯定不容易，大家也要学会调节自己的心态。尤其碰到不顺利的时候，还是尽量保持耐心。当然，实在看不下去的时候放松一下也没啥坏处，关键还是要坚持。</p><p>我自己也要感谢家人，尤其是我的妻子给我的支持。也祝各位走在CFA这条路上的朋友能顺利过关。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天早收到email，一看开头是congratulations我就激动的叫起来了，过了！说一下我的背景供大家参考，我在校期间是非金融专业，工作也和金融没半毛关系。纯粹是对这投资有兴趣。个人觉得CFA level I还是有难度的，并非网上看到的牛人们，花3，4个月轻松就过。 当然，每个人的情况不同，所以，还是要根据自己的客观情况来制定复习策略。&lt;/p&gt;
    
    </summary>
    
    
      <category term="CFA" scheme="http://yoursite.com/categories/CFA/"/>
    
    
  </entry>
  
  <entry>
    <title>Risk Parity 量化入门</title>
    <link href="http://yoursite.com/2017/07/23/risk-parity-quantitative-fundamental/"/>
    <id>http://yoursite.com/2017/07/23/risk-parity-quantitative-fundamental/</id>
    <published>2017-07-23T06:20:59.048Z</published>
    <updated>2017-07-26T12:50:23.458Z</updated>
    
    <content type="html"><![CDATA[<p>摘要：本文论述了Risk Partity的基本概念，并介绍了如何通过量化的方法构建Risk Parity的Portfolio，并以excel作为工具实现一个简单的Risk Parity的应用。</p><p>关键字：Risk Parity， Risk contribution</p><a id="more"></a><p>“不要将鸡蛋放在同一个篮子里”告诉我们要分散风险，个人觉得是一句无比智慧的话。在没有修炼成巴菲特之前我觉得还是要牢牢记住这句话。如何做到分散风险？60%的股票+40%的债券算是分散风险吗？不妨做一个情景假设，你有100w元资金，60w买入股票ETF，40w买入债券ETF，持有    1年后，假如股票下跌了10%，而债券收益2%，那么你的投资总额变成了60w<em>0.9+40w</em>1.02=95.88；相当于总资产减值了4.14%，这似乎是不太理想的，尤其是在股票和债券的correlation还是负相关的情况下。在以上情形下，如果加大债券的配置那么整个portfolio就会更加的平稳。那么接下来的问题就是该如何分配股票和债券的权重？</p><h2 id="Risk-Contribution"><a href="#Risk-Contribution" class="headerlink" title="Risk Contribution"></a>Risk Contribution</h2><p>单个risk asset可以用variance来度量风险，而要计算一个投资组合的variance的话还需要知道convariance。假设这些数据是已知的。</p><h3 id="组合中包括2种asset的情况："><a href="#组合中包括2种asset的情况：" class="headerlink" title="- 组合中包括2种asset的情况："></a>- 组合中包括2种asset的情况：</h3><p>还是假设60%股票+40%债券，并进一步假设股票的volatility是15%，债券的volatility是5%，correlation为0.2。那么根据公式：</p><img src="/2017/07/23/risk-parity-quantitative-fundamental/1.png"><p>就可以计算出组合的variance：</p><img src="/2017/07/23/risk-parity-quantitative-fundamental/2.png"><p>有了组合的variance，又该如何计算股票和债券对该组合variance的占比呢？观察以上的计算式，第一项可归因到股票，第三项可归因到债券。那么第二项（convariance）该如何归因呢？其实就是将一半归因到股票另一半归因到债券！<br>由此可知，股票的 risk contribution:</p><img src="/2017/07/23/risk-parity-quantitative-fundamental/3.png"><p>债券的risk contribution：</p><img src="/2017/07/23/risk-parity-quantitative-fundamental/4.png"><p>有了这些数据，接下来就能很容易计算出股票和债券在组合variance中的占比，也就是risk contribution的占比。<br>股票的风险占比：</p><img src="/2017/07/23/risk-parity-quantitative-fundamental/5.png"><p>债券的风险占比：</p><img src="/2017/07/23/risk-parity-quantitative-fundamental/6.png"><p>从以上数据就可以清晰的看出60/40其实是一种非常不平衡的组合，其中股票的risk contribution高达92%，也就是说在60/40的组合里面，风险主要集中在股票上。</p><h3 id="组合中包括3种asset的情况："><a href="#组合中包括3种asset的情况：" class="headerlink" title="- 组合中包括3种asset的情况："></a>- 组合中包括3种asset的情况：</h3><p>再来讨论一下组合中持有3中资产的情况，因为3中资产的情况更具一般性，可以推广到持有更多资产的情况。假设新增资产为商品，volitility 和 correlation见下表：</p><img src="/2017/07/23/risk-parity-quantitative-fundamental/7.png"><p>并假设按照40%的股票，40%的债券，20%的商品来分配。根据以下公式可以获得组合的variance：</p><img src="/2017/07/23/risk-parity-quantitative-fundamental/8.png"><p>以本例的3种asset来说，可以获得如下的计算公式：</p><img src="/2017/07/23/risk-parity-quantitative-fundamental/9.png"><p>代入上表中的数据，可得出计算出组合的variance: 0.00778<br>接下来要求每一项资产的risk contribution, 可以通过如下方式获得：</p><img src="/2017/07/23/risk-parity-quantitative-fundamental/10.png"><p>第一项是variance, 第二和第三项分别对应covariance的一半。根据以上算式可计算出：</p><ul><li>债券的risk contribution：0.00044；占比：0.00044/0.00778=6%</li><li>股票的risk contribution：0.0044；占比：0.00444/0.00778=57%</li><li>商品的risk contribution：0.0029；占比：0.0029/0.00778=37%</li></ul><p>以上数据显示，40%的债券对应的risk contribution只是6%，而股票和商品的risk contribution则显得太高了。</p><h2 id="Risk-Parity"><a href="#Risk-Parity" class="headerlink" title="Risk Parity"></a>Risk Parity</h2><p>Risk Parity (也叫risk budgeting)，是通过调整各asset在组合中所占的比重以实现个asset的risk contribution相同或等于指定的数值。</p><h3 id="组合中包括2种asset的情况：-1"><a href="#组合中包括2种asset的情况：-1" class="headerlink" title="- 组合中包括2种asset的情况："></a>- 组合中包括2种asset的情况：</h3><p>这种情况很简单了，不需要考虑2种asset的correlation, 只需要比较他们的volitility就行了，再使用之前的例子，因为股票的volatility是15%，债券的volatility是5%，那么，通过让债券的volatility对应的金额增至现在的3倍即能实现股票和债券的volatility对应的金额相等。从组合比重的角度来说，也就是1%的股票要对应3%的债券，所以，组合的权重就为：75%的债券 + 25%的股票。</p><h3 id="组合中包括3种asset的情况：-1"><a href="#组合中包括3种asset的情况：-1" class="headerlink" title="- 组合中包括3种asset的情况："></a>- 组合中包括3种asset的情况：</h3><p>这种情况会比上面复杂，因为没有直接的计算方法，需要通过数值分析法来获得近似解。这里通过excel solver举例，如下图所示：</p><img src="/2017/07/23/risk-parity-quantitative-fundamental/11.png"><p>最终的解为：股票18%，债券68%，商品14%。接着对excel的输入输出做一些说明。绿色部分是correlation matrix，作为输入参数。红色部分是solver的“可变单元格”，也就是最终的输出（参见下图）。“约束条件”设置成“总计”=1 并勾选“非负数”（参见下图）。棕色部分即risk contribution是根据组合的比重计和correlation matrix算出来的，比如股票部分的计算设置成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">=(C2^2*F6^2+C2*C3*E4*E6*F6+C2*C4*F5*F6*G6)*10000</span><br></pre></td></tr></table></figure><p>注意这里有意将最终的计算结果放大了10000倍，这么处理是便于处理Error项，因为risk contribution的值都很小。Error项作为solver的“设置目标”，也就是优化要达到的最终目标。由于优化的目标是希望Risk contribution的各项都相同，那么也就是说这3项对应的variance=0，因此，Error就设置成：=VAR.P(C7:C9)</p><img src="/2017/07/23/risk-parity-quantitative-fundamental/12.png"><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文介绍了实现risk parity的一种量化方法。并通过使用Excel的solver实现了组合中包含3类asset的risk parity的计算。以3类资产为基础很容易推广至更多类资产。个人觉得用excel+solver的方式对于组合里面asset数量在10多个的情况下是足够应付了。Risk Parity本身计算所需的输入其实很简单，就是各类asset的variance和correlation matrix。但是，所谓garbage in garbage out。如何获取可靠的variance和correlation其实倒是一个难点。</p><p>参考文献：</p><ul><li>2017 CFA level1 volumn 4 corporate finance and portfolio management</li><li>Edward E. Qian, Risk Parity Fundmentals</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;摘要：本文论述了Risk Partity的基本概念，并介绍了如何通过量化的方法构建Risk Parity的Portfolio，并以excel作为工具实现一个简单的Risk Parity的应用。&lt;/p&gt;
&lt;p&gt;关键字：Risk Parity， Risk contribution&lt;/p&gt;
    
    </summary>
    
    
      <category term="量化投资组合管理" scheme="http://yoursite.com/categories/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%E7%BB%84%E5%90%88%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="风险平价" scheme="http://yoursite.com/tags/%E9%A3%8E%E9%99%A9%E5%B9%B3%E4%BB%B7/"/>
    
  </entry>
  
</feed>
