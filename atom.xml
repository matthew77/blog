<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Liang的风险平价观</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.keepswalking.com/"/>
  <updated>2020-09-04T10:24:55.996Z</updated>
  <id>https://www.keepswalking.com/</id>
  
  <author>
    <name>Liang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>基于FCFE重新计算中国市场隐含风险溢价</title>
    <link href="https://www.keepswalking.com/2020/09/04/cn-market-implied-RP-with-FCFE/"/>
    <id>https://www.keepswalking.com/2020/09/04/cn-market-implied-RP-with-FCFE/</id>
    <published>2020-09-04T02:05:16.000Z</published>
    <updated>2020-09-04T10:24:55.996Z</updated>
    
    <content type="html"><![CDATA[<p>原来根据分红数据来计算中国市场的隐含风险溢价，但是得出的结果不太合理。现在改用FCFE（free cash flow to equity）重新计算，所得结果为Implied Risk Premium = 7.37%。</p><a id="more"></a><p>之前根据Damodaran的方法计算了中国市场的隐含风险溢价（<a href="https://www.keepswalking.com/2020/07/14/cn-market-rp/">参考 https://www.keepswalking.com/2020/07/14/cn-market-rp/</a>），但是发现计算出来的风险溢价肯定是偏低的，和美国市场也差不多，也就是说我们这里的风险偏好极大，而且有点大到不合情理。所以只能通过拍脑袋，调高永续增长率，调高分红比例来拉高，但这样计算出来的风险溢价肯定不靠谱，因此就想办法改进计算方法。经过分析，发现原因所在，是因为，美国这样的成熟市场分红+回购是接近FCFE（free cash flow to equity）。因此，对于美国这样的成熟市场使用分红和回购就能获得比较准确的风险溢价。但是，在中国这样的发展中市场，分红和FCFE是有很大差别的。这个差别有多大？我们通过数据来说明：</p><ul><li>中证800指数在2020/8/31的股息率为1.63%</li><li>而同为中证800，在同一时间的FCFE率为4.058%</li></ul><p>显然对于中国市场，通过FCFE才能计算得到准确的隐含风险溢价。比较麻烦的是如何获取市场（中证800）的FCFE。下面我就罗列一下具体的改进和对应的数据。</p><p>首先是FCFE的计算，计算公式我使用的是：$ FCFE = NI-(1-DR)(FCInv-Dep)-(1-DR)WCInv $。使用该公式分别计算每一家中证800成分公司（金融机构除外）的FCFE。其中：</p><ul><li>NI为net income。采集数据的时候使用TTM方式获得最新net income。</li><li>DR为Debt to asset ratio。asset使用最新资产负债表季报数据。而Debt的计算使用了长期借款，短期借款，应付债券，租赁负债，应付短期债券这几个科目。</li><li>FCInv为fixed capital investment 即 CAPEX。根据现金流量表里：购建固定资产、无形资产和其他长期资产支付的现金，处置固定资产、无形资产和其他长期资产收回的现金净额这两个科目，并使用TTM方式获得最新的FCInv。</li><li>Dep为depreciation，根据资产负债表，固定资产—累计折旧，无形资产—累计摊销的期初和期末值来计算最新的折旧。</li><li>WCInv为change in non cash working capital investment，使用流动资产合计减货币资金获得current asset。使用流动负债合计减短期借款，减应付短期债券获得current liability。然后用current asset 减去 current liability。</li></ul><p>该公式假设公司的杠杆率是稳定的，因为需对近800家公司分别计算他们的自由现金流，作为一个整体，这个假设可认为是成立里的（law of large number）。</p><p>金融机构（包含银行和非银）的FCFE直接使用分红数据，没有分红的则认为FCFE为0</p><p>其实难点在收集整理数据。我使用Choice，但即使是付费的软件，还是发现有很多缺失的数据。整个数据的收集，清理大概花去一整天。</p><img src="/2020/09/04/cn-market-implied-RP-with-FCFE/fcfe.PNG" class=""><p>如上图所示，还需要以下这些输入：</p><ul><li>无风险利率，直接使用10年期国债的YTM。</li><li>一致性预期是通过卖方对中证800里成分公司未来三年的盈利预测计算获得。我通过加权平均得到一个增长率。但是，我不直接使用这个值，原因是像这样自底而上的增长率估算通常偏乐观。一般卖方总是给出偏乐观的预测。因此我做了处理，参见预期高速增长率。</li><li>GDP预期，是对中国GDP增长的预测，数据来自tradingeconomics.com。该值也是用来估计FCFE的增长率的。GDP作为一个宏观数据，来的更加客观。</li><li>预期高速增长率， 该值用于FCFE在近期的增长率。通过一致性预期和GDP增长率加权平均获得，权值分别是一致性预期占40%，GDP占60%。</li><li>永续增长率，直接使用无风险利率。</li></ul><p>在计算Implied Risk Premium的时候使用三段式DCF model，即高速增长阶段，过渡阶段和永续增长阶段。</p><p>最后，得到Implied Risk Premium = 7.37%，结合无风险利率可得discount rate=10.48%。个人认为这个值还是比较和合理的。相当于说长期来看中证800的收益率为10.48%。那么也从另一方面反应了当前股市的估值还算合理。当然，由于金融所占比重比较高，因此从更微观的角度来看，可能是一种结构性的分化，金融大盘股比较便宜，而中小盘（成长股）则比较贵了。但综合来看，这个风险溢价还是让我觉得A股的估值处在合理区间。</p><p>再来验证一下这个模型以及收集的数据是否合理。我测试了两个场景：</p><ul><li>市场跌30%，那么Implied Risk Premium = 10.21%，这个风险溢价比较有吸引力。</li><li>市场涨30%，那么Implied Risk Premium = 5.78%，这个风险溢价感觉上不足以补偿投资者承担的风险。要使得这个溢价上升至一个合理的水平，要么FCFE变大（企业有护城河，也就是中证800成分公司的质量变好），要么增长率变高（多数企业处于高速发展阶段，也就是总体经济变好，经济处于上升周期）。</li></ul><p>个人觉得这个模型给出的结果还是符合common sense的。接下来我每周都会更新中国市场的Implied Risk Premium。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原来根据分红数据来计算中国市场的隐含风险溢价，但是得出的结果不太合理。现在改用FCFE（free cash flow to equity）重新计算，所得结果为Implied Risk Premium = 7.37%。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Valuation" scheme="https://www.keepswalking.com/categories/Valuation/"/>
    
    
  </entry>
  
  <entry>
    <title>招商银行估值（DDM法）</title>
    <link href="https://www.keepswalking.com/2020/08/30/valuation-cmbchina/"/>
    <id>https://www.keepswalking.com/2020/08/30/valuation-cmbchina/</id>
    <published>2020-08-30T13:30:26.000Z</published>
    <updated>2020-08-31T12:56:09.093Z</updated>
    
    <content type="html"><![CDATA[<p>没事别去重仓银行股，一笔糊涂账，虽然它已经成了价值投资的代名词。。。</p><p><em>先上结论：招行的内在价值大约为：44元。目前交易价格为38元(2020/8/31)，比内在价值低了14%，有一定的安全边际。</em></p><a id="more"></a><p>银行包括其他的金融服务机构是一个很特殊的群体，通常smart beta策略，还包括一些screening都是将他们排除在外的。原因是：</p><ul><li>银行的debt很难定义同时也很难统计，比如短期债务对银行来说其实是“原材料”而非债务！既然不知道debt，那就无法知晓enterprise value了。因此，对于银行来说，一般直接估算equity。</li><li>对于银行来说，资本性支出和working capital很难估计，因此也就难以计算自由现金流了。</li></ul><p>因为以上原因，反倒是让我解脱了！直接使用DDM（dividend discount model）就解决问题了。面对银行的财报我承认太复杂了，很难分析出个所以然来，对我而言所需要的信息其实就是分红率，ROE，净利润。That’s it!</p><h2 id="故事叙述"><a href="#故事叙述" class="headerlink" title="故事叙述"></a>故事叙述</h2><p>众所周知的一些情况是：招行的零售业务很强是特色，因此，对经济周期敏感性较其他银行来得低，但毕竟还是银行，因此也算是周期性行业；另外，招行的资产质量高，拨备充分。</p><p>从2020半年报看，营业收入同比增长了7.27%但是扣非净利润却录得负增长为-0.88%，主要原因：</p><ul><li>一是业务及管理费同比增加9.7%。这部分用于公司深入推进数字化经营<br>模式探索，可以认为是一种投资活动</li><li>另一为信用减值损失，同比增长22.32%，主要是受疫情的影响</li></ul><p>可以看出疫情对银行业的影响开始逐步显现，2020Q1的数据还是非常“靓丽”。从目前情况看，我个人的预计，疫情的影响持续时间至少2年。</p><p>由于净利润的负增长，扣非后ROE为16.92%，同比减少了2.38%。这个ROE同前几年比也不能算差。</p><p>我整理了招行相关的历史数据：</p><img src="/2020/08/30/valuation-cmbchina/hisdata.PNG" class=""><p>基于以上2020半年报的数据结合历史数据，做出如下假设：</p><ul><li>今后3年净利润不增长</li><li>第4年到第5年净利润以10.5%的速度增长</li><li>近5年dividend payout ratio维持30%</li></ul><h2 id="无风险收益率"><a href="#无风险收益率" class="headerlink" title="无风险收益率"></a>无风险收益率</h2><p>老样子，直接使用10年期国债的YTM了，当前值为3.0672%。这里需要指出，根据Damodaran的方法，risk free rate应该是真正的risk free rate（不是废话吗？！），意思是比如津巴布韦央行发行国债，但你肯定不信他100%不会违约。因此，准确的做法是将某国的risk free rate再减去该国的country default spread。其实呢，China作为发展中国家也是有country default spread滴，要获得这个数据还是有一点麻烦，我这里就偷懒了（也可以直接去Damodaran的博客获得比较新的数据<a href="http://pages.stern.nyu.edu/~adamodar/New_Home_Page/datafile/ctryprem.html）。" target="_blank" rel="noopener">http://pages.stern.nyu.edu/~adamodar/New_Home_Page/datafile/ctryprem.html）。</a> </p><h2 id="Estimate-market-risk-premium"><a href="#Estimate-market-risk-premium" class="headerlink" title="Estimate market risk premium"></a>Estimate market risk premium</h2><p>使用6.1%作为A股的equity risk premium。这里多说两句，之前自己根据Damodaran的方法基于沪深300计算了A股的RP(risk primium)，但是，后来发现这个方法出来的RP是很低的，原因是对于S&amp;P500这样成熟的市场来说，期股票的分红+回购是接近自由现金流的，但是，对于emerging market，如中国市场，公司基本很少分红，因此，分红同自由现金流比还是有很大差距的。最近自己开始使用自由现金流的形式计算RP，并会公布到网上。6.1%是直接使用Damodaran的数据。</p><h2 id="估计beta"><a href="#估计beta" class="headerlink" title="估计beta"></a>估计beta</h2><p>使用bottom-up beta，首先确定可类比公司。招行是综合银行，然后从综合性银行里剔除那些庞然大物，包括：中，农，工，建，邮储。最后得到下面列表：</p><div class="table-container"><table><thead><tr><th>股票代码</th><th>股票名称</th><th>beta</th></tr></thead><tbody><tr><td>000001.SZ</td><td>平安银行</td><td>1.1927</td></tr><tr><td>600000.SH</td><td>浦发银行</td><td>0.6773</td></tr><tr><td>600015.SH</td><td>华夏银行</td><td>0.7173</td></tr><tr><td>600016.SH</td><td>民生银行</td><td>0.6821</td></tr><tr><td>601166.SH</td><td>兴业银行</td><td>0.8381</td></tr><tr><td>601818.SH</td><td>光大银行</td><td>0.7181</td></tr><tr><td>601916.SH</td><td>浙商银行</td><td>0.8422</td></tr><tr><td>601998.SH</td><td>中信银行</td><td>0.7625</td></tr><tr><td>600036.SH</td><td>招商银行</td><td>0.9839</td></tr></tbody></table></div><p>计算beta的平均值，可得招行的beta=0.74</p><h2 id="Estimate-a-cost-of-equity"><a href="#Estimate-a-cost-of-equity" class="headerlink" title="Estimate a cost of equity"></a>Estimate a cost of equity</h2><p>容易得出 r = 3.0672% + 0.74 * 6.1% = 7.58%</p><h2 id="估算TTM净利润"><a href="#估算TTM净利润" class="headerlink" title="估算TTM净利润"></a>估算TTM净利润</h2><p>参见以下数据：</p><div class="table-container"><table><thead><tr><th>时间</th><th>2019</th><th>2020年1-6月</th><th>2019年1-6月</th><th>TTM</th></tr></thead><tbody><tr><td>扣非每股净利润（元）</td><td>3.59</td><td>1.97</td><td>1.99</td><td>3.57</td></tr></tbody></table></div><p>因此，TTM每股净利润为3.57 元。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>使用DDM。前10年的增长率灵活设置，再加最后永续增长阶段。</p><h2 id="内在价值计算"><a href="#内在价值计算" class="headerlink" title="内在价值计算"></a>内在价值计算</h2><img src="/2020/08/30/valuation-cmbchina/value.PNG" class=""><ul><li>根据假设，今后三年的净利润不增长，因此为0%</li><li>永续增长率等于无风险利率=3.07%</li><li>第四第五年10.5%的增长率是根据ROE=15%，payout ratio=30% 推算获得</li><li>terminal year’s payout ratio是根据g=3.07%, ROE=cost of equity=9.17% 推算获得</li><li>永续增长阶段可假设ROE趋近于cost of equity</li><li>永续阶段的cost of equity不同于前10年，使用beta=1计算得到</li></ul><p>据此，获得招行的内在价值大约为：44元。目前交易价格为38元，比内在价值低了14%。个人觉得还是有足够的安全边际。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;没事别去重仓银行股，一笔糊涂账，虽然它已经成了价值投资的代名词。。。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;先上结论：招行的内在价值大约为：44元。目前交易价格为38元(2020/8/31)，比内在价值低了14%，有一定的安全边际。&lt;/em&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Valuation" scheme="https://www.keepswalking.com/categories/Valuation/"/>
    
    
      <category term="招商银行" scheme="https://www.keepswalking.com/tags/%E6%8B%9B%E5%95%86%E9%93%B6%E8%A1%8C/"/>
    
  </entry>
  
  <entry>
    <title>关于自底向上贝塔(bottom-up beta)的10个常见问题</title>
    <link href="https://www.keepswalking.com/2020/08/27/ten-question-about-bottom-up-betas/"/>
    <id>https://www.keepswalking.com/2020/08/27/ten-question-about-bottom-up-betas/</id>
    <published>2020-08-27T07:28:59.000Z</published>
    <updated>2020-08-27T07:45:10.052Z</updated>
    
    <content type="html"><![CDATA[<p>本翻译获得Damodaran教授授权，转载请注明出处。原文链接：<a href="http://people.stern.nyu.edu/adamodar/New_Home_Page/TenQs/TenQsBottomupBetas.htm" target="_blank" rel="noopener">http://people.stern.nyu.edu/adamodar/New_Home_Page/TenQs/TenQsBottomupBetas.htm</a></p><a id="more"></a><h3 id="1-什么是bottom-up-beta？"><a href="#1-什么是bottom-up-beta？" class="headerlink" title="1. 什么是bottom-up beta？"></a>1. 什么是bottom-up beta？</h3><p>bottom-up beta 的估算首先从分析公司所处行业/业务出发，通过估算各个业务的基本风险或beta，然后将这些风险进行加权平均最终获得。</p><h3 id="2-估算bottom-up-beta包括哪些步骤？"><a href="#2-估算bottom-up-beta包括哪些步骤？" class="headerlink" title="2. 估算bottom-up beta包括哪些步骤？"></a>2. 估算bottom-up beta包括哪些步骤？</h3><p>一共包括4个步骤：</p><p>步骤1：将目标公司的业务做一定的拆分。GE有26种业务，而沃尔玛则是单一业务的公司。不要将业务拆分的过细，因为到了步骤2会碰到问题。</p><p>步骤2：分别估算每种业务的风险（beta）。这里的beta成为资产（asset）beta或无杠杆（unlevered）beta。</p><p>步骤3：将上一步获得的无杠杆beta进行加权平均，权值是根据某业务的价值（value）对比所有业务的价值加总来确定的。</p><p>步骤4：根据目标公司的财务杠杆（debt to equity 比值）计算最终的beta。</p><h3 id="3-如何选择可类比的公司？"><a href="#3-如何选择可类比的公司？" class="headerlink" title="3. 如何选择可类比的公司？"></a>3. 如何选择可类比的公司？</h3><p>一个比较侠义的定义是和目标公司业务相同的公司，而比较广义的定义则包括了那些因目标公司经营情况的好坏而受到影响的公司。反之亦然，即那些可类比公司的经营情况会影响到目标公司。从实际应用角度出发，你可以将可类比公司定义为非常相似的公司，例如目标公司生产游戏软件，那么你就找其他生产游戏软件的公司。如果你能找到足够多的样本（参见问题4），那么这步就结束了。否则，可以试着使用下面的方法来拓展你的搜索范围：</p><p>i. 将可类比公司的定义放宽（例如：所有的软件公司而非仅仅是游戏软件公司）</p><p>ii. 从全球范围查找业务相同的公司。例如：全球所有的游戏软件公司。</p><p>iii. 从目标公司所在供应链的上下游查找可类比的公司。例如查找那些收入大头为游戏软件的软件零售公司</p><h3 id="4-我们至少需要多样本公司？"><a href="#4-我们至少需要多样本公司？" class="headerlink" title="4. 我们至少需要多样本公司？"></a>4. 我们至少需要多样本公司？</h3><p>我们这么来思考这个问题，任何大于1的样本数量都能提高回归beta（regression beta。直接使用market return和个股的return通过线性回归获得的beta）的精度。然而，可比公司的样本越多则消除误差的效果就越明显。如果有4个样本，那么标准差（standard error）将减少一半；如果有9个样本，那么标准差将减少2/3；16个样本则减少75%…。如有可能，尽量获取2位数的样本数量。如果做不到，那么争取获取6~8个可类比公司，这样的话你也能将beta估算的误差降低很多。</p><p>当然， 在可类比公司的定义的宽松程度以及样本数量之间需要权衡。如果你对可类比公司的定义比较严格（可类比公司的规模和业务必须要同目标公司一致），那么你能获得的样本数量肯定比较少。如果这样严格的定义能让你获得2位数的样本，那就使用这些样本。如果你的样本数量太少了，那就试着使用问题3里面的方法来扩充你的样本数量。</p><h3 id="5-一旦我们获得了可类比公司，那接下来该如何估算无杠杆beta（或叫资产beta）？"><a href="#5-一旦我们获得了可类比公司，那接下来该如何估算无杠杆beta（或叫资产beta）？" class="headerlink" title="5. 一旦我们获得了可类比公司，那接下来该如何估算无杠杆beta（或叫资产beta）？"></a>5. 一旦我们获得了可类比公司，那接下来该如何估算无杠杆beta（或叫资产beta）？</h3><p>简单来说，获取这些回归beta的平均值并剔除财务杠杆以及现金的影响。实际是，还是会碰到一些问题：</p><p>a. 在获取每个可类比公司的回归beta时，必须使用同一时间段，并基于同一指数吗？</p><p>如果在一个完美的世界里，回答是yes！但是，随着样本数量的增加，你就不需要太关注这些细节了，因为大数定律（law of large numbers）会拯救你。因此，如果在你的样本里有100家来自世界各地的公司，而这些公司的回归beta都是基于本国股票市场的指数，你还是可以将这100家公司的beta做平均，因为有些beta可能被高估了而另一些则可能被低估。</p><p>b. 当我们获得了样本公司的回归beta，那接下来该使用简单平均还是加权平均？</p><p>使用简单平均。否则，你得到的beta会趋近于你样本里面最大的一家或几家公司的beta。比如，微软的beta变成了所有软件公司的beta。</p><p>c. 为何我们要剔除财务杠杆的影响？</p><p>目标公司在负债方面的政策可能和样本公司有非常大的区别。样本公司的回归beta是带杠杆的beta，它反应了样本公司的财务杠杆（而非目标公司的）。你必须剔除财务杠杆的影响（剔除掉回归beta的杠杆）以获得单一（pure play）的，或叫业务beta。</p><p>Unlevered beta = Regression beta / (1 + (1-tax rate) D/E)</p><p>d. 是该先剔除每家样本公司的回归beta的杠杆再平均呢，还是先获得各家样本公司回归beta的平均值再剔除杠杆？</p><p>我倾向先平均再剔除杠杆。单个样本公司的回归beta带有大量的噪音（标准差很大），如果先剔除杠杆，那无疑将叠加噪音的影响。如果先平均则将减少噪音，从而能获得更准确的beta估算。</p><p>e. 针对某个业务对应的样本公司，我该使用哪个税率以及debt to equity比？</p><p>保险起见，使用边际税率；使用D/E比的中值或聚合的D/E比(加总所有的D，加总所有的E再获得D/E)。D/E比经常出现异常值，这会导致简单平均失去代表性。</p><p>f. 为何需要调整现金对beta的影响，具体该怎么作？</p><p>回归beta反应了公司所有的资产包括现金。因此，如果一家公司的资产包含了60%的软件还有40%的现金，那么它的回归beta会比较低，因为现金是无风险的。由于我们希望获得纯粹的软件业务的beta，我们就需要去除现金对beta的影响。假设现金的beta为零，那么很容易可得：</p><p>Cash-adjusted beta = Unlevered beta / (1 – Cash/ Firm Value)</p><p>Firm value = Market value of Equity + Market value of Debt</p><h3 id="6-是否可能对无杠杆beta进一步调整经营杠杆（operating-leverage）？"><a href="#6-是否可能对无杠杆beta进一步调整经营杠杆（operating-leverage）？" class="headerlink" title="6. 是否可能对无杠杆beta进一步调整经营杠杆（operating leverage）？"></a>6. 是否可能对无杠杆beta进一步调整经营杠杆（operating leverage）？</h3><p>是有这个可能，前提是你知道目标公司以及所有样本公司的固定成本以及可变成本。如果你能获取这些信息，你就可以将无杠杆beta再分割成业务部分（体现了对目标公司需求的弹性）和经营杠杆部分：</p><p>Business Risk beta = Unlevered beta/ (1 +Fixed Costs/ Variable Costs)</p><p>实践中碰到的困难是获取固定成本和可变成本。</p><h3 id="7-如果已获得了目标公司对应各业务的无杠杆beta，那该如何分配权重以获得目标公司的无杠杆beta？"><a href="#7-如果已获得了目标公司对应各业务的无杠杆beta，那该如何分配权重以获得目标公司的无杠杆beta？" class="headerlink" title="7. 如果已获得了目标公司对应各业务的无杠杆beta，那该如何分配权重以获得目标公司的无杠杆beta？"></a>7. 如果已获得了目标公司对应各业务的无杠杆beta，那该如何分配权重以获得目标公司的无杠杆beta？</h3><p>理论上权重应该是基于目标公司各业务的市场价值。然而，现实情况是这些业务并无法交易（例如GE Capital并没有单独上市），因此需要估算各业务的市场价值。一种简便的做法是使用各业务的销售收入或利润来分配权重，但这样就等于假设1美元的销售收入（或利润）对不同的业务来说具有相同的价值。另一种方法是使用销售收入（或利润）的乘数来大致计算每个业务的价值。这些乘数可以通过可类比公司（就是前面用来计算beta平均数的样本公司）的乘数计算获得。因为我们关心的是业务的价值（而非权益的价值），因此我们应该使用EV（enterprise value）乘数（而非权益乘数）。如果使用销售收入，那就用EV/Sales乘数。</p><h3 id="8-如何调整财务杠杆？"><a href="#8-如何调整财务杠杆？" class="headerlink" title="8. 如何调整财务杠杆？"></a>8. 如何调整财务杠杆？</h3><p>调整财务杠杆的标准做法是假设有息债务（debt）的市场风险为0（beta=0）并使用所谓的“Hamada”调整：</p><p>Levered Beta = Unlevered beta (1 + (1- tax rate) (Debt/Equity))</p><p>可以使用目标公司当前的debt to equity比，也可以使用预期的debt to equity比（如果你觉得这个变化很快就会发生）来进行该调整的计算。</p><p>如果你觉得有息债务没有市场风险这个假设不对，那么你需要获得有息债务的beta，并使用以下公式计算：</p><p>Levered Beta = Unlevered Beta (1 + (1-t)(D/E)) – Beta of debt (1-t)(D/E)</p><p>比较棘手的问题是如何估计有息债务的beta。</p><h3 id="9-一家公司的自底向上的beta会随着时间改变吗？"><a href="#9-一家公司的自底向上的beta会随着时间改变吗？" class="headerlink" title="9. 一家公司的自底向上的beta会随着时间改变吗？"></a>9. 一家公司的自底向上的beta会随着时间改变吗？</h3><p>是的，有2个原因。一个是公司的业务组成会随着时间改变，导致无杠杆beta发生改变。另一个是公司的debt to equity比会随着时间改变，这导致了杠杆beta（回归beta）发生改变。</p><h3 id="10-为何使用自底向上的beta比使用回归beta好？"><a href="#10-为何使用自底向上的beta比使用回归beta好？" class="headerlink" title="10. 为何使用自底向上的beta比使用回归beta好？"></a>10. 为何使用自底向上的beta比使用回归beta好？</h3><p>主要有3方面的原因：</p><ul><li><p><em>更精确。</em> 自底向上的beta的标准差更小，因为它通过众多的回归beta取平均值获得。提高的精度大致等于1/SQRT(样本公司数量)。因此，即使目标公司只有一种业务且长时间保持debt to equity比稳定，你还是能从自底向上的beta获得好处。</p></li><li><p>如果目标公司<em>改变了业务组成</em>，你可以很容易地根据改变后的业务组成计算获取自底向上的beta，因为你可以设置不同业务的权重。而回归beta只能反应过去的业务组成。</p></li><li><p>如果目标公司<em>改变了debt to equity比</em>，可以通过调整自底向上的beta，就能很容易地反应这些改变。而回归beta只能反应过去的debt to equity比。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本翻译获得Damodaran教授授权，转载请注明出处。原文链接：&lt;a href=&quot;http://people.stern.nyu.edu/adamodar/New_Home_Page/TenQs/TenQsBottomupBetas.htm&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://people.stern.nyu.edu/adamodar/New_Home_Page/TenQs/TenQsBottomupBetas.htm&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Valuation" scheme="https://www.keepswalking.com/categories/Valuation/"/>
    
    
  </entry>
  
  <entry>
    <title>腾讯估值 DCF 方法</title>
    <link href="https://www.keepswalking.com/2020/08/21/valuation-tencent/"/>
    <id>https://www.keepswalking.com/2020/08/21/valuation-tencent/</id>
    <published>2020-08-21T06:38:19.000Z</published>
    <updated>2020-08-25T06:12:07.908Z</updated>
    
    <content type="html"><![CDATA[<p>先说结论吧，根据2020半年报的数据，腾讯的内在价值为510 HKD，当前股价为548HKD， 比内在价值高出7.4%，可以认为当前股价处于合理区域内。</p><a id="more"></a><h2 id="Story-Naritive"><a href="#Story-Naritive" class="headerlink" title="Story Naritive"></a>Story Naritive</h2><p>腾讯的收入分为以下几块：</p><ul><li>增值服务</li><li>网络广告</li><li>金融科技及企业服务</li><li>其它</li></ul><p>增值服务包括网络游戏和社交网络两部分业务，网络游戏国内增长总体放缓，增量时代向存量时代，而且竞争对手也开始不断成长，竞争加剧。可能在海外市场获得新的增长。社交网络业务包括QQ VIP、视频会员、版权音乐、数字阅读，所有非游戏类的付费收入，这个块业务随着内容付费在国内接受度提高，可能继续保持稳步增长。对于增值服务的预期是：今后5年保持总体10%的增长。</p><p>网络广告在经过业务整合以后（腾讯广告和微信广告整合）有可能继续保持比较高速的增长。预计今后5年保持20%的增长。</p><p>金融科技及企业服务方面，前者会成为腾讯继游戏以后另一大支柱业务，而企业服务方面，目前来看竞争激烈，虽然腾讯有发展to B的愿望，但是最近几年很难找到突破口。综合来看今后5年保持30%的增长。</p><p>而其它业务年报里面也没有披露具体信息，并且也只占总收入的2%，对股价影响非常小，按30%的增长来估算。</p><h2 id="Risk-free-rate"><a href="#Risk-free-rate" class="headerlink" title="Risk free rate"></a>Risk free rate</h2><p>由于腾讯年报里用的是人民币，因此为方便估算，整个过程全部使用人民币。以10年期国债作为无风险利率。最新的值为2.9921%</p><h2 id="Estimate-market-risk-premium"><a href="#Estimate-market-risk-premium" class="headerlink" title="Estimate market risk premium"></a>Estimate market risk premium</h2><p>根据腾讯2019年年报显示，海外收入占比只有4%，因此，只需要考虑国内市场的风险溢价就行了。而根据我目前的测算，<a href="https://www.keepswalking.com/2020/07/21/CN-implied-RP/">参见A股市场隐含风险溢价</a>，国内市场风险溢价为5.17%，个人觉得这个溢价有点低，说明整个市场有些高估了。那么，我觉得理想情况下的风险溢价水平在6.1%，后面可以根据这2个值分别计算腾讯的价值。</p><h2 id="Estimate-bottom-up-unlevered-beta"><a href="#Estimate-bottom-up-unlevered-beta" class="headerlink" title="Estimate bottom-up unlevered beta"></a>Estimate bottom-up unlevered beta</h2><p>unlevered beta的具体计算可参见<a href="https://www.keepswalking.com/2020/07/18/bottom-up-beta/">Bottom-up Beta方法 — 试算腾讯的beta</a>。这里我就直接引用Professor Damodaran的global beta数据，unlevered beta = 1.15</p><h2 id="Estimate-market-value-of-equity-and-debt"><a href="#Estimate-market-value-of-equity-and-debt" class="headerlink" title="Estimate market value of equity and debt"></a>Estimate market value of equity and debt</h2><p>最新港元人民币汇率：1港元：0.89人民币</p><p>最近20天均价：529 港元</p><p>总股本：95.82亿</p><p>最新股票市值：45113.0142 亿人民币</p><p>以下计算book value of debt，根据2020年Q2数据：</p><div class="table-container"><table><thead><tr><th>项目</th><th>金额（人民币百万元）</th></tr></thead><tbody><tr><td>非流动负债</td><td></td></tr><tr><td>借款</td><td>131,988</td></tr><tr><td>应付票据</td><td>126,785</td></tr><tr><td>其他金融负债</td><td>7,016</td></tr><tr><td>租赁负债</td><td>8,143</td></tr><tr><td>流动负债</td><td></td></tr><tr><td>借款</td><td>15,101</td></tr><tr><td>应付票据</td><td>0</td></tr><tr><td>其他金融负债</td><td>5,746</td></tr><tr><td>租赁负债</td><td>3,474</td></tr><tr><td>合计</td><td>298,253</td></tr></tbody></table></div><h2 id="Estimate-bottom-up-levered-beta"><a href="#Estimate-bottom-up-levered-beta" class="headerlink" title="Estimate bottom-up levered beta"></a>Estimate bottom-up levered beta</h2><p>根据 debt和equity的市值，可得debt-to-equity ratio为：</p><p>D/E = 2982.53/45113.0142 = 0.066。</p><p>根据腾讯2019年财报，可计算effective tax rate = 12.35% ，由unlevered beta可计算出levered beta = 1.15 * (1 + (1-12.35%) * 0.066) = 1.22</p><h2 id="Estimate-pre-tax-cost-of-debt"><a href="#Estimate-pre-tax-cost-of-debt" class="headerlink" title="Estimate pre-tax cost of debt"></a>Estimate pre-tax cost of debt</h2><p>根据2019年财报第149页显示：“于二零一九年一月一日应用于租赁负债的加权平均承租人增量借款利率为4.58%”，那么可以推断腾讯的pre-tax cost of debt 为 4.58%</p><h2 id="convert-operating-leases-into-debt"><a href="#convert-operating-leases-into-debt" class="headerlink" title="convert operating leases into debt"></a>convert operating leases into debt</h2><p>这个数据是用来估算operating lease的value，由于2019年财报已经使用了新的会计准则（IFRS16），因此这块计算可以省略。</p><h2 id="Estimate-a-tax-rate"><a href="#Estimate-a-tax-rate" class="headerlink" title="Estimate a tax rate"></a>Estimate a tax rate</h2><p>根据腾讯2019年财报，可计算effective tax rate = 12.35%</p><h2 id="Estimate-a-cost-of-capital"><a href="#Estimate-a-cost-of-capital" class="headerlink" title="Estimate a cost of capital"></a>Estimate a cost of capital</h2><p>cost of equity = 2.9921% + 1.22*6.1% = 10.43%</p><p>WACC = 10.43% * 45113.0142/(45113.0142+2982.53) + (1-12.35%) * 4.58% * 2982.53/(45113.0142+2982.53) = 10.03%</p><h2 id="Capitalize-R-amp-D-expenses"><a href="#Capitalize-R-amp-D-expenses" class="headerlink" title="Capitalize R&amp;D expenses"></a>Capitalize R&amp;D expenses</h2><p>对于腾讯这样的科技公司，将研发费用资本化能更加准确的反应经营性收入(operating income)。根据腾讯2019年年报可查得研发费用，用3年折旧，见下表：</p><div class="table-container"><table><thead><tr><th>year</th><th>index</th><th>研发开支（亿元rmb）</th><th>折旧</th></tr></thead><tbody><tr><td>2019</td><td>current</td><td>303.87</td><td>0</td></tr><tr><td>2018</td><td>-1</td><td>229.36</td><td>76.45</td></tr><tr><td>2017</td><td>-2</td><td>174.56</td><td>58.19</td></tr><tr><td>2016</td><td>-3</td><td>118.45</td><td>39.48</td></tr><tr><td></td><td></td><td>Total</td><td>174.12</td></tr></tbody></table></div><p>因此，当年研发折旧为：174.12 亿元RMB。</p><p>那么，对当年operating income的调整为：303.87-174.12 = 129.75 亿元RMB</p><h2 id="Estimate-an-adjusted-operating-income"><a href="#Estimate-an-adjusted-operating-income" class="headerlink" title="Estimate an adjusted operating income"></a>Estimate an adjusted operating income</h2><p>腾讯刚发布2020年中报，因此，结合2019年年报可以计算出TTM (trailing 12 month)的经营性收入。计算如下：</p><div class="table-container"><table><thead><tr><th>项目</th><th>2019</th><th>2020MID</th><th>2019MID</th><th>TTM</th><th>TTM/2019</th></tr></thead><tbody><tr><td>Revenues</td><td>377,289</td><td>222,948</td><td>174,286</td><td>425,951</td><td>13%</td></tr><tr><td>EBIT</td><td>118,694</td><td>76,571</td><td>64,263</td><td>131,002</td><td>10%</td></tr></tbody></table></div><p>以上单位为人民币百万元</p><p>由于将研发费用资本化，因此需要对EBIT进行调整，调整后的EBIT为：</p><p>131,002 + 12,975 = 143,977 </p><p><strong>注意：</strong> 这里的数据是需要进行标准化的，原因是2020年上半年由于新冠疫情的原因，使得游戏业务意外受益。从2020年半年报可知，半年的收入同比增长了28%！而2019年的收入比2018年增长了21%，那么，可以假设如果没有疫情影响，2020年上半年销售额同比增长为21%</p><h2 id="Estimate-net-capital-expenditures"><a href="#Estimate-net-capital-expenditures" class="headerlink" title="Estimate net capital expenditures"></a>Estimate net capital expenditures</h2><p>Net capital expenditures = capital expenditures - depreciation。从公司治理角度来看，净资本支出必须大于零，就是说资本支出要高于折旧，公司才可能有增长。</p><p>我整理了2015年~2019年年报里的数据：</p><div class="table-container"><table><thead><tr><th>资本性支出</th><th>2019</th><th>2018</th><th>2017</th><th>2016</th><th>2015</th></tr></thead><tbody><tr><td>購買物業、設備及器材、在建工程與投資物業</td><td>-22,766</td><td>-19,743</td><td>-12,108</td><td>-8,399</td><td>-5,440</td></tr><tr><td>處置物業、設備及器材</td><td>4</td><td>33</td><td>28</td><td>31</td><td>70</td></tr><tr><td>購買無形資產的付款╱預付款項</td><td>-29,866</td><td>-31,877</td><td>-19,850</td><td>-8,849</td><td>-4,505</td></tr><tr><td>購買土地使用權的付款╱預付款項</td><td>-4,356</td><td>-2,441</td><td>-46</td><td>-1,506</td><td>-3,045</td></tr><tr><td>對聯營公司的投資</td><td>-14,904</td><td>-37,776</td><td>-33,912</td><td>-12,258</td><td>-13817</td></tr><tr><td>處置於聯營公司的投資</td><td>667</td><td>429</td><td>1115</td><td>1,373</td><td>1,106</td></tr><tr><td>對合營公司的投資</td><td>-720</td><td>-2,352</td><td>-7082</td><td>-59</td><td>-500</td></tr><tr><td>total - without acquisition</td><td>-56,984</td><td>-54,028</td><td>-31,976</td><td>-18,723</td><td>-12,920</td></tr><tr><td>total - with acquisition</td><td>-71,941</td><td>-93,727</td><td>-71,855</td><td>-29,667</td><td>-26,131</td></tr></tbody></table></div><p>以下是折旧/摊销的数据：</p><div class="table-container"><table><thead><tr><th>折旧/摊销</th><th>2019</th><th>2018</th><th>2017</th><th>2016</th><th>2015</th></tr></thead><tbody><tr><td>無形資產攤銷</td><td>28,954</td><td>25,616</td><td>18,622</td><td>8,930</td><td>3,515</td></tr><tr><td>物業、設備及器材、投資物業以及使用權資產折舊</td><td>15,623</td><td>8,423</td><td>4,850</td><td>3,699</td><td>3,159 </td></tr><tr><td>土地使用權攤銷</td><td>211</td><td>353</td><td>109</td><td>95</td><td>0</td></tr><tr><td>Total</td><td>44,788</td><td>34,392</td><td>23,581</td><td>12,724</td><td>6,674</td></tr><tr><td>研发调整</td><td>129.7</td><td>101.6</td><td>79.7</td><td>46.1</td><td>34.2</td></tr><tr><td>Adj Net CAPEX with ac</td><td>27,283</td><td>59,437</td><td>48,354</td><td>16,989</td><td>19,491</td></tr><tr><td>Adj Net CAPEX without ac</td><td>12,325.7</td><td>19,737.6</td><td>8,474.7</td><td>6,045.1</td><td>6,280.2 </td></tr></tbody></table></div><ul><li>上表中，2015年土地使用权摊销已包含在无形资产摊销内，因此设置成0。</li><li>对于腾讯这样的科技公司将研发费用资本化</li><li>研发调整 = 当年研发支出-历年研发摊销，使用3年摊销。</li><li>Adj Net CAPEX = CAPEX - Depreciation + 研发调整</li></ul><p>由以上数据进行一些统计：</p><div class="table-container"><table><thead><tr><th>统计项目</th><th>2019</th><th>2018</th><th>2017</th><th>2016</th><th>2015</th></tr></thead><tbody><tr><td>Revenue</td><td>377,289</td><td>312,694</td><td>237,760</td><td>151,938</td><td>102,863</td></tr><tr><td>capex without ac / Rev</td><td>15.1%</td><td>17.3%</td><td>13.4%</td><td>12.3%</td><td>12.6%</td></tr><tr><td>adj net capex without ac / Rev</td><td>3.3%</td><td>6.3%</td><td>3.6%</td><td>4.0%</td><td>6.1%</td></tr><tr><td>adj net capex with ac / Rev</td><td>7.2%</td><td>19.0%</td><td>20.3%</td><td>11.2%</td><td>18.9%</td></tr></tbody></table></div><ul><li>上表第一行为历年销售收入</li><li>由第二行capex without ac / Rev 可见，腾讯每年的资本支出同销售收入之间的关系还是比较稳定的，基本在13% 到 17% 之间</li><li>由第三行adj net capex without ac / Rev可见，不包括并购投资的净资本支出在3.3% 到 6.1%之间浮动</li><li>第四行adj net capex with ac / Rev 包括了并购支出。并购投资通常来说不是一种稳定的行为，因此需要对此作一些平滑处理</li></ul><p>经平滑处理后，可得每年并购活动占销售额（ac/rev）为10%；而adj net capex without ac / Rev 的平均值为4.6%；因此推测<strong>总的资本性支出占销售额为14.6%</strong></p><h2 id="Estimate-non-cash-working-capital"><a href="#Estimate-non-cash-working-capital" class="headerlink" title="Estimate non-cash working capital"></a>Estimate non-cash working capital</h2><p>non-cash working capital 的定义：non-cash working capital = non-cash current asset - non-debt current liabilities</p><p>收集历年的年报数据：</p><div class="table-container"><table><thead><tr><th>项目</th><th>2019</th><th>2018</th><th>2017</th><th>2016</th><th>2015</th><th>2014</th></tr></thead><tbody><tr><td>current asset</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>存貨</td><td>718</td><td>324</td><td>295</td><td>263</td><td>222</td><td>244</td></tr><tr><td>應收賬款</td><td>35,839</td><td>28,427</td><td>16,549</td><td>10,152</td><td>7,061</td><td>4,588</td></tr><tr><td>預付款項、按金及其他資產</td><td>27,840</td><td>18,493</td><td>17,110</td><td>14,118</td><td>11,397</td><td>7,804</td></tr><tr><td>current liabilities</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>應付賬款</td><td>80,690</td><td>73,735</td><td>50,085</td><td>27,413</td><td>15,700</td><td>8,683</td></tr><tr><td>其他應付款項及預提費用</td><td>45,174</td><td>33,312</td><td>29,433</td><td>20,873</td><td>70,199</td><td>19,123</td></tr><tr><td>流動所得稅負債</td><td>9,733</td><td>10,210</td><td>8,708</td><td>5,219</td><td>1,608</td><td>461</td></tr><tr><td>其他稅項負債</td><td>1,245</td><td>1,049</td><td>934</td><td>745</td><td>462</td><td>566</td></tr><tr><td>遞延收入</td><td>60,949</td><td>42,375</td><td>42,132</td><td>31,203</td><td>21,122</td><td>16,153</td></tr><tr><td>non-cash working cap</td><td>-133,394</td><td>-113,437</td><td>-97,338</td><td>-60,920</td><td>-90,411</td><td>-32,350</td></tr><tr><td>change in working cap</td><td>-19,957</td><td>-16,099</td><td>-36,418</td><td>29,491</td><td>-58,061</td><td></td></tr><tr><td>change in working cap / rev</td><td>-5%</td><td>-5%</td><td>-15%</td><td>19%</td><td>-56%</td><td></td></tr></tbody></table></div><ul><li>从上表可见，腾讯的working capital是负的，也就是说腾讯从供应商处获得信用作为自己的资本发展业务。</li><li>另外，腾讯的change in working capital也是负的，说明随着公司不断增长，其从供应商处获得信用的能力也越来越强。但是，从长期来看，working capital变得越来越负是不现实的。从数据上看，最近3年change in working cap / rev的值不断向0的方向增加。</li></ul><p>因此，<strong>推测change in working cap占销售额</strong> 为-5%，并且不断趋向0</p><h2 id="Estimate-Free-Cash-Flow-to-Firm"><a href="#Estimate-Free-Cash-Flow-to-Firm" class="headerlink" title="Estimate Free Cash Flow to Firm"></a>Estimate Free Cash Flow to Firm</h2><p>参见 Value the firm/stock</p><h2 id="Estimate-a-historical-growth-rate-in-earnings"><a href="#Estimate-a-historical-growth-rate-in-earnings" class="headerlink" title="Estimate a historical growth rate in earnings"></a>Estimate a historical growth rate in earnings</h2><p>参见 Value the firm/stock</p><h2 id="Estimate-growth-in-earnings-from-fundamentals"><a href="#Estimate-growth-in-earnings-from-fundamentals" class="headerlink" title="Estimate growth in earnings from fundamentals"></a>Estimate growth in earnings from fundamentals</h2><p>参见 Value the firm/stock</p><h2 id="Choose-a-lenght-for-high-growth-period"><a href="#Choose-a-lenght-for-high-growth-period" class="headerlink" title="Choose a lenght for high growth period"></a>Choose a lenght for high growth period</h2><p>最近5年为高增长期，销售收入增长率为：19%</p><h2 id="Choose-a-DCF-model"><a href="#Choose-a-DCF-model" class="headerlink" title="Choose a DCF model"></a>Choose a DCF model</h2><p>三段式：前5年为高增长期，后5年为过渡期，最后为永续增长期。永续增长率为：5.98%。</p><h2 id="期权处理"><a href="#期权处理" class="headerlink" title="期权处理"></a>期权处理</h2><p>根据2020年中报，期权信息显示如下：</p><img src="/2020/08/21/valuation-tencent/option.PNG" class=""><p>根据年报信息显示，这2个计划发出的期权期限都为7年。计划II已于2017年5月16日结束。那么计划II发出的期权最晚可能于2024年5月16日到期。因此假设计划II期权到期时间为2年。</p><p>计划IV是从2017年5月17日起十年內有效，那么该计划里最早的期权将于2024年5月17日到期。因此假设计划IV期权到期时间为3.5年。</p><p>根据以下输入参数：</p><div class="table-container"><table><thead><tr><th>参数</th><th>计划II</th><th>计划IV</th></tr></thead><tbody><tr><td>当前股价</td><td>529 港元</td><td>529 港元</td></tr><tr><td>期权执行价</td><td>185.86港元</td><td>374.84港元</td></tr><tr><td>到期时间（年）</td><td>2</td><td>3.5</td></tr><tr><td>波动率</td><td>30%</td><td>30%</td></tr><tr><td>股息率</td><td>0.23%</td><td>0.23%</td></tr><tr><td>无风险利率</td><td>0.5%</td><td>0.5%</td></tr><tr><td>期权数量</td><td>48,841,295</td><td>67,983,454</td></tr><tr><td>期权单位价值</td><td>341.90</td><td>193.24 </td></tr><tr><td>所有期权价值</td><td>16,698,627,876</td><td>13,137,208,432</td></tr></tbody></table></div><p>因此，截至2020/6/30 全部期权价值为：16,698,627,876 + 13,137,208,432 = 29,835.836,308 港元 = 26,553.89 百万人民币</p><h2 id="Value-the-firm-stock"><a href="#Value-the-firm-stock" class="headerlink" title="Value the firm/stock"></a>Value the firm/stock</h2><p>参见下图：</p><img src="/2020/08/21/valuation-tencent/FCFF.PNG" class=""><p>上表中有几点需要注意：</p><ul><li>terminal year的reinvestment是通过growth equation计算获得的，即 reinvestment = (perpetual growth / ROIC) * after tax operating income</li><li>base year revenue 使用TTM，但因为疫情原因使得游戏业务意外收益，因此没有直接使用TTM，而是做了标准化</li></ul><p>最后需要计算shares outstanding。截至2020-07-6，腾讯有9,555,165,469股（数据来源choice金融终端）。根据2020年中报显示，股份奖励计划将发行80,064,689股，因此稀释后的股份数= 9,555,165,469+80,064,689 = 9,635,230,158。</p><img src="/2020/08/21/valuation-tencent/result.PNG" class=""><ul><li>最后的每股内在价值考虑了股份奖励计划以及期权对股价的稀释</li><li>目前的股价比计算出来的内在价值高7.4%，可以认为处在合理范围内</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;先说结论吧，根据2020半年报的数据，腾讯的内在价值为510 HKD，当前股价为548HKD， 比内在价值高出7.4%，可以认为当前股价处于合理区域内。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Valuation" scheme="https://www.keepswalking.com/categories/Valuation/"/>
    
    
      <category term="腾讯" scheme="https://www.keepswalking.com/tags/%E8%85%BE%E8%AE%AF/"/>
    
  </entry>
  
  <entry>
    <title>从debt和liabilities的翻译想到的</title>
    <link href="https://www.keepswalking.com/2020/08/03/debt-definition/"/>
    <id>https://www.keepswalking.com/2020/08/03/debt-definition/</id>
    <published>2020-08-03T06:40:41.000Z</published>
    <updated>2020-08-24T08:42:58.010Z</updated>
    
    <content type="html"><![CDATA[<p>前些时间在计算上市公司unlevered beta的时候需要用到debt to equtiy ratio（负债股权比率），于是就去找相关的数据来计算，找着找着自己就迷茫了。到底什么负债？我发现中文里面debt和liability都翻译成负债，债务。这就很误导我这种非会计出生的半吊子了。想想，作为母语是英语的同学就不会有这样的疑惑了。</p><a id="more"></a><p>首先，从大方向上看看liability和debt的区别：</p><ul><li>liability包括debt，而debt是liability的子类。</li><li>Debt总是以钱的形式存在的，而liability的形式就很广了，任何需要公司支出的都算</li><li>Debt比liability具有更为严重的法律后果（如果违约）</li></ul><p>知道了这一层似乎就不那么迷茫了，那么从财报分析的角度看，到底什么是debt呢？也就是debt的定义：</p><ul><li>承诺在未来的某一时点支付某个约定的金额</li><li>该支出可以获得税收上的减免</li><li>如果没有兑现该支出，则导致违约，或失去对公司的控制，并由债权人获得控制权</li></ul><p>有了以上的定义，我们就比较容易区分哪些是debt而哪些是liability了，虽然中文都叫负债。。。</p><p>具体来说，所有的有息债务不管是长期还是短期，都算debt。租赁，包括经营租赁(operating lease)和资本租赁(capital lease)都认为是debt；而应付账款以及卖方信贷(supplier credit)则不是debt。下图表述了debt和equity的一些特征：</p><p>那么，再进一步，如何计算debt。是否直接拿财报上的数值就行了呢？往下细挖一下，发现其实还是有很多弯弯需要绕的。那么，先从一些具体的应用开始：</p><ul><li>通过FCFF (free cash flow for the firm) discount方式获得企业价值(Enterprise value — EV)后，需要推算出equity value。而EV = market value of equity + market value of debt - cash。这里显然需要debt的市值，而非报表上的book value（账面价值）。</li><li>再来，Debt to equity ratio (D/E)，比如我们在使用pure play方法计算unlevered beta时需要使用到D/E，那这里使用的是市值还是账面价值呢？其实，仔细分析一下，我们计算beta是为了进一步计算requried return，作为投资者，他们投资是账面价值还是市场价值呢？显然是后者。因此，这里的D/E应该用市值来计算</li></ul><p>由以上两例可见，很多情况下我们需要用到debt的市值！那么企业的debt未必都是以公开发行债券的方式。另外，还有些debt，如operating lease甚至看上去都不是debt，那该如何计算市值呢？</p><p>那么，我们就以腾讯控股为例，试着算算她的debt的市值是多少。翻开腾讯2019年的年报，第141，142页展示了负债部分，如下图所示：</p><img src="/2020/08/03/debt-definition/tencent_debt1.PNG" class=""><img src="/2020/08/03/debt-definition/tencent_debt2.PNG" class=""><p>先来确定哪几项可以归入debt的范畴，对于非流动负债部分：</p><div class="table-container"><table><thead><tr><th>项目</th><th>是否计入debt？</th><th>金额（人民币百万元）</th><th>说明</th></tr></thead><tbody><tr><td>借款</td><td>是</td><td>104,257</td><td>根据说明，借款是指向银行的长期借款</td></tr><tr><td>应付票据</td><td>是</td><td>83,327</td><td>根据说明，应付票据是指浮动利率债</td></tr><tr><td>长期应付款项</td><td>否</td><td>3,577</td><td></td></tr><tr><td>其他金融负债</td><td>是</td><td>5,242</td><td></td></tr><tr><td>递延所得税负债</td><td>否</td><td>12,841</td><td></td></tr><tr><td>租赁负债</td><td>是</td><td>8,428</td><td></td></tr><tr><td>递延收入</td><td>否</td><td>7,334</td><td></td></tr></tbody></table></div><p>对于流动负债部分：</p><div class="table-container"><table><thead><tr><th>项目</th><th>是否计入debt？</th><th>金额（人民币百万元）</th><th>说明</th></tr></thead><tbody><tr><td>应付账款</td><td>否</td><td>80,690</td><td></td></tr><tr><td>其他应付款项及预提费用</td><td>否</td><td>45,174</td></tr><tr><td>借款</td><td>是</td><td>22,695</td><td>银行借款</td></tr><tr><td>应付票据</td><td>是</td><td>10,534</td><td></td></tr><tr><td>流动所得税负债</td><td>否</td><td>9,733</td><td></td></tr><tr><td>其他税项负债</td><td>否</td><td>1,245</td><td></td></tr><tr><td>其他金融负债</td><td>是</td><td>5,857</td><td></td></tr></tbody></table></div><p>因此可得debt的总额为285,514。此金额是账面价值而非市场价值。在实践中，这个账面价值可以近似认为是市场价值。因为，要得出市场价值还是有很多困难，一般来说，需要将公司的debt当作一只债券，需要知道债券的yeild to maturity，期限，每年支付的利息，到期的面额，这些都是需要进行估算。从腾讯的财报中可知，应付票据是按照市场价格来计算的，再加上腾讯本事偿债能力非常强，因此，可以将报表上的金额近似认为是市场价值。</p><p>对公司进行估值的时候很多数据是不能拿来直接用的，以上debt就是一例，如果直接拿债务(liability)那就会出现很大的偏差。进一步，对于腾讯这样的大型公司，她进行了很多的并购，而其中有些被并购的公司是并表的，而有些是以equity method方式计入报表的，如果需要知道腾讯真实的价值，还需要对这些并购的公司分别进行估值才行。所以，常常觉得对公司进行估值不是一件容易的事情，真的是需要具备方方面面的知识。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前些时间在计算上市公司unlevered beta的时候需要用到debt to equtiy ratio（负债股权比率），于是就去找相关的数据来计算，找着找着自己就迷茫了。到底什么负债？我发现中文里面debt和liability都翻译成负债，债务。这就很误导我这种非会计出生的半吊子了。想想，作为母语是英语的同学就不会有这样的疑惑了。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Valuation" scheme="https://www.keepswalking.com/categories/Valuation/"/>
    
    
  </entry>
  
  <entry>
    <title>A股市场隐含风险溢价(Implied Equity Risk Premium)</title>
    <link href="https://www.keepswalking.com/2020/07/21/CN-implied-RP/"/>
    <id>https://www.keepswalking.com/2020/07/21/CN-implied-RP/</id>
    <published>2020-07-21T02:42:08.000Z</published>
    <updated>2020-09-04T01:55:02.857Z</updated>
    
    <content type="html"><![CDATA[<p>说明：</p><ol><li>隐含风险溢价(Implied Equity Risk Premium)的具体计算参见：<a href="/2020/07/14/cn-market-rp/" title="中国股票市场的风险溢价">中国股票市场的风险溢价</a></li><li>市场参照对象为沪深300指数</li><li>每周更新</li></ol><a id="more"></a><div class="table-container"><table><thead><tr><th>时间</th><th>Implied RP</th><th>HS800点位</th><th>FCFE</th><th>高速增长率%</th><th>无风险收益率%</th><th>永续增长率%</th><th>说明</th></tr></thead><tbody><tr><td>2020/9/2</td><td>5.45%</td><td>5153.16</td><td>151.14</td><td>15.1%</td><td>3.11%</td><td>3.11%</td><td>计算方法从分红改为自由现金流<br>无风险利率持续走高，体现了对通胀的预期<br>5.45% 的RP说明市场风险偏好比较高，美股的RP大约在5.23%，A股明显偏贵</td></tr></tbody></table></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;说明：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;隐含风险溢价(Implied Equity Risk Premium)的具体计算参见：&lt;a href=&quot;/2020/07/14/cn-market-rp/&quot; title=&quot;中国股票市场的风险溢价&quot;&gt;中国股票市场的风险溢价&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;市场参照对象为沪深300指数&lt;/li&gt;
&lt;li&gt;每周更新&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="Data" scheme="https://www.keepswalking.com/categories/Data/"/>
    
    
  </entry>
  
  <entry>
    <title>Bottom-up Beta方法 -- 试算腾讯的beta</title>
    <link href="https://www.keepswalking.com/2020/07/18/bottom-up-beta/"/>
    <id>https://www.keepswalking.com/2020/07/18/bottom-up-beta/</id>
    <published>2020-07-18T13:03:53.000Z</published>
    <updated>2020-07-19T07:19:24.049Z</updated>
    
    <content type="html"><![CDATA[<p>Beta是一个非常流行的个股风险度量尺度，通过大名鼎鼎的CAPM广为传播。当需要确定某个股的discount rate的时候最常用的方式就是通过CAPM：$ E(R)=R_f + \beta * (R_m-R_f) $ 。其中$R_m$ 是 expected market return。 </p><a id="more"></a><p>那么该如何获取beta呢？一种方式就是通过linear regression。将independent variables:$ R_m, R_f $ 以及dependent variable E(R) 通过linear regression便可获取参数beta了。但是，很多终端包括在网上找到的所有例子，在计算beta时不考虑$R_f$，直接通过$ E(R) = \beta*(R_m) $ 来确定beta；这种做法可能有一定的道理，由于$R_f$本身很小，比如国内10年期国债的yield在3%左右，如果以weekly return计算的话，对应的return才0.0577%，而国外的$R_f$就更不用说了，因此可以近似忽略。另外，如何选择risk free rate本身又是一件不太简单的事情。例如，对于成熟市场来说如美国来说，就可以有多种选择，除了国债外（10年期或其他？）还有Overnight Index Swap (OIS)等。</p><p>接下来通过linear regression方法来计算腾讯的beta。因为腾讯在香港上市，因此，market return使用恒生指数。$ R_f $使用香港政府10年期国债。仿照Bloomberg计算beta的方法，使用最近2年的weekly return数据。我自己计算获得的结果是：</p><ul><li>不包含$ R_f $，使用regression 计算得出的beta = 1.219</li><li>包含$ R_f $，使用regression 计算得出的beta = 1.22</li><li>直接使用公式：$Covariance(market<em>{rt}, stock</em>{rt}) / Var(market_{rt})$，计算得出的beta = 1.231</li></ul><p>可见，risk free rate对于beta的影响确实很小。另外，我在Choice终端上，使用同样的周期得到的结果是1.2123。而yahoo finance上，5 年，月return显示的beta是1.02，我用yahoo的数据自己算了一下，发现它的结果是不对的！应该是1.2471，接近我上面的计算结果。看来大网站的数据也不能相信。</p><p>当然，本文不是讨论regression beta如何计算，从上面可以看出来，单个股票的beta其实是一个含有大量噪音的数据，如果直接就这么用，那心里肯定没底，十有八九这个beta的值是有问题的。因此，有一种“bottom-up Betas”的方法可以比较好的消除单只股票的beta的噪音。</p><p>什么是“bottom-up beta”？就是根据目标公司所在的行业，在这些行业里找到同目标公司相似的一堆公司，用这些公司的beta计算平均值获得一个比较准确的beta。bottom-up beta的一大好处就是，当你找到足够多的样本公司，你估算出来的目标公司beta的standaerd error会大大降低。另一个好处就是即使当目标公司没有很多历史数据的情况下，你一样可以通过bottom-up beta计算得到目标公司的beta。</p><p>以下是bottom-up beta的具体计算方法：</p><ol><li>确定目标公司所处的行业，目标公司有可能是单一行业，也有可能是跨多个行业。</li><li>在每个行业里找到相似的公司，当然多多益善。如果是跨行业的公司，则需要分别在每个行业里面找到相似的公司，并获得这些公司的beta。然后分别按行业计算这些公司beta的平均值。</li><li>计算unlevered beta。$ \beta_u = \beta_r / (1 + (1-tax) D/E) $ 。其中D/E是debt to equity ratio，这里指相似公司的平均D/E。</li><li>确定目标公司在各业务上的比重，通常可以按照销售收入或opterating income来确定比重。当然最理想的方式是估算各业务的价值，以价值来确定比重。</li><li>计算目标公司unlevered beta的加权平均值。个业务unlevered beta来自第3步，权重来自第4步。</li><li>最后计算levered beta，$ \beta_{bottomup} = \beta_u (1+ (1-tax) (D/E))$，这里的D/E是目标公司的D/E。</li></ol><p>方法讲完了，接下来就以腾讯控股为例来确定她的bottom-up beta。按照以上第1步，先确定腾讯所在行业即她涉及哪些业务。腾讯的收入包括如下几块：</p><ul><li>网络游戏</li><li>数字内容，包括：视频（传统+短视频），音乐，阅读</li><li>网络广告</li><li>金融科技</li><li>云及企业服务</li></ul><p>而从GICS分类来看腾讯属于：信息技术-软件与服务-信息技术服务-互联网服务与基础设施。显然这样的分类不能体现腾讯的全部业务。一种比较仔细的做法是根据腾讯的业务，分别找到对应的公司。这里，我就比较粗粒度的找到一些我觉得相关的在香港上市的公司，见下表：</p><div class="table-container"><table><thead><tr><th>证券代码</th><th>证券名称</th><th>所属GICS行业</th><th>Beta</th><th>总市值（亿元）</th><th>D/E</th></tr></thead><tbody><tr><td>09988.HK</td><td>阿里巴巴-SW</td><td>非日常生活消费品-零售业-互联网与直销零售-互联网与直销零售</td><td>0.9181</td><td>51,250.7415</td><td>0.493</td></tr><tr><td>03690.HK</td><td>美团点评-W</td><td>信息技术-软件与服务-软件-应用软件</td><td>0.8307</td><td>11,231.8189</td><td>0.339</td></tr><tr><td>01810.HK</td><td>小米集团-W</td><td>信息技术-软件与服务-信息技术服务-互联网服务与基础设施</td><td>1.1003</td><td>3,733.3914</td><td>1.070</td></tr><tr><td>00268.HK</td><td>金蝶国际</td><td>信息技术-软件与服务-软件-应用软件</td><td>1.5451</td><td>598.9131</td><td>0.370</td></tr><tr><td>08083.HK</td><td>中国有赞</td><td>非日常生活消费品-零售业-互联网与直销零售-互联网与直销零售</td><td>1.1385</td><td>282.5032</td><td>1.693</td></tr><tr><td>02013.HK</td><td>微盟集团</td><td>信息技术-软件与服务-信息技术服务-互联网服务与基础设施</td><td>1.9174</td><td>252.0400</td><td>0.660</td></tr><tr><td>00136.HK</td><td>恒腾网络</td><td>信息技术-软件与服务-信息技术服务-互联网服务与基础设施</td><td>0.7934</td><td>182.7986</td><td>0.347</td></tr><tr><td>01686.HK</td><td>新意网集团</td><td>信息技术-软件与服务-信息技术服务-互联网服务与基础设施</td><td>1.1249</td><td>136.7236</td><td>2.696</td></tr><tr><td>00777.HK</td><td>网龙</td><td>信息技术-软件与服务-软件-应用软件</td><td>1.4445</td><td>127.7308</td><td>0.447</td></tr><tr><td>00799.HK</td><td>IGG</td><td>信息技术-软件与服务-信息技术服务-互联网服务与基础设施</td><td>1.5530</td><td>93.0871</td><td>0.272</td></tr><tr><td>01089.HK</td><td>乐游科技控股</td><td>信息技术-软件与服务-信息技术服务-互联网服务与基础设施</td><td>0.6488</td><td>92.4880</td><td>0.282</td></tr><tr><td>00302.HK</td><td>中手游</td><td>信息技术-软件与服务-软件-应用软件</td><td>1.0677</td><td>77.5940</td><td>0.403</td></tr><tr><td>01675.HK</td><td>亚信科技</td><td>信息技术-软件与服务-软件-应用软件</td><td>0.4815</td><td>71.0940</td><td>0.244</td></tr><tr><td>01357.HK</td><td>美图公司</td><td>信息技术-软件与服务-信息技术服务-互联网服务与基础设施</td><td>1.3548</td><td>68.5665</td><td>0.236</td></tr><tr><td>01137.HK</td><td>香港电视</td><td>通讯服务-媒体与娱乐-娱乐-电影与娱乐</td><td>1.1290</td><td>56.4704</td><td>0.656</td></tr><tr><td>01806.HK</td><td>汇付天下</td><td>金融-综合金融-综合金融服务-其它综合性金融服务</td><td>0.8373</td><td>39.5487</td><td>3.959</td></tr><tr><td>BIDU.O</td><td>百度</td><td>信息技术-软件与服务-信息技术服务-互联网服务与基础设施</td><td>1.4500</td><td>423.1467</td><td>0.751</td></tr><tr><td>NTES.O</td><td>网易</td><td>信息技术-软件与服务-信息技术服务-互联网服务与基础设施</td><td>0.7400</td><td>631.9686</td><td>0.599</td></tr></tbody></table></div><p>以上数据来自东方财富Choice数据。</p><p>一共12家上市公司。在查找这些公司的时候首先我考虑市值，毕竟腾讯的体量摆在那里，找对应的公司市值也不能太小，否则就没有参考意义了；另外，还得考虑上市时间，像网易，京东这样的公司虽然比较匹配，但是上市时间实在太短，历史数据太少，因此也没参考意义，我就直接拿美股的数据过来了（虽然跨市场，但只要样本足够多，通过大数定律还是可以得到比较准确的结果）。</p><p>有了参照数据，很容易就得到了beta的平均值：1.115。然后计算unlevered beta，需要先获得相似公司的D/E，我这里将所有相关公司的detb和equity分别加总，再用这两个总和求得D/E = 0.573。然后还需要确定tax rate，根据国家政策，高新技术企业税率是15%，代入这些数据可得：</p><p>$\beta_u$ = 1.115/(1+(1-0.15) * 0.573) = 0.75</p><p>最后，为了获取腾讯的bottom-up beta，我们还需要确定腾讯的D/E，根据腾讯的财报，可得到她的D/E = 0.976。那么，对应的beta应该为：</p><p>$\beta_{bottomup}$ = 0.75*(1+(1-0.15)*0.976) = 1.372</p><p>这就是腾讯的beta。那么，将这个bottom-up beta同腾讯的regressison beta（=1.22）比大了一点（12.5%）。但1.372也是有一定道理的，因为，从D/E比来看，网易，百度，阿里都比腾讯低。因此，我个人觉得1.372这个值还是比较靠谱。确定beta作为股票估值的重要的一步到此完成。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Beta是一个非常流行的个股风险度量尺度，通过大名鼎鼎的CAPM广为传播。当需要确定某个股的discount rate的时候最常用的方式就是通过CAPM：$ E(R)=R_f + \beta * (R_m-R_f) $ 。其中$R_m$ 是 expected market return。 &lt;/p&gt;
    
    </summary>
    
    
      <category term="Valuation" scheme="https://www.keepswalking.com/categories/Valuation/"/>
    
    
  </entry>
  
  <entry>
    <title>中国股票市场的风险溢价</title>
    <link href="https://www.keepswalking.com/2020/07/14/cn-market-rp/"/>
    <id>https://www.keepswalking.com/2020/07/14/cn-market-rp/</id>
    <published>2020-07-14T13:11:41.000Z</published>
    <updated>2020-07-21T03:37:32.779Z</updated>
    
    <content type="html"><![CDATA[<p>所谓的Risk Premium（以下简写成RP）是值市场的预期收益(expected return)减去无风险收益率（risk free rate）$ r_f $。RP是估值的关键，当对某只股票进行估值的时候，你首先需要确定其required return（或称discount rate）r。而$ r = r_f + \beta*RP $。那么中国市场的RP是多少呢？作为emerging market，中国市场的RP是否就比成熟市场高呢？</p><a id="more"></a><p>RP的确定通常有2种方式。Historical Premium 和 Implied Premium。Historical Premium其实就是根据历史数据计算市场的平均收益率（算数或几何平均）。这里以沪深300指数来代表中国股票市场整体。那么，先来看看Historical Premium是多少？参见下表：</p><div class="table-container"><table><thead><tr><th>时间</th><th>开盘</th><th>收盘</th><th>收益</th></tr></thead><tbody><tr><td> 2005/12/30</td><td>994.76</td><td>923.45</td><td>-7.17%</td></tr><tr><td> 2006/12/29</td><td>926.56</td><td>2041.05</td><td>121.02%</td></tr><tr><td> 2007/12/28</td><td>2073.25</td><td>5338.27</td><td>161.55%</td></tr><tr><td> 2008/12/31</td><td>5349.76</td><td>1817.72</td><td>-65.95%</td></tr><tr><td> 2009/12/31</td><td>1848.33</td><td>3575.68</td><td>96.71%</td></tr><tr><td> 2010/12/31</td><td>3592.47</td><td>3128.26</td><td>-12.51%</td></tr><tr><td> 2011/12/30</td><td>3155.56</td><td>2345.74</td><td>-25.01%</td></tr><tr><td> 2012/12/31</td><td>2361.5</td><td>2522.95</td><td>7.55%</td></tr><tr><td> 2013/12/31</td><td>2551.81</td><td>2330.03</td><td>-7.65%</td></tr><tr><td> 2014/12/31</td><td>2323.43</td><td>3533.71</td><td>51.66%</td></tr><tr><td> 2015/12/31</td><td>3566.09</td><td>3731</td><td>5.58%</td></tr><tr><td> 2016/12/30</td><td>3725.86</td><td>3310.08</td><td>-11.28%</td></tr><tr><td> 2017/12/29</td><td>3313.95</td><td>4030.86</td><td>21.78%</td></tr><tr><td> 2018/12/28</td><td>4045.21</td><td>3010.65</td><td>-25.31%</td></tr><tr><td> 2019/12/31</td><td>3017.07</td><td>4096.58</td><td>36.07%</td></tr><tr><td> 2020/07/14</td><td>4121.35</td><td>4806.69</td><td>17.33%</td></tr></tbody></table></div><p>一共才16条数据，数据样本很小，未必能揭示真实的RP。如果以算数平均计算，可得沪深300的预期收益是：22.77%。而以几何平均计算，预期收益是：10.35%，几何平均更合理，通常来说几何平均是会小于算数平均。对于无风险收益率来说，选取10年期国债的收益率：3.06%，由此可得RP=10.35%-3.06% = 7.29%。看上去好像还蛮不错的哦！</p><p>当然，使用historical premium的问题也很明显，你需要有大量的历史数据，对于成熟市场，如果美国股市，可以得到相对可靠的数据（其实方差也很大），而对于中国这样的emerging market，historical premium的准确性更是值得怀疑。另外一个问题，历史数据只能反应历史，而很多新的变化就无法体现在结果中，就好比看着后视镜开车。</p><p>Implied Premium，在计算过程中使用了“未来数据”，将最新的预期加入计算，因此更加能反应未来的变化。那么，如何计算implied premium呢？简单来说就是根据dividend discount model (DDM) 来推算出r—required return, 我们知道DDM的简单形式可以写成：</p><script type="math/tex; mode=display">P = D_1/(r-g)</script><p>其中，P对应现在的指数，D1=下一年的分红，g=收入增长率。那么，如果知道P，D1，g就可以推算出r来了。而D1，g都是预测值，是对未来的预期。通过这种方式计算出来的r对估值具有更好的效果。</p><p>接下来还是以沪深300为例来计算Implied Premium。在本例中使用两段式的DDM， 即一开始的5年为高速增长期，5年以后为稳定增长期。输入参数如下：</p><div class="table-container"><table><thead><tr><th>参数</th><th>值</th><th>说明</th></tr></thead><tbody><tr><td>当前指数</td><td>4806.69</td><td>这个很容获取，直接填上最新的指数值，我这里填的式2020/7/14日沪深300的收盘价</td></tr><tr><td>分红和回购率</td><td>2.07%</td><td>这个要花一点功夫了，不得不说中证指数有限公司工作很粗糙，照理这些数据应该提供的，我在他们网站上找了好久也没找到，而S&amp;P以及恒生指数都提供这些数据的。最后只能自己计算了，将300只成分股一个一个数据汇集起来。最终的结果也符合直觉，国内红率不是很高，尤其对应这几天的大涨，分红率就更低了。</td></tr><tr><td>今后5年收入增加率</td><td>14.364%</td><td>这个数据也挺难弄。我从Choice终端上查找一致性预期获得的。原始数据是今后2年复合增长率21.9%，但是感觉这数据太高了。有可能是疫情的原因。我自己作了调整。</td></tr><tr><td>五年以后增长率</td><td>3.06%</td><td>这个值代表了永续增长率，我取和无风险收益保持一致</td></tr><tr><td>10年期国债收益率</td><td>3.06%</td><td>无风险收益率</td></tr></tbody></table></div><p>然后，利用MS Excel的规划求解就能得到implied risk premium，参见下图：</p><img src="/2020/07/14/cn-market-rp/irp.PNG" class=""><p>expected dividend是根据今后5年收入增长率推算得到。terminal value根据DDM公式计算得到。应用规划求解，可变单元格的值设置为Implied Risk Premium，最后的结果为3.49%，同historical premium的7.29%比低了好多啊！！！至于说哪个值更加符合市场实际情况呢？我个人觉得3.49%似乎比较符合直观感受。我大哀股如同绞肉机，说直白一点，就是收益和风险不对称，股民承担了很高的风险却为了不太理想的收益。</p><p>为何implied premium会那么低？首先，分红+回购比例很低！这也导致了terminal value很低，而另一方面，指数本身又很高。这就进一步解释了高风险低收益的市场状况。假设市场跌去50%，而分红绝对值保持不变，那么implied premium就变成了6.81%，相对来说就改善了很多。</p><p>所以，implied premium能从风险收益比较的角度来提供我们观察市场的一种方法。那么，究竟该使用historical premium还是implied premium呢？</p><p>下表是Damodaran教授提供的美国市场的数据：</p><img src="/2020/07/14/cn-market-rp/which_to_use.PNG" class=""><p>显然implied premium效果好很多了。不过需要指出的一点，implied premium需要高质量的预期数据，例如今后几年的盈利增长率，分红率。获得高质量的一致性预期数据就显得尤为重要。当然还需要指出的是implied premium并没有一个客观正确的值，每个人可以有自己预期，并得到对应的值，就好像每个人对市场都可以有自己的看法一样。RP是一项很关键的数据，后面的股票估值直接需要RP的参与。我打算每周都会更新implied risk premium。</p><p>：：：更新（2020/7/21）：：：</p><ol><li>后来我对一些假设作了修改，因此计算出来的结果发生了改变，主要假设如下：</li><li>永续增长率使用10年国开债3.44%，认为中国5年后还是emerging market，因此永续增长率比无风险利率偏高一些</li><li>假设5年后的股息率提高至3.94%，即10年国开债+0.5%（当前10年国开债为3.44%）</li><li>5年逐步增长至目标分红率</li></ol><p>结果参见：<a href="/2020/07/21/CN-implied-RP/" title="A股市场隐含风险溢价(Implied Risk Premium)">A股市场隐含风险溢价(Implied Risk Premium)</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;所谓的Risk Premium（以下简写成RP）是值市场的预期收益(expected return)减去无风险收益率（risk free rate）$ r_f $。RP是估值的关键，当对某只股票进行估值的时候，你首先需要确定其required return（或称discount rate）r。而$ r = r_f + \beta*RP $。那么中国市场的RP是多少呢？作为emerging market，中国市场的RP是否就比成熟市场高呢？&lt;/p&gt;
    
    </summary>
    
    
      <category term="Valuation" scheme="https://www.keepswalking.com/categories/Valuation/"/>
    
    
  </entry>
  
  <entry>
    <title>vanishing gradients</title>
    <link href="https://www.keepswalking.com/2020/06/02/vanishing-gradients/"/>
    <id>https://www.keepswalking.com/2020/06/02/vanishing-gradients/</id>
    <published>2020-06-02T09:13:32.000Z</published>
    <updated>2020-06-02T09:37:34.701Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Vanishing-Gradients"><a href="#Vanishing-Gradients" class="headerlink" title="Vanishing Gradients"></a>Vanishing Gradients</h1><p>这里只使用numpy来实现一个neural network，而非借助pytorch这样的框架，如此，可以更好的帮助我理解neural network以及vanishing gradients.</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">10.0</span>, <span class="number">8.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># generate random data -- not linearly separable</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">N = <span class="number">100</span> <span class="comment"># number of points per class</span></span><br><span class="line">D = <span class="number">2</span> <span class="comment"># dimensionality (向量维度)</span></span><br><span class="line">K = <span class="number">3</span> <span class="comment"># number of classes</span></span><br><span class="line">X = np.zeros((N*K, D))</span><br><span class="line">num_train_examples = X.shape[<span class="number">0</span>]</span><br><span class="line">y = np.zeros(N*K, dtype=<span class="string">'uint8'</span>)  <span class="comment"># 无符号整数</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(K):</span><br><span class="line">    ix = range(N*j, N*(j+<span class="number">1</span>))</span><br><span class="line">    r = np.linspace(<span class="number">0.0</span>, <span class="number">1</span>, N) <span class="comment"># radius, evenly spaced numbers</span></span><br><span class="line">    t = np.linspace(j*<span class="number">4</span>, (j+<span class="number">1</span>)*<span class="number">4</span>, N) + np.random.randn(N)*<span class="number">0.2</span> <span class="comment"># theta</span></span><br><span class="line">    X[ix] = np.c_[r*np.sin(t), r*np.cos(t)] <span class="comment"># 变成2列的matrix</span></span><br><span class="line">    y[ix] = j</span><br><span class="line">fig = plt.figure()</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.Spectral)</span><br><span class="line">plt.xlim([<span class="number">-1</span>,<span class="number">1</span>])</span><br><span class="line">plt.ylim([<span class="number">-1</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p><img src="1.svg" alt="svg"></p><p>sigmoid 函数的值在0,1之间，尤其是在输入值的绝对值很大的情况下，两端会无限接近0或1，因此变得很扁平，由此，对应的梯度（Gradient）就会无限逼近0。这就导致了所谓的梯度消失（vanishing gradients）的现象。因为梯度消失使得神经网络无法进行有效的学习（因为每次迭代对参数W的修正几乎都是0）。</p><p>而另一方面，relu函数不会出现因为输入参数变大而输出变得不敏感。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    x = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_grad</span><span class="params">(x)</span>:</span> <span class="comment"># sigmoid函数的导数</span></span><br><span class="line">    <span class="keyword">return</span> (x)*(<span class="number">1</span>-x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>,x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot the 2 function</span></span><br><span class="line">x = np.linspace(<span class="number">-3</span>,<span class="number">3</span>,<span class="number">100</span>)</span><br><span class="line">y_sigmoid = sigmoid(x)</span><br><span class="line">y_relu = relu(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y_sigmoid, <span class="string">'b-'</span>, x, y_relu, <span class="string">'r-'</span>)</span><br></pre></td></tr></table></figure><p><img src="2.svg" alt="svg"></p><p>接下来让我们看一下2种不同的非线性函数（sigmoid和relu）对神经网络在训练时的影响。以下，我们会创建一个简单的3层神经网路（2 hidden layers）。 通过使用sigmoid和relu我们可以比较在训练过程中的区别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># function to train a 3 layer neural net with either relu or sigmoid nonlinearity via vanilla grad decent.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">three_layer_net</span><span class="params">(NONLINEARITY, X, y, model, step_size, reg)</span>:</span></span><br><span class="line">    <span class="comment"># param init</span></span><br><span class="line">    h = model[<span class="string">'h'</span>]</span><br><span class="line">    h2 = model[<span class="string">'h2'</span>]</span><br><span class="line">    W1 = model[<span class="string">'W1'</span>]</span><br><span class="line">    W2 = model[<span class="string">'W2'</span>]</span><br><span class="line">    W3 = model[<span class="string">'W3'</span>]</span><br><span class="line">    b1 = model[<span class="string">'b1'</span>]</span><br><span class="line">    b2 = model[<span class="string">'b2'</span>]</span><br><span class="line">    b3 = model[<span class="string">'b3'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># some hyperparameters</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># gradient descent loop</span></span><br><span class="line">    num_examples = X.shape[<span class="number">0</span>]</span><br><span class="line">    plot_array_1 = []</span><br><span class="line">    plot_array_2 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">50000</span>):</span><br><span class="line">        <span class="comment"># forward prop</span></span><br><span class="line">        <span class="comment"># 假设有X的维度[N, M], 即有N条training 数据，每条数据有M个feature</span></span><br><span class="line">        <span class="keyword">if</span> NONLINEARITY == <span class="string">'RELU'</span>:</span><br><span class="line">            <span class="comment"># 从X到hidden layer 1, X=[N*2], W1=[2*50] --&gt; hidden layer=[N*50]</span></span><br><span class="line">            hidden_layer = relu(np.dot(X,W1) + b1)  </span><br><span class="line">            <span class="comment"># 从 hidden layer 1 到 layer2. hidden 1 = [N*50], W2=[50*50]--&gt;hidden2 = [N*50]</span></span><br><span class="line">            hidden_layer2 = relu(np.dot(hidden_layer, W2) + b2)  </span><br><span class="line">            <span class="comment"># 从 hidden 2 到 最总output layer， hidden 2 = [1*50], W3=[50*3] --&gt; output = [3] </span></span><br><span class="line">            scores = np.dot(hidden_layer2, W3) + b3  <span class="comment"># scores的维度是[N,3]</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">elif</span> NONLINEARITY == <span class="string">'SIGM'</span>:</span><br><span class="line">            hidden_layer = sigmoid(np.dot(X,W1) + b1)  <span class="comment"># 从X到hidden layer 1</span></span><br><span class="line">            hidden_layer2 = sigmoid(np.dot(hidden_layer, W2) + b2)  <span class="comment"># 从 hidden layer 1 到 layer2</span></span><br><span class="line">            scores = np.dot(hidden_layer2, W3) + b3</span><br><span class="line"></span><br><span class="line">        exp_scores = np.exp(scores) <span class="comment"># [N*K], K=3</span></span><br><span class="line">        <span class="comment"># keepdims=True 意思是np.sum被sum的维度变为1，另一个维度保持不变。</span></span><br><span class="line">        probs = exp_scores / np.sum(exp_scores, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>) <span class="comment"># normalize，行方向sum</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute the loss: average cross-entropy loss and regularization</span></span><br><span class="line">        corect_logprobs = -np.log(probs[range(num_examples), y]) <span class="comment"># y的取值范围0，1，2。结果[N*1]</span></span><br><span class="line">        data_loss = np.sum(corect_logprobs)/num_examples</span><br><span class="line">        <span class="comment"># regularization项</span></span><br><span class="line">        reg_loss = <span class="number">0.5</span>*reg*np.sum(W1*W1) + <span class="number">0.5</span>*reg*np.sum(W2*W2)+<span class="number">0.5</span>*reg*np.sum(W3*W3)</span><br><span class="line">        loss = data_loss + reg_loss</span><br><span class="line">        <span class="comment"># if i % 1000 == 0:</span></span><br><span class="line">        <span class="comment">#     print("iteration &#123;0&#125;: loss &#123;1&#125;".format(i, loss))</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># comput the gradient on scores</span></span><br><span class="line">        dscores = probs  <span class="comment">#  -- [N*K]</span></span><br><span class="line">        dscores[range(num_examples), y] -= <span class="number">1</span>  <span class="comment"># error value -- [N*K]</span></span><br><span class="line">        dscores /= num_examples</span><br><span class="line"></span><br><span class="line">        <span class="comment"># backprop here</span></span><br><span class="line">        <span class="comment"># 从output layer back prop 到 hidden layer2, 由于该层没有nonlinearity</span></span><br><span class="line">        <span class="comment"># 即 hidden2*W3 = s, ds/dW3 = hidden2 (local gradient), 因此back prop error</span></span><br><span class="line">        <span class="comment"># 只需要将error * local gradient，即hidden2.</span></span><br><span class="line">        dW3 = (hidden_layer2.T).dot(dscores)  <span class="comment"># [50*N] * [N*3] = [50*3]</span></span><br><span class="line">        db3 = np.sum(dscores, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>) <span class="comment"># [3]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> NONLINEARITY == <span class="string">'RELU'</span>:</span><br><span class="line">            <span class="comment"># backprop Relu nonlinearity here</span></span><br><span class="line">            dhidden2 = np.dot(dscores, W3.T)</span><br><span class="line">            dhidden2[hidden_layer2 &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">            dW2 = np.dot(hidden_layer.T, dhidden2)</span><br><span class="line">            plot_array_2.append(np.sum(np.abs(dW2))/np.sum(np.abs(dW2.shape)))</span><br><span class="line">            db2 = np.sum(dhidden2, axis=<span class="number">0</span>)</span><br><span class="line">            dhidden = np.dot(dhidden2, W2.T)</span><br><span class="line">            dhidden[hidden_layer &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">elif</span> NONLINEARITY == <span class="string">'SIGM'</span>:</span><br><span class="line">            <span class="comment"># backprop sigmoid nonlinearity there</span></span><br><span class="line">            <span class="comment"># hidden layer 2 是 sigmoid的输出，因此需要乘以一个local gradiant</span></span><br><span class="line">            dhidden2 = dscores.dot(W3.T)*sigmoid_grad(hidden_layer2) </span><br><span class="line">            dW2 = (hidden_layer.T).dot(dhidden2)</span><br><span class="line">            plot_array_2.append(np.sum(np.abs(dW2))/np.sum(np.abs(dW2.shape)))</span><br><span class="line">            db2 = np.sum(dhidden2, axis=<span class="number">0</span>)</span><br><span class="line">            dhidden = dhidden2.dot(W2.T)*sigmoid_grad(hidden_layer)</span><br><span class="line"></span><br><span class="line">        dW1 = np.dot(X.T, dhidden)</span><br><span class="line">        plot_array_1.append(np.sum(np.abs(dW1))/np.sum(np.abs(dW1.shape)))</span><br><span class="line">        db1 = np.sum(dhidden, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># add regularization</span></span><br><span class="line">        dW3 += reg * W3</span><br><span class="line">        dW2 += reg * W2</span><br><span class="line">        dW1 += reg * W1</span><br><span class="line"></span><br><span class="line">        <span class="comment"># option to return loss, grads, </span></span><br><span class="line">        <span class="comment"># grads = &#123;&#125;</span></span><br><span class="line">        <span class="comment"># grads['W1'] = dW1</span></span><br><span class="line">        <span class="comment"># grads['W2'] = dW2</span></span><br><span class="line">        <span class="comment"># grads['W3'] = dW3</span></span><br><span class="line">        <span class="comment"># grads['b1'] = db1</span></span><br><span class="line">        <span class="comment"># grads['b2'] = db2</span></span><br><span class="line">        <span class="comment"># grads['b3'] = db3</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># update grads</span></span><br><span class="line">        W1 += -step_size * dW1</span><br><span class="line">        b1 += -step_size * db1</span><br><span class="line">        W2 += -step_size * dW2</span><br><span class="line">        b2 += -step_size * db2        </span><br><span class="line">        W3 += -step_size * dW3</span><br><span class="line">        b3 += -step_size * db3</span><br><span class="line">    <span class="comment"># evaluate training set accuracy， 使用training出来的W1，W2,W3,b1,b2,b3</span></span><br><span class="line">    <span class="comment"># 来计算结果。</span></span><br><span class="line">    <span class="keyword">if</span> NONLINEARITY == <span class="string">'RELU'</span>:</span><br><span class="line">        hidden_layer = relu(np.dot(X, W1) + b1)</span><br><span class="line">        hidden_layer2 = relu(np.dot(hidden_layer, W2) + b2)</span><br><span class="line">    <span class="keyword">elif</span> NONLINEARITY == <span class="string">'SIGM'</span>:</span><br><span class="line">        hidden_layer = sigmoid(np.dot(X, W1) + b1)</span><br><span class="line">        hidden_layer2 = sigmoid(np.dot(hidden_layer, W2) + b2)</span><br><span class="line"></span><br><span class="line">    scores = np.dot(hidden_layer2, W3) + b3</span><br><span class="line">    predicted_class = np.argmax(scores, axis=<span class="number">1</span>)  <span class="comment"># 找出每一行的最大值的index</span></span><br><span class="line">    print(<span class="string">'training accuracy: &#123;0&#125;'</span>.format(np.mean(predicted_class==y)))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> plot_array_1, plot_array_2, W1, W2, W3, b1, b2, b3</span><br></pre></td></tr></table></figure><p>关于back propagation 的计算参见下图。 其实就是对chain rule的应用。</p><img src="/2020/06/02/vanishing-gradients/8.jpg" class=""><h3 id="Train-net-with-sigmoid-nonlinearity-first"><a href="#Train-net-with-sigmoid-nonlinearity-first" class="headerlink" title="Train net with sigmoid nonlinearity first"></a>Train net with sigmoid nonlinearity first</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize toy model, train sigmoid net </span></span><br><span class="line"></span><br><span class="line">N = <span class="number">100</span>  <span class="comment"># number of points per class</span></span><br><span class="line">D = <span class="number">2</span>  <span class="comment"># dimensionality</span></span><br><span class="line">K = <span class="number">3</span>  <span class="comment"># number of classes</span></span><br><span class="line">h = <span class="number">50</span>  <span class="comment"># hidden layer 1 size</span></span><br><span class="line">h2 = <span class="number">50</span>  <span class="comment"># hidden layer 2 size</span></span><br><span class="line">num_train_examples = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">model = &#123;&#125;</span><br><span class="line">model[<span class="string">'h'</span>] = h</span><br><span class="line">model[<span class="string">'h2'</span>] = h2</span><br><span class="line">model[<span class="string">'W1'</span>] = <span class="number">0.1</span> * np.random.randn(D, h)</span><br><span class="line">model[<span class="string">'b1'</span>] = np.zeros((<span class="number">1</span>,h))</span><br><span class="line">model[<span class="string">'W2'</span>] = <span class="number">0.1</span> * np.random.randn(h, h2)</span><br><span class="line">model[<span class="string">'b2'</span>] = np.zeros((<span class="number">1</span>,h2))</span><br><span class="line">model[<span class="string">'W3'</span>] = <span class="number">0.1</span> * np.random.randn(h2, K)</span><br><span class="line">model[<span class="string">'b3'</span>] = np.zeros((<span class="number">1</span>,K))</span><br><span class="line"></span><br><span class="line">(sigm_array_1, sigm_array_2, s_W1, s_W2, s_W3, s_b1, s_b2, s_b3) = three_layer_net(<span class="string">'SIGM'</span>, X, y, model, step_size=<span class="number">1e-1</span>, reg=<span class="number">1e-3</span>)</span><br></pre></td></tr></table></figure><p>training accuracy: 0.97</p><h3 id="Now-train-net-with-ReLU-nonlinearity"><a href="#Now-train-net-with-ReLU-nonlinearity" class="headerlink" title="Now train net with ReLU nonlinearity"></a>Now train net with ReLU nonlinearity</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Re-initialize model, train relu net</span></span><br><span class="line">model = &#123;&#125;</span><br><span class="line">model[<span class="string">'h'</span>] = h</span><br><span class="line">model[<span class="string">'h2'</span>] = h2</span><br><span class="line">model[<span class="string">'W1'</span>] = <span class="number">0.1</span> * np.random.randn(D, h)</span><br><span class="line">model[<span class="string">'b1'</span>] = np.zeros((<span class="number">1</span>,h))</span><br><span class="line">model[<span class="string">'W2'</span>] = <span class="number">0.1</span> * np.random.randn(h, h2)</span><br><span class="line">model[<span class="string">'b2'</span>] = np.zeros((<span class="number">1</span>,h2))</span><br><span class="line">model[<span class="string">'W3'</span>] = <span class="number">0.1</span> * np.random.randn(h2, K)</span><br><span class="line">model[<span class="string">'b3'</span>] = np.zeros((<span class="number">1</span>,K))</span><br><span class="line"></span><br><span class="line">(relu_array_1, relu_array_2, r_W1, r_W2, r_W3, r_b1, r_b2, r_b3) = three_layer_net(<span class="string">'RELU'</span>, X, y, model, step_size=<span class="number">1e-1</span>, reg=<span class="number">1e-3</span>)</span><br></pre></td></tr></table></figure><p>training accuracy: 0.9933333333333333</p><h2 id="The-Vanishing-Gradient-Issue-—-梯度消失的问题"><a href="#The-Vanishing-Gradient-Issue-—-梯度消失的问题" class="headerlink" title="The Vanishing Gradient Issue — 梯度消失的问题"></a>The Vanishing Gradient Issue — 梯度消失的问题</h2><p>我们可以对某一hidden层W的梯度（dW）进行加总，用这个简单的指标来衡量学习的速度，显然，sum(dW)越大，说明神经网络的学习速度越快。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(np.array(sigm_array_1))</span><br><span class="line">plt.plot(np.array(sigm_array_2))</span><br><span class="line">plt.title(<span class="string">'Sum of magnitudes of gradients -- SIGM weights'</span>)</span><br><span class="line">plt.legend((<span class="string">"sigm first layer"</span>, <span class="string">"sigm second layer"</span>))</span><br></pre></td></tr></table></figure><p><img src="3.svg" alt="svg"></p><p>由上图可见，第二层的梯度显著大于第一层。说明在进行back prop的时候，hidden层越多，那么排在最前的hidden层对应的梯度（dW）就会变得越来越小。直观的讲，因为chain rule的原因，hidden层越多，则chain rule里乘的local gradient越多，而另一方面，由于nonlinearity使用的是sigmod，sigmoid grad = δ*(1-δ)，输出必在[0,1]之间。所以local gradient的值都落在[0，1]，那么随着hidden层的增加，排在最前的hidden层对应的梯度（dW）就会变得越来越小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(np.array(relu_array_1))</span><br><span class="line">plt.plot(np.array(relu_array_2))</span><br><span class="line">plt.title(<span class="string">'Sum of magnitudes of gradients -- ReLU weights'</span>)</span><br><span class="line">plt.legend((<span class="string">"relu first layer"</span>, <span class="string">"relu second layer"</span>))</span><br></pre></td></tr></table></figure><p><img src="4.svg" alt="svg"></p><p>由上图可见，ReLU收敛的速度比sigmoid快很多，而且收敛后就很稳定。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># overlaying the 2 plots to compare</span></span><br><span class="line">plt.plot(np.array(sigm_array_1))</span><br><span class="line">plt.plot(np.array(sigm_array_2))</span><br><span class="line">plt.plot(np.array(relu_array_1))</span><br><span class="line">plt.plot(np.array(relu_array_2))</span><br><span class="line">plt.title(<span class="string">'Sum of magnitudes of gradients -- hidden layer neurons'</span>)</span><br><span class="line">plt.legend((<span class="string">"sigm first layer"</span>, <span class="string">"sigm second layer"</span>, <span class="string">"relu first layer"</span>, <span class="string">"relu second layer"</span>))</span><br></pre></td></tr></table></figure><p><img src="5.svg" alt="svg"></p><p>上图可以更明显的看到，ReLU的收敛速度快，一开始的gradient更高。</p><p>最后，看看2种分类器的表现，由于ReLU训练速度更快，因此用同样的epochs，ReLU表现的更好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot the classifiers -- SIGMOID</span></span><br><span class="line">h = <span class="number">0.02</span></span><br><span class="line">x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">1</span>, X[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line">y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">1</span>, X[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                     np.arange(y_min, y_max, h))</span><br><span class="line">Z = np.dot(sigmoid(np.dot(sigmoid(np.dot(np.c_[xx.ravel(), yy.ravel()], s_W1) + s_b1), s_W2) + s_b2), s_W3) + s_b3</span><br><span class="line">Z = np.argmax(Z, axis=<span class="number">1</span>)</span><br><span class="line">Z = Z.reshape(xx.shape)</span><br><span class="line">fig = plt.figure()</span><br><span class="line">plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=<span class="number">0.8</span>)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.Spectral)</span><br><span class="line">plt.xlim(xx.min(), xx.max())</span><br><span class="line">plt.ylim(yy.min(), yy.max())</span><br></pre></td></tr></table></figure><p><img src="6.svg" alt="svg"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot the classifiers-- RELU</span></span><br><span class="line">h = <span class="number">0.02</span></span><br><span class="line">x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">1</span>, X[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line">y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">1</span>, X[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                     np.arange(y_min, y_max, h))</span><br><span class="line">Z = np.dot(relu(np.dot(relu(np.dot(np.c_[xx.ravel(), yy.ravel()], r_W1) + r_b1), r_W2) + r_b2), r_W3) + r_b3</span><br><span class="line">Z = np.argmax(Z, axis=<span class="number">1</span>)</span><br><span class="line">Z = Z.reshape(xx.shape)</span><br><span class="line">fig = plt.figure()</span><br><span class="line">plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=<span class="number">0.8</span>)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=plt.cm.Spectral)</span><br><span class="line">plt.xlim(xx.min(), xx.max())</span><br><span class="line">plt.ylim(yy.min(), yy.max())</span><br></pre></td></tr></table></figure><p><img src="7.svg" alt="svg"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Vanishing-Gradients&quot;&gt;&lt;a href=&quot;#Vanishing-Gradients&quot; class=&quot;headerlink&quot; title=&quot;Vanishing Gradients&quot;&gt;&lt;/a&gt;Vanishing Gradients&lt;/h1&gt;&lt;p&gt;这里只使用numpy来实现一个neural network，而非借助pytorch这样的框架，如此，可以更好的帮助我理解neural network以及vanishing gradients.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Financial Machine Learning" scheme="https://www.keepswalking.com/categories/Financial-Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Bars part 1</title>
    <link href="https://www.keepswalking.com/2020/03/28/bars-part-1/"/>
    <id>https://www.keepswalking.com/2020/03/28/bars-part-1/</id>
    <published>2020-03-28T09:51:32.000Z</published>
    <updated>2020-03-28T12:26:36.820Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Financial-Machine-Learning-Bars"><a href="#Financial-Machine-Learning-Bars" class="headerlink" title="Financial Machine Learning - Bars"></a>Financial Machine Learning - Bars</h1><p>最近在啃Marcos Lopez de Prado的 Advances in Financial Machine Learning，感觉很有难度，我想通过笔记的形式将自己的理解慢慢记录下来。总体讲这本书给我很多的启发。个人感觉，网上很多所谓machine learning在finance和investment上的应用都是谬误的，其实这本书并不会讨论具体的算法，而更多的是提供一种machine learning在finance 应用上的标准流程或者说是方法论，懂得了这些流程/方法论并不能保证你就能写出赚钱的模型来，但是，可以帮助你避免很多错误和陷阱，从而节约了你很多的时间。本书的观点认为，ML在投资领域的应用犹如一种工业化的生产过程，在当今市场的有效性条件下，企图靠某个个人所谓的深厚功力写出一个盈利可观的模型变得越来越不切实际了，只有依靠大量的人力，标准化的流程才可能在高度有效的市场环境下生存。</p><a id="more"></a><p>该书首先从数据准备讲起。他的观点是，如果某项数据所有人都在用，那这个数据的价值就不大了。比如财报数据，当然，传统bar（K线）数据的价值也不大，一方面人人都在用，另外一方k线数据的采样是有缺陷的。举例来说，假如我们使用5分钟线，那么通常在每天开盘和收盘的时间段成交比较密集，也就意味着这段时间包含的信息量比较多。但是，传统的k线只做定期采样，造成的结果就是，各个5分钟k线包含的信息量是不等的，导致很多的信息量无法被体现出来。为了避免这样的缺陷，该书提出了以下几种bar：</p><ul><li><p>Tick Bars</p></li><li><p>Volume Bars</p></li><li><p>Dollar Bars</p></li><li><p>Imbalance Bars</p></li></ul><p>接下来我以沪深300指数的1分钟k线为基础，分别来看这些Bar有什么区别。</p><h2 id="Time-Bars"><a href="#Time-Bars" class="headerlink" title="Time Bars"></a>Time Bars</h2><p>Time bars就是指最常见的k线了。原始数据是1分钟k线，我可以将其合成为15分钟k线</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load data</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">"hs300.csv"</span>) <span class="comment">#沪深300 1分钟k线</span></span><br><span class="line"><span class="comment"># change column name</span></span><br><span class="line">data.rename(columns=&#123;data.columns[<span class="number">0</span>]:<span class="string">"timestamp"</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line">data[<span class="string">'timestamp'</span>] = pd.to_datetime(data[<span class="string">'timestamp'</span>])</span><br><span class="line"><span class="comment"># data['timestamp'] = data.timestamp.map(lambda t: datetime.strptime(t, "%Y-%m-%d %H:%M:%S"))</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_vwap</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="comment"># 因为groupby有upsampling的问题，所以，必须剔出这些额外增加出来的非交易时段的ts</span></span><br><span class="line">    <span class="keyword">if</span> df.isna().to_numpy().any():</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    q = df[<span class="string">'volume'</span>]</span><br><span class="line">    p = df[<span class="string">'close'</span>] <span class="comment"># use close price</span></span><br><span class="line">    vwap = np.sum(p * q) / np.sum(q)</span><br><span class="line">    df[<span class="string">'vwap'</span>] = vwap</span><br><span class="line">    <span class="comment"># print(df.head())</span></span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line">data_timeidx = data.set_index(<span class="string">'timestamp'</span>)</span><br><span class="line"></span><br><span class="line">sub_data = data_timeidx[<span class="string">'2015-05'</span>] <span class="comment">#选取2015年5月的数据</span></span><br><span class="line">data_time_grp = sub_data.groupby(pd.Grouper(freq=<span class="string">'15Min'</span>, closed=<span class="string">'right'</span>))</span><br><span class="line">num_time_bars = <span class="number">1</span></span><br><span class="line"><span class="comment"># 计算groupby以后一共有多少跟k线</span></span><br><span class="line"><span class="keyword">for</span> name, group <span class="keyword">in</span> data_time_grp:</span><br><span class="line">    <span class="keyword">if</span> group.empty == <span class="literal">False</span>:</span><br><span class="line">        num_time_bars += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">data_time_vwap = data_time_grp.apply(compute_vwap)    </span><br><span class="line">sub_df = data_time_vwap.copy()</span><br><span class="line">sub_df.index = sub_df.index.map(str)</span><br><span class="line">sub_df[<span class="string">'vwap'</span>].plot()</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x180d0b81f88&gt;</code></pre><p><img src="bars_4_1.svg" alt="svg"></p><h2 id="Tick-Bars"><a href="#Tick-Bars" class="headerlink" title="Tick Bars"></a>Tick Bars</h2><p>为了避免上述Time bar的问题，一种方法是构建tick bar。 所谓tick bar是以某个固定的tick数量来切分并生成对应的bar，例如：每个tick bar包含50个tick。这样做的好显而易见，就是当市场的某些时段交易很活跃的时候，就有更多的tick bar生成出来即更多的采样，反之，生成的tick bar的数量减少。因此，相较于传统的time bar，每个tick bar包含的信息更加均衡。</p><p>但是tick bar也有一个缺点，例如：一个tick对应10手买单，和10个tick，每个tick分别对应1手买单。但是，他们包含的信息量显然不一样，简单来说，前者可能是掌握了某些信息的市场参与者的行为，而后者可能更多的是一种随机行为。按照tick bar的生成方式，前者就无法被及时采样，并容易被其他低信息量的tick淹没。</p><p>由于我的原始数据是1分钟k线而非tick数据，因此无法合成tick bar。</p><h2 id="Volume-Bars"><a href="#Volume-Bars" class="headerlink" title="Volume Bars"></a>Volume Bars</h2><p>所谓的volume bar就是以某个固定的成交量来切分并生成对应的bar。例如：每个bar包含10000股的成交量。这样就改进了tick bar存在的问题，使得每个volume bar所含的信息量更为平均</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">data_cm_vol = sub_data.assign(cmVol=sub_data[<span class="string">'volume'</span>].cumsum())  <span class="comment">#计算总成交量</span></span><br><span class="line">total_vol = data_cm_vol.cmVol.values[<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># 令volume bar的数量和time bar的数量相同，便于显示的时候做比较</span></span><br><span class="line">vol_per_bar = total_vol / num_time_bars</span><br><span class="line">vol_per_bar = round(vol_per_bar, <span class="number">0</span>) </span><br><span class="line">data_vol_grp = data_cm_vol.assign(grpId=<span class="keyword">lambda</span> row: row.cmVol // vol_per_bar)</span><br><span class="line">data_vol_vwap =  data_vol_grp.groupby(<span class="string">'grpId'</span>).apply(compute_vwap)</span><br><span class="line"><span class="comment"># plot</span></span><br><span class="line">sub_df1 = data_vol_vwap.copy()</span><br><span class="line">sub_df1.index = sub_df1.index.map(str)</span><br><span class="line">plot_data = &#123;<span class="string">'time'</span>: sub_df[<span class="string">'vwap'</span>], <span class="string">'volume'</span>:sub_df1[<span class="string">'vwap'</span>]&#125;</span><br><span class="line">plot_df = pd.DataFrame(plot_data)</span><br><span class="line">plot_df.plot(alpha=<span class="number">0.6</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x180ce3cf788&gt;</code></pre><p><img src="bars_7_1.svg" alt="svg"></p><p>由上图可见volume bar在一些波峰和波谷的位置更加的显著，说明在偏极端的行情的环境下，volume bar保存的信息更多。如下所示的放大图。</p><img src="/2020/03/28/bars-part-1/enlarge1.PNG" class=""><h2 id="Dollar-Bars"><a href="#Dollar-Bars" class="headerlink" title="Dollar Bars"></a>Dollar Bars</h2><p>同Volume bar类似，所谓的Dollar bar就是以某个固定的成交金额来切分并生成对应的bar。例如：每个bar包含100000元。一个比较直观想法是：当上证综合指数在6000点买入100手股票同在1000点买入100手股票是完全不同的概念，由于在资金量上的巨大的差异，虽然手数相同，但是这两个决定所包含的信息应该是差异很大。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">data_cm_dollar = sub_data.assign(cmDollar=sub_data[<span class="string">'total_turnover'</span>].cumsum()) </span><br><span class="line">total_dollar = data_cm_dollar.cmDollar.values[<span class="number">-1</span>]</span><br><span class="line">dollar_per_bar = total_dollar / num_time_bars</span><br><span class="line">dollar_per_bar = round(dollar_per_bar, <span class="number">0</span>) </span><br><span class="line">data_dollar_grp = data_cm_dollar.assign(grpId=<span class="keyword">lambda</span> row: row.cmDollar // dollar_per_bar)</span><br><span class="line">data_dollar_vwap =  data_dollar_grp.groupby(<span class="string">'grpId'</span>).apply(compute_vwap)</span><br><span class="line"><span class="comment"># plot</span></span><br><span class="line">sub_df2 = data_dollar_vwap.copy()</span><br><span class="line">sub_df2.index = sub_df2.index.map(str)</span><br><span class="line"></span><br><span class="line">plot_data = &#123;<span class="string">'dollar'</span>: sub_df2[<span class="string">'vwap'</span>], <span class="string">'volume'</span>:sub_df1[<span class="string">'vwap'</span>]&#125;</span><br><span class="line">plot_df = pd.DataFrame(plot_data)</span><br><span class="line">plot_df.plot(alpha=<span class="number">0.6</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x180d1e57988&gt;</code></pre><p><img src="bars_10_1.svg" alt="svg"></p><p>上图是dollar bar 和 volume bar的比较。区别不是很大。可以看出，在一些比较极端的走势图形上，dollar bar和volume bar的采样还是有一定区别。如下图所示：</p><img src="/2020/03/28/bars-part-1/enlarge2.PNG" class=""><p>除了以上这些bar外，还有一种bar叫做Imbalance bar，由于相对比较复杂，所以会单独开一篇来介绍。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Financial-Machine-Learning-Bars&quot;&gt;&lt;a href=&quot;#Financial-Machine-Learning-Bars&quot; class=&quot;headerlink&quot; title=&quot;Financial Machine Learning - Bars&quot;&gt;&lt;/a&gt;Financial Machine Learning - Bars&lt;/h1&gt;&lt;p&gt;最近在啃Marcos Lopez de Prado的 Advances in Financial Machine Learning，感觉很有难度，我想通过笔记的形式将自己的理解慢慢记录下来。总体讲这本书给我很多的启发。个人感觉，网上很多所谓machine learning在finance和investment上的应用都是谬误的，其实这本书并不会讨论具体的算法，而更多的是提供一种machine learning在finance 应用上的标准流程或者说是方法论，懂得了这些流程/方法论并不能保证你就能写出赚钱的模型来，但是，可以帮助你避免很多错误和陷阱，从而节约了你很多的时间。本书的观点认为，ML在投资领域的应用犹如一种工业化的生产过程，在当今市场的有效性条件下，企图靠某个个人所谓的深厚功力写出一个盈利可观的模型变得越来越不切实际了，只有依靠大量的人力，标准化的流程才可能在高度有效的市场环境下生存。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Financial Machine Learning" scheme="https://www.keepswalking.com/categories/Financial-Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>AdaBoost Example</title>
    <link href="https://www.keepswalking.com/2020/02/23/AdaBoost-Example/"/>
    <id>https://www.keepswalking.com/2020/02/23/AdaBoost-Example/</id>
    <published>2020-02-23T14:08:03.000Z</published>
    <updated>2020-02-26T08:50:10.899Z</updated>
    
    <content type="html"><![CDATA[<p>我终于想到自己应该滚过来更新了。实在是因为考完CFA以后，整个人生好像没有了明确的目标，导致自己什么都想做，结果什么都做不好。这段时间自己的状态一直保持低迷，原因肯定是对自己各种的放纵。前阵子，德约科维奇又拿下来澳网的冠军，我就顺便看了一本他的自传，印象最深的是极度的自律。在拿到某次冠军后（好像也是澳网），他说他想吃一块巧克力庆祝一下（因为平时绝对不会碰这种东西，怕影响状态），结果就吃了一小口。只有能做到这样的自律，才能取得不平凡的成绩。</p><p>说了一堆废话，还是言归正传。前段时间正好用到random forrest就想顺便把AdaBoost也看一下，李航《统计学习方法》里有一个例子，但是我怎么都没看懂（智商不够用，顺便吐槽一下这本书，虽然很多人推荐，但我一直没觉得这本书好在哪里），于是就到网上找例子，到油管上看视频，结果发现能把AdaBoost讲明白的例子几乎没有。后来不知道哪天突然开悟了，李航那本书上的例子竟然被我看明白了！</p><a id="more"></a><p>我就直接上例子了，AdaBoost具体的理论还请自行脑补。训练数据如下表：</p><div class="table-container"><table><thead><tr><th>序号</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10 </th></tr></thead><tbody><tr><td>x</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9 </td></tr><tr><td>y</td><td>1</td><td>1</td><td>1</td><td>-1</td><td>-1</td><td>-1</td><td>1</td><td>1</td><td>1</td><td>-1</td></tr></tbody></table></div><p>x是特征，y是label。显然根据特征x对y分类，分为2种类型1，-1。我一开始一直没搞明白评价分类结果依据什么？后来才理解了评价分类的依据是各训练数据的权值！我们知道Adaboost会将分类错误的训练数据的权值提高，这样，在下一次弱分类器的训练过程中就可以重点“关照”这些被分错的数据了。那么，也就有可能出现以下这种情况：某个弱分类器可能错误率很高但却被采纳了，因为它可能将权重更高的训练数据进行准确地分类。参见以下训练过程</p><h3 id="m-1-训练第一个弱分类器"><a href="#m-1-训练第一个弱分类器" class="headerlink" title="m=1 (训练第一个弱分类器)"></a>m=1 (训练第一个弱分类器)</h3><p>对于第一个弱分类器而言，每个训练数据的权值w都是一样的，即1/N（初始情况下的默认值），N表示训练数据量。就本例而言，N=10，每个训练数据的权值w为0.1，在此前提下可以训练出一个最优的弱分类器：</p><script type="math/tex; mode=display">    G_1(x) =     \\begin{cases}    1,  & \text{x $<$ 2.5} \\\\    -1, & \text{x $>$ 2.5} \\\\    \\end{cases}</script><p>切分点为x=2.5，因为误差率（分错权值加总）最低，注意这里需要强调的是选择弱分类器的依据是误差率最低的那个，而不是分对的数量最多的那个（当然，权值相同的时候分对数量最高则代表误差率最低），我一开始没看明白就是在这里犯糊涂了。那么有3个数据被错误分类了（x=6，7，8），由于w都为0.1，因此误差率$e_1$（分错权值加总）=0.3。这个误差率是当前权值（w）下最小的。</p><p>计算第一个弱分类器的系数（权重），套用公式了，$ \alpha_1 = \frac{1}{2}log\frac{1-e_1}{e_1} $ = 0.5*ln((1-0.3)/0.3) = 0.4236 ； 直观的理解就是如果这个分类器的误差率越低，则在一系列弱分类器的组合里所占的权重越高。</p><p>更新训练数据的权值，也就是将分对的数据的权值调低，而将分错的数据的权值调高，计算权值的公式为：$w<em>{m+1, i} = w</em>{m, i}exp(-\alpha_m y_i G_m(x_i))$，计算结果如下：</p><div class="table-container"><table><thead><tr><th>序号</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10 </th></tr></thead><tbody><tr><td>y</td><td>1</td><td>1</td><td>1</td><td>-1</td><td>-1</td><td>-1</td><td>1</td><td>1</td><td>1</td><td>-1</td></tr><tr><td>$w_1$</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td></tr><tr><td>$G_1(x)$</td><td>1</td><td>1</td><td>1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td></tr><tr><td>$ \alpha_1 $</td><td>0.4236</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>$w_2$ without normalized</td><td>0.06547</td><td>0.06547</td><td>0.06547</td><td>0.06547</td><td>0.06547</td><td>0.06547</td><td>0.15275</td><td>0.15275</td><td>0.15275</td><td>0.06547 </td></tr><tr><td>Z</td><td>0.9165</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>$w_2$ normalized</td><td>0.07143</td><td>0.07143</td><td>0.07143</td><td>0.07143</td><td>0.07143</td><td>0.07143</td><td>0.16667</td><td>0.16667</td><td>0.16667</td><td>0.07143 </td></tr></tbody></table></div><p>以计算第一个$w_2$为例，原来的$w_2=0.1, \alpha=0.4236, y=1, G_1(x_1)=1$，也就是分类正确，那么可得: 0.1 <em> exp(-0.4236 </em> 1 * 1) = 0.06547<br>，标准化后为0.07143（$w_2$ 需要标准化，也就是$w_2$的sum为1，表中的Z为所有w的加总，那么normalized w = w without normalized / Z），因此权重降低了！。而对$x_7, x_8, x_9$来说，由于分错了，因此权重升高了。至此第一个弱分类器训练完成，可得：</p><script type="math/tex; mode=display">f_1(x) = 0.4236G_1(x)</script><h3 id="m-2-训练第二个弱分类器"><a href="#m-2-训练第二个弱分类器" class="headerlink" title="m=2 (训练第二个弱分类器)"></a>m=2 (训练第二个弱分类器)</h3><p>通过训练可得x=8.5最为最佳切分点，因为误差率最小，即：</p><script type="math/tex; mode=display">    G_2(x) =     \\begin{cases}    1,  & \text{x $<$ 8.5} \\\\    -1, & \text{x $>$ 8.5}    \\end{cases}</script><p>$G_2(x)$的误差率为$e_2 = 0.2143$。因为当以8.5为切分点的时候，4，5，6被错误分类了，而他们对应的权重$w_2$ 分别为：0.07143，0.07143，0.07143，即 0.07143 * 3 = 0.2143。如果选择8.5以外的切分点，误差率都比0.2143高。</p><p>同样的计算$\alpha_2 = 0.5*LN((1-0.2143)/0.2143) = 0.6496$</p><p>其余数据参见下表：</p><div class="table-container"><table><thead><tr><th>序号</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10 </th></tr></thead><tbody><tr><td>y</td><td>1</td><td>1</td><td>1</td><td>-1</td><td>-1</td><td>-1</td><td>1</td><td>1</td><td>1</td><td>-1</td></tr><tr><td>$w_2$</td><td>0.07143</td><td>0.07143</td><td>0.07143</td><td>0.07143</td><td>0.07143</td><td>0.07143</td><td>0.16667</td><td>0.16667</td><td>0.16667</td><td>0.07143 </td></tr><tr><td>$G_2(x)$</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>-1</td><td>-1</td></tr><tr><td>$ \alpha_2 $</td><td>0.6496</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>$w_3$ without normalized</td><td>0.0373</td><td>0.0373</td><td>0.0373</td><td>0.1368</td><td>0.1368</td><td>0.1368</td><td>0.0870</td><td>0.0870</td><td>0.0870</td><td>0.0373  </td></tr><tr><td>Z</td><td>0.8207</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>$w_3$ normalized</td><td>0.0455</td><td>0.0455</td><td>0.0455</td><td>0.1667</td><td>0.1667</td><td>0.1667</td><td>0.1061</td><td>0.1061</td><td>0.1061</td><td>0.0455 </td></tr></tbody></table></div><p>至此第二个弱分类器训练完成，可得：</p><script type="math/tex; mode=display">f_2(x) = 0.4236G_1(x) + 0.6496G_2(x)</script><p>如果，此时将数据喂给$f_2(x)$，可得以下结果：</p><div class="table-container"><table><thead><tr><th>序号</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10 </th></tr></thead><tbody><tr><td>x</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9 </td></tr><tr><td>y</td><td>1</td><td>1</td><td>1</td><td>-1</td><td>-1</td><td>-1</td><td>1</td><td>1</td><td>1</td><td>-1</td></tr><tr><td>$f_2(x)$</td><td>1.0732</td><td>1.0732</td><td>1.0732</td><td>0.226</td><td>0.226</td><td>0.226</td><td>0.226</td><td>0.226</td><td>0.226</td><td>-1.0732</td></tr><tr><td>$sign(f_2(x))$</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>-1</td></tr></tbody></table></div><p>由y同$sign(f_2(x))$比较可知，一共有3个误分类点：4，5，6。</p><h3 id="m-3-训练第三个弱分类器"><a href="#m-3-训练第三个弱分类器" class="headerlink" title="m=3 (训练第三个弱分类器)"></a>m=3 (训练第三个弱分类器)</h3><p>通过训练可得x=5.5最为最佳切分点，因为误差率最小，即：</p><script type="math/tex; mode=display">    G_3(x) =     \\begin{cases}    1,  & \text{x $>$ 5.5} \\\\    -1, & \text{x $<$ 5.5}    \\end{cases}</script><p>$G_3(x)$的误差率为$e_3 = 0.1818$。因为当以5.5为切分点的时候1, 2, 3, 10被错误分类了，而他们对应的权重$w_3$ 分别为：0.0455, 0.0455, 0.0455, 0.0455，即 0.0455 * 4 = 0.1818。</p><p>同样的计算$\alpha_3 = 0.5*LN((1-0.1818)/0.0.1818) = 0.7520$</p><p>至此第三个弱分类器训练完成，可得：</p><script type="math/tex; mode=display">f_3(x) = 0.4236G_1(x) + 0.6496G_2(x) + 0.7520G_3(x)</script><p>如果，此时将数据喂给$f_3(x)$，可得以下结果：</p><div class="table-container"><table><thead><tr><th>序号</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10 </th></tr></thead><tbody><tr><td>x</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9 </td></tr><tr><td>y</td><td>1</td><td>1</td><td>1</td><td>-1</td><td>-1</td><td>-1</td><td>1</td><td>1</td><td>1</td><td>-1</td></tr><tr><td>$f_3(x)$</td><td>0.3212</td><td>0.3212</td><td>0.3212</td><td>-0.526</td><td>-0.526</td><td>-0.526</td><td>0.978</td><td>0.978</td><td>0.978</td><td>-0.3212</td></tr><tr><td>$sign(f_3(x))$</td><td>1</td><td>1</td><td>1</td><td>-1</td><td>-1</td><td>-1</td><td>1</td><td>1</td><td>1</td><td>-1</td></tr></tbody></table></div><p>由y同$sign(f_3(x))$比较可知已经无错误分类点！因此，可以认为达到了精度要求，无需再进一步训练。</p><p>以上就是对李航《统计学习方法》里关于AdaBoost例子的详细解释，作此笔记以备今后复习。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我终于想到自己应该滚过来更新了。实在是因为考完CFA以后，整个人生好像没有了明确的目标，导致自己什么都想做，结果什么都做不好。这段时间自己的状态一直保持低迷，原因肯定是对自己各种的放纵。前阵子，德约科维奇又拿下来澳网的冠军，我就顺便看了一本他的自传，印象最深的是极度的自律。在拿到某次冠军后（好像也是澳网），他说他想吃一块巧克力庆祝一下（因为平时绝对不会碰这种东西，怕影响状态），结果就吃了一小口。只有能做到这样的自律，才能取得不平凡的成绩。&lt;/p&gt;
&lt;p&gt;说了一堆废话，还是言归正传。前段时间正好用到random forrest就想顺便把AdaBoost也看一下，李航《统计学习方法》里有一个例子，但是我怎么都没看懂（智商不够用，顺便吐槽一下这本书，虽然很多人推荐，但我一直没觉得这本书好在哪里），于是就到网上找例子，到油管上看视频，结果发现能把AdaBoost讲明白的例子几乎没有。后来不知道哪天突然开悟了，李航那本书上的例子竟然被我看明白了！&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://www.keepswalking.com/categories/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Lasso Regression Notes</title>
    <link href="https://www.keepswalking.com/2019/09/08/lasso-regression-notes/"/>
    <id>https://www.keepswalking.com/2019/09/08/lasso-regression-notes/</id>
    <published>2019-09-08T09:48:58.000Z</published>
    <updated>2019-09-08T12:16:28.512Z</updated>
    
    <content type="html"><![CDATA[<p>这周读了一篇论文《基于LASSO和神经网络的量化交易智能系统构建》，借机看了一下Lasso的概念。做点笔记以备复习。</p><a id="more"></a><p>Lasso 全称Least absolute shrinkage and selection operator。主要的作用是regularization，也就是防止overfitting。通常用在linear regression和logistic regression里。说到regularization，另一种方式是ridge regression。两种表现形式如下：</p><ul><li>Ridge:<img src="/2019/09/08/lasso-regression-notes/ridge.PNG" class=""></li></ul><ul><li>Lasso:<img src="/2019/09/08/lasso-regression-notes/lasso.PNG" class=""></li></ul><p>Ridge和Lasso最重要的区别是，Ridge regression只能将不相关variables的beta减小至接近0；而Lasso regression则可以将beta减小至0。另外，需要注意的是Ridge Regression是无偏的，而Lasso则是有偏的（biased）。</p><p>那为何要用lasso呢？由于Lasso的特性—能够将无关variables的beta减少至0，因此它具有feature selection的能力。它能自动识别出哪些variables对模型的预测起到作用。</p><p>下面这副图解释了为何Lasso能将无关variables的beta减少至0。</p><img src="/2019/09/08/lasso-regression-notes/lasso_ridge.png" class=""><p>右边是Ridge regression，左边是Lasso regression。等高线的中点β hat代表了最优拟合点，但是，这个点通常是会造成overfitting的。为了避免overfitting，必须对beta的取值做出限制。也就是蓝色的区域。由图可见，Lasso区别Ridge的地方在于Lasso的beta取值范围是一个正方形，而Ridge则是圆形。在加入了regularization后的最优点一定落在红色等高线和蓝色区域相切的位置。而Lasso的切点一般会是正方形的顶点。也就是说，某些variables的beta取值为0。因此，Lasso具有feature selection的能力。</p><p>在看下图：</p><img src="/2019/09/08/lasso-regression-notes/lasso-reduce.png" class=""><p>横轴代表了λ（hyper parameter）的取值范围，纵轴代表了variables的数量。从图上可以看到，在λ取值较小的时候，所有的variables都包括在了模型内，而随着λ的增加，模型里的variables不断减少。因此，当遇到variables非常多的情况（模型复杂），Lasso是可以帮助识别出哪些variables对模型影响最大。</p><p>如何确定λ的值？通常使用cross validation的方式。根据不同的λ在cross validation set上所得到的误差来确定合理的值。</p><p>最后还是要吐槽一下国内博士的水平。个人觉得这篇论文质量不怎么样，也许是期刊《投资研究》本身水平不行？该博士第一步用linear regression + lasso 来选择feature（各种股票技术指标），第二步，这些通过lasso筛选出来的featrue被用于神经网络。最后得出的结论是lasso + 神经网络在预测股市涨跌方面的效果最好，sharpe ratio最高。我能看到的一个明显的逻辑上的问题是lasso是用在linear regression上的，也就是说这一步feature selection体现的线性相关性。而神经网络可用于非线性关系的发掘。那么在第一步中筛选出来的是线性显著的技术指标，这一步很可能把非线性的关系给过滤掉了。然后在第二步中，把这些线性显著的技术指标在输入给神经网络，那是希望从已知的线性关系中进一步发掘非线性关系？！觉得逻辑上挺混乱的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这周读了一篇论文《基于LASSO和神经网络的量化交易智能系统构建》，借机看了一下Lasso的概念。做点笔记以备复习。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://www.keepswalking.com/categories/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>CFA Level III 准备过程 【2019年6月】</title>
    <link href="https://www.keepswalking.com/2019/08/25/How-did-I-prepare-CFA-level-III/"/>
    <id>https://www.keepswalking.com/2019/08/25/How-did-I-prepare-CFA-level-III/</id>
    <published>2019-08-25T09:48:33.000Z</published>
    <updated>2019-08-25T12:14:00.304Z</updated>
    
    <content type="html"><![CDATA[<p>很幸运一次性过了CFA level III, 一二三级连续一次过关，真的是上天保佑了。<br><a id="more"></a><br>先说说考试本身。上午是essay，下午同1，2级一样是选择题。相对一二级来说，我三级的准备不是太充分。有一部分事先准备的复习资料最后也是没有时间去看了。因此上考场的时候心里就有点虚的。上午的essay可谓是悲惨啊，一上来就卡住了，结果花了1个小时前2题都没做完，当时心里就毛了，然后开始赶时间，后面也是有几个地方做不出，最后一大题彻底没时间了，只完成了第一个小问题。考完当时的第一反应就是：我也算是坚持到最后一秒了，没有放弃！但是，心里觉得这次考试很玄了。整个上午可以用梦游或魂不守舍来形容。中午休息的时候，依旧看到很多同学拿着资料在复习，我对这些人的敬佩之心如滔滔江水。已经考了3个小时了，还不休息一下？！下午还有3个小时啊。。。这时候还在复习，下午还有精力吗？反正我是觉得中午还是不要看什么资料了，好好休息休息，迎接下午的考试。下午进入考场的时候，发现原本坐我右边的同学撤了。。。我猜估计上午也遭到打击了！下午的难度明显低了很多，做题的速度因此变的很快。零星碰到一点有难度的题目，反正不是很多。因此，下午整个人的状态就很放松，不如上午来的那么崩溃了。不过还是受到了打击，还是源自上午的essay！我下午做题的时候，突然发现，计算器上计算PV的模式设置成了annuity due!!!当时如五雷轰顶啊～～～上午有2题是需要用到pv计算的，而且只需要普通的模式就行了，当时两眼一黑，想完了，上午肯定是一塌糊涂了。然后我也只能调整心态，不去想这件事了。所以，各位同学，必须要吸取我的教训，一定注意自己计算器的pv模式是否设置对了。</p><h2 id="关于复习资料"><a href="#关于复习资料" class="headerlink" title="关于复习资料"></a>关于复习资料</h2><p>老规矩，官方教材我还是老老实实看了一遍的。不过ethics我这次直接放弃了。因为，我在level II的时候花了很多的时间去复习这部分，结果成绩还是很差！这次我干脆就放弃了。ethics的内容相当多，handbook就有100多页，我觉得通读对提高我的成绩没有帮助，性价比太低了，因此3级复习ethics直接忽略，只是做书后习题+mock上的题目。事实证明这么做是对的，因为，成绩出来ethics考的不错。官方教材的所有习题我做了2变。</p><p>然后的Notes，依旧从网上下的:(  看了一遍。有些章节看了多变。这里需要指出的是3级的复习其实是有陷阱的。粗看好像3级没啥特别难的地方，看教材，看Notes看完了觉得好像不难，但是，看完了又觉得自己好像也没记住什么东西，再看一遍的时候，觉得好像新的内容一样。我觉得之所以会有这样的情况和三级的侧重点有关系。1，2级的考点其实很具体，尤其涉及计算的，一旦你掌握了，基本不会忘记。但是，3级以portfolio manager的视角，关注的是如何给client定制资产管理的方案，里面的知识点都是围绕这个主题。相对high level，概念性的东西偏多。因此，造成了说看完一遍感觉不难，但是，要记住这些概念，并能理解使用这些概念那就非常难了！一方面是内容多，一方面是要会应用，真的不容易的。我到后期觉得再通看notes并不会给我带来多少提升，所以还是以刷题为主。</p><p>再次是Mock题目了，同2级一个道理，历年Mock题目的重复率太高了（下午考试部分），我从网上找了2010年开始的历年mock，做了几套，觉得重复率太高了，再这么做下去没啥意义。而且难度差异巨大。因此，个人觉得Mock的成绩没太多参考价值。我的建议是，Mock题目如果时间充裕那就做，如果不充裕的，那还是挑近几年的做一下了。2019年的mock题目我觉得超级难。。。</p><p>再后是essay真题了（CFA网站提供近3年的），这块强烈建议大家重点关照！！！我找了从2005年开始的essay真题。由于复习时间不够用了，只做了1遍，再回扫了一部分。个人觉得做一遍肯定不够的。因为，essay是新题型，常考的范围是哪些，如何组织你的答案，这些都需要不断练习的。切记不是说你写的多了就安全了！这种想法是危险的，首先是时间，你会发现essay的时间非常紧张的，历年的真题我也基本来不及做，到了考场，我也依旧来不及，最后一大题没时间做。其次，答案如果回答不到点，不给分的。其实essay重点就是回答要简洁，切题！语法无所谓。我建议历年真题最好能反复做个3遍，很有用。虽然不会考同样的题目，但是，真题的难度毋庸置疑，真题的答案是需要自己好好分析总结的，因此绝对有价值。</p><p>最后，我这次花了血本，用了200元买了Kaplan出的practice exam！一共4套题目。我个人也是推荐的。题目的质量真心不错，难度接近考试。更重要的是，答案里提供了分值的计算方式。比如，essay的分值是如何计算的。这对我考试有很大的帮助。有些题目最终答案我不知，但是，我知道把一些相关的分析和计算写上去肯定也是得分的。复习资料就这些了。</p><h2 id="复习"><a href="#复习" class="headerlink" title="复习"></a>复习</h2><p>我没上任何的复习班，还是坚持自己的观点：没必要（估计那些培训机构要吐血了）。时间一定要留足。尤其尤其是刷题的时间！！！一个月刷题是不够的！！！我只留了1.5个月来刷题，事实证明是不够的。essay历年真题最好能做个3遍，Kaplan practice exam至少2遍，再加历年mock选择题，1个月怎么也不够的。试想，一天3个小时复习，也就够做一份essay或下午的选择题（而且3小时肯定不够）。再加上对答案，分析，总结至少2个小时，也就是说一份考卷需要差不多5个小时。光essay就有10多份！需要自己一开始就能估计好刷题的时间。切记留足刷题的时间。</p><p>通读教材和notes的效果不好，有时间可以多读几遍，没时间的话还通过刷题来找自己的弱项吧！</p><p>和1，2级一样，强调笔记的重要性！我的那本笔记本真的是被我翻烂了，literally！</p><h2 id="考试"><a href="#考试" class="headerlink" title="考试"></a>考试</h2><p>这块大家经历了1，2级都经验很丰富了，也没啥好多写的。重点还是合理安排答题时间。我这次essay来不及就是因为一开始的2道大题卡住了。当时，没有很好的调整自己的心态，事后觉得应该马上跳过，做后面。但是，考场上，头脑一热，觉得自己肯定能做出来啊！就一直卡在那里。也有部分原因是因为新题型，如果是选择题，可能也就跳过了。但是，碰到这种新题型，脑子就有点发晕了。</p><p>带个耳塞，因为二级的时候有惨痛的教训，很奇怪，早上开考以后，外面会有隐隐约约广播的声音，严重影响我集中注意力，可能我对噪音比较敏感。这次带了3M的耳塞，效果还不错。由于早上essay是直接做在考卷上的，周围都是翻页的声音，带了耳塞感觉安静很多。</p><h2 id="心路历程"><a href="#心路历程" class="headerlink" title="心路历程"></a>心路历程</h2><p>还是非常幸运的，自己能一口气连过3级！你要问我难不难，我的回答是难！难在哪里？未必说考试本身有多难。但是，要在近3年的时间里，有工作的情况下，每天还需要留出2，3个小时复习，并且是专注地复习，这真的是太难了！！！2，3级准备期间都发生了很多分心的事情，2级的时候差点崩溃，literally，之前很难理解为啥有些人会因为一件事情变成疯子或是自杀，但是考2级的时候接二连三的打击，真真切切感受到自己不堪再承受的临界点，因此也可以理解那些做出极端行为的人，他们的行为真的已经不受自己控制了。3级的阻碍也不少，工作上的烦心事、家人住院、自己又忙装修，还有自己的健康。各种烦心的事情接二连三，导致了复习断断续续。那时内心很挣扎，到底该怎么办？其他准备考试的人肯定比我复习的好很多。后来我对自己说，即使牛人碰到这些事情肯定也是会受影响的，因此我复习受到影响也是很正常的，不能要求自己在这样的客观条件下还能保持专注高效的复习，因此，我可以允许我自己在种情况下不坚持复习，因为即使坚持也没有啥效率。那就干脆安安心心地处理这些烦心事，等处理完成了再回来继续专注复习。鼓励自己不要放弃。也保持弹性。这种情况下复习到哪里是哪里，但是keep walking不要放弃！这里必须感谢家人尤其是妻子。没有她的默默支持，我根本不可能完成考试。自己在压力之下也会脾气失控，更要感谢妻子和家人的包容。</p><p>越是临近考试，越感到自己濒临极限了。有那么几次周末根本连书都不想翻，一开始复习就想吐了。真是强弩之末了。因此干脆也就只能看看电视休息休息。最后两周请假在家复习，也是碰到意外事件，丈母娘紧急送医院。可能是压力太多或是神经衰弱，睡眠也开始出现了问题，只能靠吃安眠药。我承受压力的能力就是这么差。</p><p>这段时间自己一直在调整恢复，也在思考，考CFA的整个过程带给我什么？我想整个过程让我更清楚地知道了我是谁。近3年的压力，回看自己的表现，更清楚自己几斤几两，例如承受压力的能力，体力，智力乃至情商。从而能让自己更加心平气和，接受自己的平庸。清楚看到自己和牛人之间的差距之大。因此，也就不应该对自己有不切实际的期望和要求。就这点来看整个过程还是很值的。在收到考试成绩之前，我也在思考，如果没过要不要继续？我自己也没想好。觉得如果继续，那是有点浪费时间了，因为，我可以将这些时间做更有意义的事情上去，如管理自己的portfolio，看看我有兴趣的书。考完以后，整个人的状态自由落体。什么都不想做，什么都不想看。幸运的是自己通过了level 3，菩萨保佑！接下来我想多花些时间在家人身上，也要规划一下该如何前行了，keep walking！</p><p>最后祝大家复习顺利，考试成功！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;很幸运一次性过了CFA level III, 一二三级连续一次过关，真的是上天保佑了。&lt;br&gt;
    
    </summary>
    
    
      <category term="CFA" scheme="https://www.keepswalking.com/categories/CFA/"/>
    
    
  </entry>
  
  <entry>
    <title>CFA Level II 准备过程 【2018年6月】</title>
    <link href="https://www.keepswalking.com/2018/08/16/How-did-I-prepare-CFA-level-II/"/>
    <id>https://www.keepswalking.com/2018/08/16/How-did-I-prepare-CFA-level-II/</id>
    <published>2018-08-15T16:00:00.000Z</published>
    <updated>2018-08-25T10:13:21.403Z</updated>
    
    <content type="html"><![CDATA[<p>很幸运一次过了CFA level II，算下来差不多一年半的时间连续过了levelI和level II, 接下来的目标就是level III了，不过说实话觉得好累，对于level III好像没有那么高的passion了，真想好好休息一年。因为考level II的时候真的感觉压力太大了，压力主要来自要同时处理很多的事情。如今准备考试，不像在学校，可以安心看书。我需要同时面对很多事情，不断切换，而且其他事情会影响心情，每次都需要逼着自己去集中注意力在CFA复习上，那是非常非常疲劳和折磨的，而且还是一个长期的过程。我不像网上的有些人花3，4个月就能搞定，我没那么大能力，必须拉长准备的周期。</p><a id="more"></a><p>我差不多从9月份就开始准备了。先说说CFA 2级的难度。我一开始也看到网上考过的人说过二级比一级难很多很多很多，甚至有说难10倍的。当时我觉得那些人就是夸张，能有多难呢？后来自己开始准备的时候就闷逼了。真的是很难很难很难！而且我觉得即使没有难10倍，但难个5倍也是有的！！！所以这里需要提醒大家，如果你觉得自己基础很一般的话，那么真的要多留一些时间的！</p><p>同level I一样，level II我也没参加学习班，我的观点就是没有意义，你也并不知道这些所谓的学习班水平到底如何。而且说白了教材就在那，考试范围也都明确写出来了，我更相信自己。CFA范围很广，靠猜题我更是觉得不靠谱。教材方面依旧是官方的电子版，我成了IPAD的重度用户，说个励志的事情：被我看坏一本IPAD！！！哈哈，是不是真的被我看坏的我不知道，但是确确实实坏掉了，所以，大部分level II的教材阅读是在新的IPAD上完成，够励志吧！教材之外是notes。我是教材看完一遍，一些对我来说很难的地方，教材是看了多遍的，比如Financial Reporting。Notes更是看了无数遍了。。。如果大家没有时间的话那就看Notes吧，很精简。</p><p>二级从教材的内容角度来看，量化部分难度增加了不少，比如如何判断数据间是否相关，如果判断是否有异方差，如何判断unit root，如何判断serial correlation，如何做F test，autoregressive （AR）model等。总之，这部分我自己笔记记了一大把。economics里面汇率的三角套利。corparate finance里关于项目的评估，这部分level I里很简单，就是比较NPV，但是level II里，计算项目的NPV就复杂的多了。Financial reporting这部分我自己觉得是难度增加的最高的一块了。这些topic对我来说都是很高级的，包括了公司间投资，比如一家公司投资另一家公司，可以有三种类型，每种类型在报表上该如何体现。employee compensation是关于年金如何计算并体现在财报上的。还有一大块是关于跨国公司如何将不同的币种收入计入在报表上的。总之，财务报表分析这部分我一直是觉得很迷茫，看了无数遍的教材和notes，慢慢的消化理解。equity这块是level II的大头，但是其实这块从概念上来说没什么新东西，增加了一些新的价值估算的方法，比如free cash flow valuation，residule income valuation等。剩下的fix income, derivatives, alternative investment和portfolio management都难，真的！反正就是不断重复的看notes。</p><p>这里想插一些学习CFA，尤其是level II以后的感想。网上有说CFA没用的。但是，我自己的感觉就是CFA的内容编排贴近实际，对实践很有帮助。尤其是财务报表，corparate finance和equity，对于理解实际股票估值很有帮助。比如我现在看《巴菲特致股东信》就觉得很有收获。看一些价值投资方面的书籍，比如《巴菲特之道》，《聪明的投资者》等觉得也容易理解。总之，我觉得通过学习CFA，对我在理解投资方面的帮助是很明显的，虽然过程很枯燥。CFA关于量化和Portfolio Management方面的知识更是对我有特别的帮助，因为我自己走的是量化的方向，对资产配置的兴趣大过个股分析，我觉得CFA上的这些知识极大的提高了在阅读一些论文和量化专业书籍上的效率。可以这么说，没学CFA之前，我看《QUANTITATIVE EQUITY PORTFOLIO MANAGEMENT》(作者Chincarini)这本书的时间没啥特别感觉，很多东西无法深入理解，但是学了CFA二级以后，再看这本书，真切感觉到对书里的内容理解更深刻了。而且，自己也有机会接触了更多的portfolio management的内容，比如，我也实践了risk parity。可以说，没有在CFA上所投入的时间和精力，就不可能让我形成自己的投资理念，也不会让我更好地看懂各类投资方面的书籍包括量化和价值投资！</p><p>说一下Ethical部分的复习，handbook我看了大概有2～3遍了，但是这次考二级得了个C！我觉得Ethical部分投入的时间是性价比最低的。如果我知道自己花了那么多的精力和时间还是得了C，那我一定是会将时间用到其他部分的。所以，我觉得，对于时间不够的朋友们来说，Ethical不如放弃了。真的性价比太低了。</p><p>还是强调自己的笔记的重要性，将你不懂的部分，容易做错的题目等一一记录到你的笔记本上，我记了有50多页啊！这些是你后期复习的重点。CFA内容那么多，想要从头到尾多看几遍显然不现实，最后阶段就是挑自己最没有把握的部分重点加强，而你的笔记能让你有的放矢。一定一定要记笔记。</p><p>关于考试时间，我上下午都没来得及做完所有题目，尤其是上午，最后一个case就做了一题，其它就只能涂成b应付了事了。下午有2题来不及，也用了同样的方法。而我在做mock的时候基本上能剩余20分钟样子，所以，我上考场的时候有意放慢了节奏，希望自己不要粗心大意，出现低级失误。但是，实际考试的题目难度和mock比，我自己觉得并不会低的。所以，还是要控制自己的节奏，尽量做的快一点，遇到需要思考的题目，或明显没有思路的题目应该先跳过。我没有这么做，导致最后15分钟的时候很慌乱，有些题目根本就没有精力去应付了，反而会造成失误，把一些应该会做的题目都做错了，让我后悔不已！如果最后能有30分钟的时间来处理一些前面遗留的题目，那么心态上就不一样了，放松会让自己更好的来应对这些题目。我在level I的时候就是这样，效果很好。但是这次真的是做的很不好。不过也要提醒一句，那就是二级的题目确实是有难度的。</p><p>准备CFA二级的时候，我也收集了历年mock题目（从2010年开始），原本想复制一级时候的策略。但是，结果告诉我，这样做的效果不大。首先，很明显level I的时候每一年有240题，从概率角度来说题型，知识点会覆盖的比较全面，因此将此作为复习重点是事半功倍的！但是到了二级，情况不一样了，level II每一年只有120题，而知识点和level I比起来一点也不少，甚至更多。所以从覆盖知识点和题型的角度来说肯定没有level I的mock来的效果好。其次，更要命的是，level II每年的mock题目大部分都是相同的！！！所以，我最后还是去重新做了一遍教材上的习题。</p><p>最后，还是应该感谢家人，尤其是妻子的支持与鼓励。今年18年真的是比较难熬的一年，就在考试前一段时间，甚至是离考试只剩下一周的时间，各种烦心事，打击，分心的事情，让我真的有濒临崩溃的感觉。有了妻子和家人的支持，让我熬过来了，至少我走进了考场，完成了考试！很幸运，最终也是过了。我想说，很多事情是必须要熬的，就是坚持。第一遍看教材还觉得可以学到很多新的东西，但是后面就是不断重复，很枯燥，我时常觉得是浪费时间，有这些时间是不是可以做更有意义的事情呢？比如看更多quant方面的书。但是，我后来觉得，其实就是因为这样的重复看教材，看notes，做题，才让我把很多知识掌握扎实。而光靠看一遍教材，很多地方其实是不能理解的。花掉的这些时间其实是在帮我打基础，是非常值得的。制定目标之前先把意义之类的事情想清楚了，最好能写下来，目标一旦定了，就不要犹豫怀疑，想东想西了。就是埋头干，直到达成目标。中间会有怀疑，犹豫和放弃的念头。我觉得有这些想法是很正常的，反正我是有的。比如，得知炒币的都是什么一两百倍的收益，那我是不是该去炒币啊，或者一边复习一遍炒币呢？哈哈！但是，冷静下来想想，我想还是CFA对于我来说更有意义，因此，也就放弃了“加入”币圈的念头。我不知道这样的决定是否正确，但是，我知道，我的能力只适合一步一步走，一口一口吃饭。既要。。。又要。。。还要。。。，臣妾做不到啊！</p><p>每个人情况都不一样，大家还是要结合自己的实际情况，找到最适合自己的复习CFA level II的方法。最后，祝大家复习顺利，考试成功！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;很幸运一次过了CFA level II，算下来差不多一年半的时间连续过了levelI和level II, 接下来的目标就是level III了，不过说实话觉得好累，对于level III好像没有那么高的passion了，真想好好休息一年。因为考level II的时候真的感觉压力太大了，压力主要来自要同时处理很多的事情。如今准备考试，不像在学校，可以安心看书。我需要同时面对很多事情，不断切换，而且其他事情会影响心情，每次都需要逼着自己去集中注意力在CFA复习上，那是非常非常疲劳和折磨的，而且还是一个长期的过程。我不像网上的有些人花3，4个月就能搞定，我没那么大能力，必须拉长准备的周期。&lt;/p&gt;
    
    </summary>
    
    
      <category term="CFA" scheme="https://www.keepswalking.com/categories/CFA/"/>
    
    
  </entry>
  
  <entry>
    <title>Trump税改的宏观分析</title>
    <link href="https://www.keepswalking.com/2017/12/07/Trump-tax-reform/"/>
    <id>https://www.keepswalking.com/2017/12/07/Trump-tax-reform/</id>
    <published>2017-12-07T13:21:53.000Z</published>
    <updated>2017-12-07T13:25:05.320Z</updated>
    
    <content type="html"><![CDATA[<p>每次市场上出现一些热点事件后，通常都是众说风云，自己看了这些新闻，以及网上大V们的论述之后往往觉得是一头雾水，也没有办法分清楚谁对谁错。最近想想，觉得其实自己有武器却没有利用起来啊，毕竟也是学过宏观经济学的人，教材里面清清楚楚的把分析框架提供给我了，而我却视而不见，反倒是迷失在各种标题党中。想想觉得自己也是蛮愚蠢的。情愿相信不可靠的消息和评论，却不依靠教材上提供的可靠的框架。经常听见说教材上的东西都是过时的，没有实际作用的，我自己也很轻易就相信了。但是，稍微分析一下：首先：教材上的东西都是经典，是经过时间积累下来的精华部分，如果没有用怎么会写进教材呢？其次，这些经典理论的作者都是大家，比我要聪明无数倍了，我没有理由不去使用他们。最后，反省一下我看了那么多所谓大V的文章有收获和进步么？</p><a id="more"></a><p>Trump的税改于2017/12/02日在参议院通过。税改的主要目的就是减税，而减税作为一项财政政策，会对宏观经济，包括：GDP，利率，汇率，通胀，股市造成哪些影响呢？接下来就根据教材上提供的框架来一一分析之。</p><p>宏观分析的时间纬度是一个非常重要的着眼点，很多时候大家为了某些观点争论不休，其原因可能就是考察的时间纬度不同。拿长期的观点和短期的观点进行比较显然会造成各说各的。那么，首先从短期的时间纬度进行分析，从短期来看，需求决定了产出（短期究竟是多久？答案是不一定，有可能几个月，也有可能几年，主要看决定长期的因素—price level有没有开始发生改变，如果price level开始改变，那么短期的作用也就over了）。刚才提到了减税是一项财政政策，能刺激需求端。使用短期的分析框架：IS-LM模型，如下图所示：</p><img src="/2017/12/07/Trump-tax-reform/IS-LM.png" class=""><p>从上图可见，因为短期内由于减税，消费者的可支配收入增加，进而增加消费，通过乘数效应进一步增加产出。也就是说IS curve右移至IS’。那么，在来看LM curve，从美联储的货币政策来看，当前处在加息通道中，那么从保守的角度来说，LM保持不变，而如果美联储在2018年继续加息的话，其实LM是该向上平移的。由此，我们从IS-LM模型可知：产出（GDP）会比当前增加，而利率会上升，而利率上升会导致货币升值，也就是美元汇率也上涨。</p><p>接下来分析一下减税的长期情况。这里有一个概念叫做“natural level of output”。从短期来看output是可以偏离natural level of output的，但是长期来说是要回归的，因为price level是会做出调整的。之所以会调整是因为，劳动力市场本身也存在一个natrual rate of umemployement，由于短期的财政政策的刺激导致失业率低于这个natrual rate, 使得工资上升，从而导致物价上升。从长期角度来说，可以使用AS-AD模型进行分析。影响AS curve主要有以下这些因素：</p><ul><li>工资水平；这块要关注的是工资增长率，如果工资增长速度加快，那么就会影响到price level，使得AS curve向上平移。目前来看美国工资的增长相对平稳，所以多AS curve的影响不大。</li><li>原油价格；这一项其实应该算做是短期因素，原油价格最近上涨较快，就原油价格对AS curve影响主要考虑原油价格的上涨是永久的还是短期波动。我更倾向于是一种短期的波动，那么对AS的影响不会很大。</li><li>汇率；这项是短期因素还是长期因素呢？如果产生一个趋势，那么影响的时间就会比较长了。从短期分析可知，美元汇率是会受到减税的影响而上涨，那么，上涨的美元会对美国的price level产生抑制作用。</li></ul><p>另外，还要提一下财政赤字，此次降税如果没有伴随财政紧缩，所以会造成财政赤字上升。财政赤字的上升会影响消费者的预期从而影响到AD curve。但是，现在似乎大家都不关心这事情，一则是因为相信Trump的供给侧改革会成功，要么就是今朝有酒今朝醉，因此，财政赤字对AD curve的影响不明朗。</p><p>综上，工资水平使得AS curve轻微向上平移，原油价格使得AS curve轻微向上平移，汇率使得AS curve轻微向下平移。总体来说，AS会向上，但是程度很小。如下图所示：</p><img src="/2017/12/07/Trump-tax-reform/AS-AD.png" class=""><p>再从open economy的角度来分析：减税会刺激国内的需求，但是这些新增的需求并非会全部购买国内的商品和服务，有部分会去购买国外的产品和服务，所以进口增加而出口不变，因此贸易赤字增加，另外，对国内的产出（GDP）的刺激效果也相应的减弱了。之前在短期情况的分析可知利率会走高，汇率也会走高而通胀保持稳定，那么美国的金融市场会比较有吸引力。为了保证减税的效果，从美国的角度来说，还是希望美元不要太强势。</p><p>综合以上的分析，可得出美国会在一段时间内维持一个高于natural level of output。也就是相对高的增长，而通胀相对比较温和。所以可以得到以下这些结论：</p><ul><li>利率会稳步抬高，对债市而言是利空；</li><li>股市依旧有机会，崩盘的可能性近期不大，下跌仍旧是买入机会；</li><li>商品：由于经济继续增长对商品是利好，另一方面通胀压力不大，所以预期不会有很大的波动行情；</li><li>美元汇率：温和上涨，因为处于加息通道，并且经济持续增长；</li><li>黄金：震荡向下。作为零息债券，随着实际利率的抬升，不看好它的表现。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;每次市场上出现一些热点事件后，通常都是众说风云，自己看了这些新闻，以及网上大V们的论述之后往往觉得是一头雾水，也没有办法分清楚谁对谁错。最近想想，觉得其实自己有武器却没有利用起来啊，毕竟也是学过宏观经济学的人，教材里面清清楚楚的把分析框架提供给我了，而我却视而不见，反倒是迷失在各种标题党中。想想觉得自己也是蛮愚蠢的。情愿相信不可靠的消息和评论，却不依靠教材上提供的可靠的框架。经常听见说教材上的东西都是过时的，没有实际作用的，我自己也很轻易就相信了。但是，稍微分析一下：首先：教材上的东西都是经典，是经过时间积累下来的精华部分，如果没有用怎么会写进教材呢？其次，这些经典理论的作者都是大家，比我要聪明无数倍了，我没有理由不去使用他们。最后，反省一下我看了那么多所谓大V的文章有收获和进步么？&lt;/p&gt;
    
    </summary>
    
    
      <category term="Macro" scheme="https://www.keepswalking.com/categories/Macro/"/>
    
    
  </entry>
  
  <entry>
    <title>风险评价策略（Risk Parity)基准指数</title>
    <link href="https://www.keepswalking.com/2017/10/16/rp-benchmark-index/"/>
    <id>https://www.keepswalking.com/2017/10/16/rp-benchmark-index/</id>
    <published>2017-10-16T13:09:07.000Z</published>
    <updated>2017-10-16T13:12:09.203Z</updated>
    
    <content type="html"><![CDATA[<p>6月份考完CFA leve I 就打算用R实现一个Risk Parity的策略，原本觉得很快就能搞定，但是没想到越写东西越多，细节的问题不断浮出水面，直到最近才完成了大部分的功能。写了1000多行的R代码，本篇就是梳理自己在编程过程中碰到的一些问题和想法。如果想了解Risk Parity量化的方法可以参考之前的《<a href="http://www.keepswalking.com/2017/07/23/risk-parity-quantitative-fundamental/">Risk Parity 量化入门</a>》<br><a id="more"></a></p><h2 id="最根本的想法：为什么要实现一个Risk-Parity，或为什么Risk-Parity能实现收益？"><a href="#最根本的想法：为什么要实现一个Risk-Parity，或为什么Risk-Parity能实现收益？" class="headerlink" title="最根本的想法：为什么要实现一个Risk Parity，或为什么Risk Parity能实现收益？"></a>最根本的想法：为什么要实现一个Risk Parity，或为什么Risk Parity能实现收益？</h2><p>我自己主要是参考了Ray Dalio关于全天候基金的一些介绍，觉得很有道理。主要有以下几点：</p><ul><li>长期来看，beta也就是资产的收益一定是超过现金的。</li><li>理由是因为，其一，这是有资本主义制度本质决定的，资金总是流向能产生更高收益的地方。而那些能产生正收益的人或项目就会借入资金，而投资人则会借出资金获取收益。显然，资本市场就是投资人和项目汇集地。当然，有些制度下未必如此。。。</li><li>其二，风险应该获得对应的补偿。简而言之就是高风险高收益。</li></ul><p>对于我自己而言，我需要一个benchmark，来衡量自己投资的绩效。同时，这个基准指数，也是我用来观察市场的一个很好的工具。因此，我觉得实现这个基准指数是一件很有意义的事情。</p><h2 id="为什么不去追求alpha？"><a href="#为什么不去追求alpha？" class="headerlink" title="为什么不去追求alpha？"></a>为什么不去追求alpha？</h2><p>我觉得是一个因人而异的事情。alpha是零和游戏，你获得的alpha其实别人的loss，从这点来看就知道获取alpha的竞争激烈，也就是说，你必须比别人在某些方面强，你才能从他人身上抢到钱。明白了这点，就需要对自身进行一些必要分析，自己到底在哪些方面比别人强？要长期跑赢市场是一件几乎不可能完成的任务！在这个市场里面，有人靠基本面分析，有人靠技术分析，有的靠行为心理。。。但是，我相信这个市场的效率会越来越高，越来越逼近semi-strong efficient market。秉持这个观点，再加之我自己并没有什么过人之处，那么，我应该安心于获取beta，或者说，要获取alpha我会借助别人的能力，比如选则基金。这里我想多说一句，对于大部分散户而言，不要去走所谓的基本面这条路，因为其实这条路是相当昂贵的，光从数据角度来看，要获取和机构对等的数据就是一件不可能的事情！机构可以获取各类研究报告，机构有专职人员看某个行业，当需要了解行业细节时候，随时有人可以说清楚，他们甚至可以花钱用卫星来数船。</p><p>以上的大原则指导了我努力的方向。那么，要获取beta，搞个股票指数基金定投也ok啊，为啥要用Risk Parity呢？如果你能承受得起指数的波动并能坚持到底，我想也是一条可选则的路。就我自己而言，我相信分散投资，而Risk Parity能做到比指数基金更好的分散投资，因此也就实现了更低的波动。</p><h2 id="Risk-Parity的组合里面该包括什么品种？"><a href="#Risk-Parity的组合里面该包括什么品种？" class="headerlink" title="Risk Parity的组合里面该包括什么品种？"></a>Risk Parity的组合里面该包括什么品种？</h2><p>我主要是根据Edward E. Qian的那本 Risk Parity Fundmentals来选择品种。大类上来讲包括了以下三种：</p><ul><li>interest risk premium，也就是债券了，其实主是指利率债。长期来看，投资者通过承担利率风险（比如买入长期国债）获得利率的风险溢价。</li><li>equity risk premium，也就是股票了，长期来看投资者通过投资股票必然可以获得高于无风险利率的收益。其实从微观的角度来看，企业主通过股票市场融资，通过持续经营，获得高于融资成本的收益。</li><li>inflation risk premium，也就是商品，长期来看商品可以对冲通货膨胀。</li></ul><p>而这三大类资产类别本身的相关性不高，所以是比较理想的构建portfolio的要素。比如，在高通胀时期，股票和债券的表现不好，但是，商品的表现会比较突出；而在经济低迷的时期，债券的表现要好于股票和商品。需要说明的是以上三大类都包含了“长期来看”这个前提，因为从某个时间段来说，会出现三类资产都在下跌的情况。但是从长期来看，这3类资产的期望收益都是正的。</p><h2 id="都有哪些资产入选到Risk-Parity基准指数里？"><a href="#都有哪些资产入选到Risk-Parity基准指数里？" class="headerlink" title="都有哪些资产入选到Risk Parity基准指数里？"></a>都有哪些资产入选到Risk Parity基准指数里？</h2><p>刚一开始我并不想加入太多的资产品种，我想从简单的开始，逐步完善，而基准指数也是我观察市场的一个窗口。我初步的想法是在大类的risk parity下包含小类的risk parity。大类上已经提到了要包括债券，股票和商品。而每个大类下，我想再通过跨国配置做进一步的risk parity，目的是将风险分散的更彻底。那么需要包括哪些国家呢？我想以：</p><ul><li>消费国：美国，欧洲</li><li>生产国：中国</li><li>资源国：澳洲</li></ul><p>这样的分类来选择国家，之所以这样想，是因为，通常这些国家在同一时间会处于不同的经济周期上，因此，也就能更好的分散风险。所以，最后每个大类都会包含这些国家的资产。例如，在股票的大类下会分别包含：美国、欧洲、中国和澳洲的股票，而这些不同国家的股票之间也要实现risk parity。以下就是目前包括的基准指数里包含的资产：</p><ul><li>股票<ul><li>美国：S&amp;P500 - 标普500指数(Total Return)</li><li>欧洲：德国DAX指数，本来想用stoxx 50的，但是不确定stoxx是否是total return（有谁知道麻烦告知一下），而DAX指数本身就是total return的方式。</li><li>中国：沪深300指数，但是找不到沪深300的total return指数，所以就直接用沪深300EFT（510310）并使用后复权获得最终的数据。不过300EFT历史太短了，导致我很难做回测，我就用50ETF接300ETF，这么处理一下后就有比较长的历史数据了！</li><li>澳洲：ASX200 - 澳洲200指数（Total Return）</li></ul></li><li>债券<ul><li>美国：Thomson Reuters US 10 Year Government Benchmark - 美国10年期债券指数 （目测应该是total return）</li><li>欧洲：S&amp;P Eurozone Sovereign Bond 7-10 Years Index （total return）</li><li>中国：Shanghai SE Treasury Bond - 国债指数 （目测应该是total return）</li><li>澳洲：S&amp;P/ASX Government Bond 5-10 Year Index (Total return)</li></ul></li><li>商品，对于商品而言，并没有国别的区分，主要考虑到国内市场和国际市场会有不同步的现象，所以分别使用两个商品指数。同时，将黄金单列出来，我的想法是黄金本身是对冲通胀的一种工具，同时她兼具一种信用保障，比如她一种避险资产，因此将她单列出来，目的是能起到更好的分散风险的作用。<ul><li>黄金 - 国内黄金现货价格 </li><li>国际商品：S&amp;P GSCI 商品指数（total return）</li><li>国内商品：通达信期货通里面有一个大宗商品指数（T001），不清楚是谁编制的，暂时就先用这个指数吧！这个指数应该不是total return。不过影响应该不会太大。</li></ul></li></ul><p>还是要提一句数据收集，没想到花掉那么多精力，之前也没有考虑指数是否是total return。随着测试的开始，渐渐意识到这个问题。而我自己也没有什么好的数据终端（花不起这个钱啊！）好在investing.com上的数据还比较全面。后来又发现了www.spindices.com这个网站，他是标普的一个关于指数汇编的网站，提供了非常全面的指数品种，还能下载数据，不过普通用户只能回溯10年的数据。看来以后花钱买数据的必然的了。回测的过程中发现了很多数据上的问题，需要自己不断地对数据进行修正，数据的准备和清理觉绝对是一件耗时耗精力的事情，远远超出了自己的预期。</p><h2 id="资金管理"><a href="#资金管理" class="headerlink" title="资金管理"></a>资金管理</h2><p>本系统使用一个很简单的资金管理模型：1倍恒定杠杆（1份本金+1份融资）。也就是在任何情况下保持1倍左右的杠杆。从资金管理的角度来看，重点关注是否会爆仓。那么1倍杠杆爆仓会发生在所持的portfolio瞬时下跌50%。这种可能性从一个交易日的时间窗口来看几乎是不可能发生的。因为portfolio本身是分散投资的，包括了债券、股票和商品。另外由于是恒定1倍杠杆的算法，当资产价格下跌，导致超过1倍杠杆时就会触发系统卖出所持资产保证杠杆维持在1倍。</p><h2 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h2><p>以下罗列一些实现上的细节：</p><ul><li>Risk Parity everywhere。不光是在资产大类上实现Risk Parity，在子类上也实现Risk Parity，例如在股票大类里面可以分为，中国股票，美国股票，德国股票等，那么这些品种也会是Risk Parity。 再下一层，比如中国股票里面可以分创业板，上证50，那么仍旧是Risk Parity。 而我编写的R程序可以自动完成这些工作，我所需要做的仅仅是写出一个资产配置的树形结构。</li><li>Rebalance 策略。我没有使用定期Rebalance的策略，而是针对每个品种设置一个threadhold。超过了就进行Rebalance。这个threadhold是根据每个品种的return std(standard deviation)计算出来的。</li><li>correlation matrix不是静态的，而是会变动的，目前的策略就是根据最近3~5年的数据进行调整。判断correlation matrix是否需要更新的标准是同上次使用的matrix做比较，如果发生统计意义上的改变，则correlation matrix需要更新，那么对应的，各类资产的比重也都需要调整。R程序里使用biotools包的boxM方法来进行判断。</li><li>计算各类资产子Risk Parity下的比重的时候，需要用到优化包，R程序里使用的是BB包里面的spg方法。她能模拟Excel里面的规划求解（solver）。</li><li>计算指数的时候考虑到了交易费用和借款利息。交易费用目前固定为1/1000，借款利息使用的是14天回购利率（204014）。不清楚这样是否合理。</li><li>将所有国外的资产价格换算成RMB（CNY），因为，在计算correlation的时候，必须用同种货币才有意义。</li></ul><h2 id="下一个阶段需要做哪些方面的改进？"><a href="#下一个阶段需要做哪些方面的改进？" class="headerlink" title="下一个阶段需要做哪些方面的改进？"></a>下一个阶段需要做哪些方面的改进？</h2><p>目前我最希望能加入的是timing model，也就择时调整个资产的比重。一个直观的想法是，如果债券处在高位了，那么我希望在Risk Parity里面减少债券的比重。同样，如果商品处在低位，那么我希望能增加该品种的比重。看了一些资料，目前的方向是引入momentum/trend following的方法，对各个品种的比重进行调整。当然也有很多的细节问题需要考虑，比如：当某个品种加速上冲的时候，也往往是最后一波行情了，比如2015年6月份的股灾。那么，这个时候，momentum/trend following并不能指出这种风险。这就需要设计一个指标，根据历史数据的统计加入判断，比如乖离率的统计。具体我也没有想清楚。有时候觉得，既然是一个benchmark，就不应加入这些目的是为了增加alpha的东西。而这些可以加入到我自己的真实的投资组合当中去。</p><h2 id="回测演示"><a href="#回测演示" class="headerlink" title="回测演示"></a>回测演示</h2><p>最后就显示一下自己做的Risk Parity benchmark index的资金曲线。如下图所示：<br><img src="/2017/10/16/rp-benchmark-index/rp_ts.jpg" class=""></p><p>红色是1倍杠杆的净值曲线，黑色是S&amp;P500 total return并换算成人民币。下面针对回测结果做一下说明和总结：</p><ul><li>首先，还是有点出乎意外，竟然能和S&amp;P500差不多打平，要知道我才用了1倍的杠杆。</li><li>该净值曲线是在扣除了交易费用之后获得的。不过没有计算融资费用，也就是1倍杠杆的融资费用。这算是小小耍流氓。会在下次回测的时候做一下灵敏度的测试，看看不同的融资费用和交易费用会对净值造成多大的影响</li><li>资产分配的权重演变来看，初始权重（不包括杠杆，也就是总计为1的情况下），最初债券的权重为74%，演变到后来债券的权重为85%以上，几乎就是一只债券基金了。主要的原因就是债券超低的波动率，尤其是上证国债的超低波动率（几乎就是一根直线），再加上全球央行的大放水，造成了债券的超低波动率，这就直接导致了在Risk Parity中，债券占比的大幅提高。一个值得思考的点就是，如果债券波动率突然增加，就会对现有的配置造成很大的冲击。是否应该给每个大类设置设置一个配置的上限呢？</li><li>其实，这个Risk Parity的组合是可以通过增加杠杆来提高收益的。因为，通过数据可以看到，未加杠杆的情况下（总计权重为1）的Risk Parity的standard deviation和S&amp;P500 total return (人民币计价)的standard deviation的比为：1：5.2。也就是说，如果将Risk Parity的波动率调成和S&amp;P500一致的话，可以用4倍杠杆！而目前只用了1倍杠杆。所以，在实际运行Risk Parity的时候，也可以考虑动态杠杆。</li><li>Risk Parity回测结果的Annualized Sharpe Ratio为2.13。很不错的值，不过考虑到其实她几乎就是一只债券基金，这个sharp ratio也算靠谱。</li><li>最后提一下08年金融危机时的表现，从净值曲线的表现来看是远好于S&amp;P500的，这得益于危机期间债券，黄金和其他资产之间的负相关性。</li></ul><p>参考文献：</p><ul><li>Edward E. Qian, Risk Parity Fundmentals</li><li>Bridgewater, Engineering targeted return &amp; risks</li><li>Bridgewater, Our thoughts about Risk Parity and All Weather</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;6月份考完CFA leve I 就打算用R实现一个Risk Parity的策略，原本觉得很快就能搞定，但是没想到越写东西越多，细节的问题不断浮出水面，直到最近才完成了大部分的功能。写了1000多行的R代码，本篇就是梳理自己在编程过程中碰到的一些问题和想法。如果想了解Risk Parity量化的方法可以参考之前的《&lt;a href=&quot;http://www.keepswalking.com/2017/07/23/risk-parity-quantitative-fundamental/&quot;&gt;Risk Parity 量化入门&lt;/a&gt;》&lt;br&gt;
    
    </summary>
    
    
      <category term="Risk Parity" scheme="https://www.keepswalking.com/categories/Risk-Parity/"/>
    
    
  </entry>
  
  <entry>
    <title>随笔如何准备CFA Level I 【2017年6月】</title>
    <link href="https://www.keepswalking.com/2017/07/26/How-did-I-prepare-CFA-level-I/"/>
    <id>https://www.keepswalking.com/2017/07/26/How-did-I-prepare-CFA-level-I/</id>
    <published>2017-07-26T12:54:48.000Z</published>
    <updated>2017-07-26T13:52:06.551Z</updated>
    
    <content type="html"><![CDATA[<p>今天早收到email，一看开头是congratulations我就激动的叫起来了，过了！说一下我的背景供大家参考，我在校期间是非金融专业，工作也和金融没半毛关系。纯粹是对这投资有兴趣。个人觉得CFA level I还是有难度的，并非网上看到的牛人们，花3，4个月轻松就过。 当然，每个人的情况不同，所以，还是要根据自己的客观情况来制定复习策略。</p><a id="more"></a><p>虽然我非金融专业，但是在考CFA之前的近2年时间里面，几乎把CFA所涉及的范围都看到了，说来也很奇怪，我在涉及这些内容的时候根本没想到要去考CFA，只是觉得要学投资的话就需要这些知识，到后来听说CFA，打算去考的时候，才发现原来自己之前看的东西和CFA都非常的切合，这也从另外一个侧面说明了CFA确实是系统的覆盖了投资的方方面面。</p><p>接下来说说我是怎么准备CFA level I的考试的， 关于教材，我就直接用的电子档。CFA报名后就可以下载。我没有买什么纸质的教材，而且觉得也没有这个必要。我也没有上任何的课程或看视频，我也觉得没这个必要。我复习的材料就是官方电子版教材+notes（电子版）。</p><p>我一共复习了6个月，一开始的5个月每天平均2个小时，最后1个月每天至少3，4个小时。</p><p>电子版教材有6本，我是从头到尾看过一遍的。这里的一些建议是：</p><ul><li>顺序就按照官方给的顺序来吧，我觉得这个顺序蛮好的</li><li>每个章节的题目一定要做，而且要把错误的题目记录下来，这些错题代表了你对这些知识点不熟悉，所以要准备一个笔记本，专门将这些薄弱的知识点记录下来，这样在后期复习的时候会非常有帮助！！！</li></ul><p>看完了教材之后，我又看了一遍notes，notes一共5本。看notes的主要目的，一是复习，二是notes会针对考点来编写内容，这样的编排对考试很有帮助。看notes的时候也是要发现自己薄弱环节，将这些点记录下来。</p><p>看完了notes以后，我觉得自己在Financial Reporting &amp; Analysis, Corporate Finance方面还是比较弱，所以又看了一遍这些topic的notes。</p><p>在最后的一个月中就是题海大战了，除了昨天还是做题。个人觉得这样做题的效果是相当显著的！这里需要强调一下，我用的题目都是官方历年的模拟题，不建议大家去找什么非官方的题目。一张试卷就有120道题目，一套模拟题又有2张试卷（上下午），所以一套题目就有240题，我做了2008到2017年的所有的模拟题。我觉得这些题目足以覆盖绝大部分的题型了！从统计角度来看也应该是这样了。事实也证明这样的题海练习还是蛮有用的。错的题目要记录，要反复看。</p><p>Ethical &amp; Professional Standards对于我来说是相当有难度的，每次模拟如果是新的题目，一般都做的不理想。这个没有办法，只能把handbook多看几遍，当然越多越好了。但是，我复习的时候这块没安排好，就看了一遍多一点。不过，最后分数上看还算满意。</p><p>正式考试分上下午，各3个小时，对体能也是相当大的考验，建议带些硬糖，这是被允许的。巧克力是不行的，坐我旁边一个小姑娘带了巧克力，被没收了，结果考试时看我吃糖。。。红牛太大瓶了，喝了会小便，还是用小瓶的功能性饮料吧。我喝力保健。中午最好自己带个三明治。这样方便又节约时间。</p><p>关于考试时间是否来得及的问题，我觉得应该来得及。正式考试不如模拟考那么难，基本上做完应该是没啥问题的。建议一路扫下去，一看不会的或比较耗时间的题目先放掉，回过头来再做。我用的是这样的策略。</p><p>复习的时候，我觉得Financial Reporting &amp; Analysis里面关于tax，deffered tax asset/liability很难，像我这种非专业人士真的很难理解这些点，而官方的教材写的也不是很容易明白，所以，我自己花了不少时间在网上找相关的材料。</p><p>还要提一下阅读速度，我觉得挺关键的，我自己阅读速度很慢，看教材就花了大量的时间。所以，我现在也在努力想办法提高自己的阅读速度。</p><p>准备CFA是一个漫长的过程，一路走来肯定不容易，大家也要学会调节自己的心态。尤其碰到不顺利的时候，还是尽量保持耐心。当然，实在看不下去的时候放松一下也没啥坏处，关键还是要坚持。</p><p>我自己也要感谢家人，尤其是我的妻子给我的支持。也祝各位走在CFA这条路上的朋友能顺利过关。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天早收到email，一看开头是congratulations我就激动的叫起来了，过了！说一下我的背景供大家参考，我在校期间是非金融专业，工作也和金融没半毛关系。纯粹是对这投资有兴趣。个人觉得CFA level I还是有难度的，并非网上看到的牛人们，花3，4个月轻松就过。 当然，每个人的情况不同，所以，还是要根据自己的客观情况来制定复习策略。&lt;/p&gt;
    
    </summary>
    
    
      <category term="CFA" scheme="https://www.keepswalking.com/categories/CFA/"/>
    
    
  </entry>
  
  <entry>
    <title>Risk Parity 量化入门</title>
    <link href="https://www.keepswalking.com/2017/07/23/risk-parity-quantitative-fundamental/"/>
    <id>https://www.keepswalking.com/2017/07/23/risk-parity-quantitative-fundamental/</id>
    <published>2017-07-23T06:20:59.048Z</published>
    <updated>2017-07-26T12:50:23.458Z</updated>
    
    <content type="html"><![CDATA[<p>摘要：本文论述了Risk Partity的基本概念，并介绍了如何通过量化的方法构建Risk Parity的Portfolio，并以excel作为工具实现一个简单的Risk Parity的应用。</p><p>关键字：Risk Parity， Risk contribution</p><a id="more"></a><p>“不要将鸡蛋放在同一个篮子里”告诉我们要分散风险，个人觉得是一句无比智慧的话。在没有修炼成巴菲特之前我觉得还是要牢牢记住这句话。如何做到分散风险？60%的股票+40%的债券算是分散风险吗？不妨做一个情景假设，你有100w元资金，60w买入股票ETF，40w买入债券ETF，持有    1年后，假如股票下跌了10%，而债券收益2%，那么你的投资总额变成了60w<em>0.9+40w</em>1.02=95.88；相当于总资产减值了4.14%，这似乎是不太理想的，尤其是在股票和债券的correlation还是负相关的情况下。在以上情形下，如果加大债券的配置那么整个portfolio就会更加的平稳。那么接下来的问题就是该如何分配股票和债券的权重？</p><h2 id="Risk-Contribution"><a href="#Risk-Contribution" class="headerlink" title="Risk Contribution"></a>Risk Contribution</h2><p>单个risk asset可以用variance来度量风险，而要计算一个投资组合的variance的话还需要知道convariance。假设这些数据是已知的。</p><h3 id="组合中包括2种asset的情况："><a href="#组合中包括2种asset的情况：" class="headerlink" title="- 组合中包括2种asset的情况："></a>- 组合中包括2种asset的情况：</h3><p>还是假设60%股票+40%债券，并进一步假设股票的volatility是15%，债券的volatility是5%，correlation为0.2。那么根据公式：<br><img src="/2017/07/23/risk-parity-quantitative-fundamental/1.png" class=""></p><p>就可以计算出组合的variance：<br><img src="/2017/07/23/risk-parity-quantitative-fundamental/2.png" class=""></p><p>有了组合的variance，又该如何计算股票和债券对该组合variance的占比呢？观察以上的计算式，第一项可归因到股票，第三项可归因到债券。那么第二项（convariance）该如何归因呢？其实就是将一半归因到股票另一半归因到债券！<br>由此可知，股票的 risk contribution:<br><img src="/2017/07/23/risk-parity-quantitative-fundamental/3.png" class=""></p><p>债券的risk contribution：<br><img src="/2017/07/23/risk-parity-quantitative-fundamental/4.png" class=""></p><p>有了这些数据，接下来就能很容易计算出股票和债券在组合variance中的占比，也就是risk contribution的占比。<br>股票的风险占比：<br><img src="/2017/07/23/risk-parity-quantitative-fundamental/5.png" class=""></p><p>债券的风险占比：<br><img src="/2017/07/23/risk-parity-quantitative-fundamental/6.png" class=""></p><p>从以上数据就可以清晰的看出60/40其实是一种非常不平衡的组合，其中股票的risk contribution高达92%，也就是说在60/40的组合里面，风险主要集中在股票上。</p><h3 id="组合中包括3种asset的情况："><a href="#组合中包括3种asset的情况：" class="headerlink" title="- 组合中包括3种asset的情况："></a>- 组合中包括3种asset的情况：</h3><p>再来讨论一下组合中持有3中资产的情况，因为3中资产的情况更具一般性，可以推广到持有更多资产的情况。假设新增资产为商品，volitility 和 correlation见下表：<br><img src="/2017/07/23/risk-parity-quantitative-fundamental/7.png" class=""></p><p>并假设按照40%的股票，40%的债券，20%的商品来分配。根据以下公式可以获得组合的variance：<br><img src="/2017/07/23/risk-parity-quantitative-fundamental/8.png" class=""></p><p>以本例的3种asset来说，可以获得如下的计算公式：<br><img src="/2017/07/23/risk-parity-quantitative-fundamental/9.png" class=""></p><p>代入上表中的数据，可得出计算出组合的variance: 0.00778<br>接下来要求每一项资产的risk contribution, 可以通过如下方式获得：<br><img src="/2017/07/23/risk-parity-quantitative-fundamental/10.png" class=""></p><p>第一项是variance, 第二和第三项分别对应covariance的一半。根据以上算式可计算出：</p><ul><li>债券的risk contribution：0.00044；占比：0.00044/0.00778=6%</li><li>股票的risk contribution：0.0044；占比：0.00444/0.00778=57%</li><li>商品的risk contribution：0.0029；占比：0.0029/0.00778=37%</li></ul><p>以上数据显示，40%的债券对应的risk contribution只是6%，而股票和商品的risk contribution则显得太高了。</p><h2 id="Risk-Parity"><a href="#Risk-Parity" class="headerlink" title="Risk Parity"></a>Risk Parity</h2><p>Risk Parity (也叫risk budgeting)，是通过调整各asset在组合中所占的比重以实现个asset的risk contribution相同或等于指定的数值。</p><h3 id="组合中包括2种asset的情况：-1"><a href="#组合中包括2种asset的情况：-1" class="headerlink" title="- 组合中包括2种asset的情况："></a>- 组合中包括2种asset的情况：</h3><p>这种情况很简单了，不需要考虑2种asset的correlation, 只需要比较他们的volitility就行了，再使用之前的例子，因为股票的volatility是15%，债券的volatility是5%，那么，通过让债券的volatility对应的金额增至现在的3倍即能实现股票和债券的volatility对应的金额相等。从组合比重的角度来说，也就是1%的股票要对应3%的债券，所以，组合的权重就为：75%的债券 + 25%的股票。</p><h3 id="组合中包括3种asset的情况：-1"><a href="#组合中包括3种asset的情况：-1" class="headerlink" title="- 组合中包括3种asset的情况："></a>- 组合中包括3种asset的情况：</h3><p>这种情况会比上面复杂，因为没有直接的计算方法，需要通过数值分析法来获得近似解。这里通过excel solver举例，如下图所示：<br><img src="/2017/07/23/risk-parity-quantitative-fundamental/11.png" class=""></p><p>最终的解为：股票18%，债券68%，商品14%。接着对excel的输入输出做一些说明。绿色部分是correlation matrix，作为输入参数。红色部分是solver的“可变单元格”，也就是最终的输出（参见下图）。“约束条件”设置成“总计”=1 并勾选“非负数”（参见下图）。棕色部分即risk contribution是根据组合的比重计和correlation matrix算出来的，比如股票部分的计算设置成：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x3D;(C2^2*F6^2+C2*C3*E4*E6*F6+C2*C4*F5*F6*G6)*10000</span><br></pre></td></tr></table></figure><br>注意这里有意将最终的计算结果放大了10000倍，这么处理是便于处理Error项，因为risk contribution的值都很小。Error项作为solver的“设置目标”，也就是优化要达到的最终目标。由于优化的目标是希望Risk contribution的各项都相同，那么也就是说这3项对应的variance=0，因此，Error就设置成：=VAR.P(C7:C9)<br><img src="/2017/07/23/risk-parity-quantitative-fundamental/12.png" class=""></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文介绍了实现risk parity的一种量化方法。并通过使用Excel的solver实现了组合中包含3类asset的risk parity的计算。以3类资产为基础很容易推广至更多类资产。个人觉得用excel+solver的方式对于组合里面asset数量在10多个的情况下是足够应付了。Risk Parity本身计算所需的输入其实很简单，就是各类asset的variance和correlation matrix。但是，所谓garbage in garbage out。如何获取可靠的variance和correlation其实倒是一个难点。</p><p>参考文献：</p><ul><li>2017 CFA level1 volumn 4 corporate finance and portfolio management</li><li>Edward E. Qian, Risk Parity Fundmentals</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;摘要：本文论述了Risk Partity的基本概念，并介绍了如何通过量化的方法构建Risk Parity的Portfolio，并以excel作为工具实现一个简单的Risk Parity的应用。&lt;/p&gt;
&lt;p&gt;关键字：Risk Parity， Risk contribution&lt;/p&gt;
    
    </summary>
    
    
      <category term="量化投资组合管理" scheme="https://www.keepswalking.com/categories/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%E7%BB%84%E5%90%88%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="风险平价" scheme="https://www.keepswalking.com/tags/%E9%A3%8E%E9%99%A9%E5%B9%B3%E4%BB%B7/"/>
    
  </entry>
  
</feed>
